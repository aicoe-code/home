[
  {
    "id": "2507.23357v2",
    "title": "Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures",
    "summary": "This report analyzes the evolution of key design patterns in computer vision\nby examining six influential papers. The analysis begins with foundational\narchitectures for image recognition. We review ResNet, which introduced\nresidual connections to overcome the vanishing gradient problem and enable\neffective training of significantly deeper convolutional networks.\nSubsequently, we examine the Vision Transformer (ViT), which established a new\nparadigm by applying the Transformer architecture to sequences of image\npatches, demonstrating the efficacy of attention-based models for large-scale\nimage recognition. Building on these visual representation backbones, we\ninvestigate generative models. Generative Adversarial Networks (GANs) are\nanalyzed for their novel adversarial training process, which challenges a\ngenerator against a discriminator to learn complex data distributions. Then,\nLatent Diffusion Models (LDMs) are covered, which improve upon prior generative\nmethods by performing a sequential denoising process in a perceptually\ncompressed latent space. LDMs achieve high-fidelity synthesis with greater\ncomputational efficiency, representing the current state-of-the-art for image\ngeneration. Finally, we explore self-supervised learning techniques that reduce\ndependency on labeled data. DINO is a self-distillation framework in which a\nstudent network learns to match the output of a momentum-updated teacher,\nyielding features with strong k-NN classification performance. We conclude with\nMasked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design\nto reconstruct heavily masked inputs, providing a highly scalable and effective\nmethod for pre-training large-scale vision models.",
    "authors": [
      "Radu-Andrei Bourceanu",
      "Neil De La Fuente",
      "Jan Grimm",
      "Andrei Jardan",
      "Andriy Manucharyan",
      "Cornelius Weiss",
      "Daniel Cremers",
      "Roman Pflugfelder"
    ],
    "published": "2025-07-31T09:08:11Z",
    "updated": "2025-09-04T07:47:30Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.23357v2",
    "arxiv_url": "http://arxiv.org/abs/2507.23357v2",
    "comment": null,
    "relevance_score": 8.5
  },
  {
    "id": "2509.03895v1",
    "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of   Vision-Language Model",
    "summary": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.",
    "authors": [
      "Phuoc-Nguyen Bui",
      "Khanh-Binh Nguyen",
      "Hyunseung Choo"
    ],
    "published": "2025-09-04T05:42:02Z",
    "updated": "2025-09-04T05:42:02Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03895v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03895v1",
    "comment": "ICCV 2025 - LIMIT Workshop",
    "relevance_score": 8.5
  },
  {
    "id": "2509.04066v1",
    "title": "Arabic Chatbot Technologies in Education: An Overview",
    "summary": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field.",
    "authors": [
      "Hicham Bourhil",
      "Yacine El Younoussi"
    ],
    "published": "2025-09-04T09:55:16Z",
    "updated": "2025-09-04T09:55:16Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04066v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04066v1",
    "comment": "Published as a book chapter in: Transformaci\\'on Digital en la\n  Educaci\\'on: Innovaciones y Desaf\\'ios desde los Campus Virtuales (UA\n  Journals, 2024), pp. 11-14",
    "relevance_score": 8.5
  },
  {
    "id": "2509.02761v2",
    "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
    "summary": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.",
    "authors": [
      "Ananth Hariharan",
      "Vardhan Dongre",
      "Dilek Hakkani-Tür",
      "Gokhan Tur"
    ],
    "published": "2025-09-02T19:06:56Z",
    "updated": "2025-09-04T15:30:53Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.02761v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02761v2",
    "comment": null,
    "relevance_score": 7.5
  },
  {
    "id": "2410.13287v6",
    "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware   Selection of Generative Models and LLMs",
    "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.",
    "authors": [
      "Xiaoyan Hu",
      "Ho-fung Leung",
      "Farzan Farnia"
    ],
    "published": "2024-10-17T07:33:35Z",
    "updated": "2025-09-04T13:51:56Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2410.13287v6",
    "arxiv_url": "http://arxiv.org/abs/2410.13287v6",
    "comment": "accepted to ICML 2025",
    "relevance_score": 7.5
  },
  {
    "id": "2509.04058v1",
    "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
    "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
    "authors": [
      "Lei Zhong",
      "Yi Yang",
      "Changjian Li"
    ],
    "published": "2025-09-04T09:41:18Z",
    "updated": "2025-09-04T09:41:18Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04058v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04058v1",
    "comment": null,
    "relevance_score": 7.5
  },
  {
    "id": "2509.03957v1",
    "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese   Misinformation Fact-Checking",
    "summary": "The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY",
    "authors": [
      "Ruiling Guo",
      "Xinwei Yang",
      "Chen Huang",
      "Tong Zhang",
      "Yong Hu"
    ],
    "published": "2025-09-04T07:33:44Z",
    "updated": "2025-09-04T07:33:44Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03957v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03957v1",
    "comment": "Findings of EMNLP 2025",
    "relevance_score": 7.5
  },
  {
    "id": "2506.07900v2",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.",
    "authors": [
      " MiniCPM Team",
      "Chaojun Xiao",
      "Yuxuan Li",
      "Xu Han",
      "Yuzhuo Bai",
      "Jie Cai",
      "Haotian Chen",
      "Wentong Chen",
      "Xin Cong",
      "Ganqu Cui",
      "Ning Ding",
      "Shengda Fan",
      "Yewei Fang",
      "Zixuan Fu",
      "Wenyu Guan",
      "Yitong Guan",
      "Junshao Guo",
      "Yufeng Han",
      "Bingxiang He",
      "Yuxiang Huang",
      "Baoxi Ji",
      "Cunliang Kong",
      "Qiuzuo Li",
      "Siyuan Li",
      "Wenhao Li",
      "Xin Li",
      "Yanghao Li",
      "Yishan Li",
      "Zhen Li",
      "Dan Liu",
      "Biyuan Lin",
      "Yankai Lin",
      "Xiang Long",
      "Quanyu Lu",
      "Yaxi Lu",
      "Peiyan Luo",
      "Hongya Lyu",
      "Litu Ou",
      "Yinxu Pan",
      "Lushi Pu",
      "Zekai Qu",
      "Qundong Shi",
      "Zijun Song",
      "Jiayuan Su",
      "Zhou Su",
      "Ao Sun",
      "Xianghui Sun",
      "Peijun Tang",
      "Fangzheng Wang",
      "Feng Wang",
      "Shuo Wang",
      "Yudong Wang",
      "Zheng Wang",
      "Yesai Wu",
      "Zhenyu Xiao",
      "Jie Xie",
      "Zihao Xie",
      "Xiaoyue Xu",
      "Yukun Yan",
      "Jiarui Yuan",
      "Jinqian Zhang",
      "Kaihuo Zhang",
      "Lei Zhang",
      "Linyue Zhang",
      "Xueren Zhang",
      "Yudi Zhang",
      "Hengyu Zhao",
      "Weilin Zhao",
      "Weilun Zhao",
      "Yuanqian Zhao",
      "Zhi Zheng",
      "Chuyue Zhou",
      "Ge Zhou",
      "Jie Zhou",
      "Wei Zhou",
      "Yanghao Zhou",
      "Zihan Zhou",
      "Zixuan Zhou",
      "Zhiyuan Liu",
      "Guoyang Zeng",
      "Chao Jia",
      "Dahai Li",
      "Maosong Sun"
    ],
    "published": "2025-06-09T16:16:50Z",
    "updated": "2025-09-04T16:23:02Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2506.07900v2",
    "arxiv_url": "http://arxiv.org/abs/2506.07900v2",
    "comment": "MiniCPM4 Technical Report",
    "relevance_score": 6.5
  },
  {
    "id": "2509.04338v1",
    "title": "From Editor to Dense Geometry Estimator",
    "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.",
    "authors": [
      "JiYuan Wang",
      "Chunyu Lin",
      "Lei Sun",
      "Rongying Liu",
      "Lang Nie",
      "Mingxing Li",
      "Kang Liao",
      "Xiangxiang Chu",
      "Yao Zhao"
    ],
    "published": "2025-09-04T15:58:50Z",
    "updated": "2025-09-04T15:58:50Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04338v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04338v1",
    "comment": "20pages",
    "relevance_score": 6.5
  },
  {
    "id": "2502.14791v4",
    "title": "Rapid Word Learning Through Meta In-Context Learning",
    "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
    "authors": [
      "Wentao Wang",
      "Guangyuan Jiang",
      "Tal Linzen",
      "Brenden M. Lake"
    ],
    "published": "2025-02-20T18:11:38Z",
    "updated": "2025-09-04T14:58:09Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2502.14791v4",
    "arxiv_url": "http://arxiv.org/abs/2502.14791v4",
    "comment": "EMNLP 2025",
    "relevance_score": 6.5
  },
  {
    "id": "2505.11701v2",
    "title": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior",
    "summary": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model.",
    "authors": [
      "Shaghayegh Abedi",
      "Amin Jalali"
    ],
    "published": "2025-05-16T21:09:36Z",
    "updated": "2025-09-04T14:12:36Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2505.11701v2",
    "arxiv_url": "http://arxiv.org/abs/2505.11701v2",
    "comment": "Large Language Models, Decision Model and Notation, Automated\n  Feedback, Prompt Engineering",
    "relevance_score": 6.5
  },
  {
    "id": "2509.04009v1",
    "title": "Detecting Regional Spurious Correlations in Vision Transformers via   Token Discarding",
    "summary": "Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.",
    "authors": [
      "Solha Kang",
      "Esla Timothy Anzaku",
      "Wesley De Neve",
      "Arnout Van Messem",
      "Joris Vankerschaver",
      "Francois Rameau",
      "Utku Ozbulak"
    ],
    "published": "2025-09-04T08:40:40Z",
    "updated": "2025-09-04T08:40:40Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04009v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04009v1",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2508.07809v2",
    "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
    "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.",
    "authors": [
      "Huanyu Liu",
      "Jia Li",
      "Chang Yu",
      "Taozhi Chen",
      "Yihong Dong",
      "Lecheng Wang",
      "XiaoLong Hu",
      "Ge Li"
    ],
    "published": "2025-08-11T09:49:01Z",
    "updated": "2025-09-04T15:41:36Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2508.07809v2",
    "arxiv_url": "http://arxiv.org/abs/2508.07809v2",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2505.03498v2",
    "title": "Res-MoCoDiff: Residual-guided diffusion models for motion artifact   correction in brain MRI",
    "summary": "Objective. Motion artifacts in brain MRI, mainly from rigid head motion,\ndegrade image quality and hinder downstream applications. Conventional methods\nto mitigate these artifacts, including repeated acquisitions or motion\ntracking, impose workflow burdens. This study introduces Res-MoCoDiff, an\nefficient denoising diffusion probabilistic model specifically designed for MRI\nmotion artifact correction.Approach.Res-MoCoDiff exploits a novel residual\nerror shifting mechanism during the forward diffusion process to incorporate\ninformation from motion-corrupted images. This mechanism allows the model to\nsimulate the evolution of noise with a probability distribution closely\nmatching that of the corrupted data, enabling a reverse diffusion process that\nrequires only four steps. The model employs a U-net backbone, with attention\nlayers replaced by Swin Transformer blocks, to enhance robustness across\nresolutions. Furthermore, the training process integrates a combined l1+l2 loss\nfunction, which promotes image sharpness and reduces pixel-level errors.\nRes-MoCoDiff was evaluated on both an in-silico dataset generated using a\nrealistic motion simulation framework and an in-vivo MR-ART dataset.\nComparative analyses were conducted against established methods, including\nCycleGAN, Pix2pix, and a diffusion model with a vision transformer backbone,\nusing quantitative metrics such as PSNR, SSIM, and NMSE.Main results. The\nproposed method demonstrated superior performance in removing motion artifacts\nacross minor, moderate, and heavy distortion levels. Res-MoCoDiff consistently\nachieved the highest SSIM and the lowest NMSE values, with a PSNR of up to\n41.91+-2.94 dB for minor distortions. Notably, the average sampling time was\nreduced to 0.37 seconds per batch of two image slices, compared with 101.74\nseconds for conventional approaches.",
    "authors": [
      "Mojtaba Safari",
      "Shansong Wang",
      "Qiang Li",
      "Zach Eidex",
      "Richard L. J. Qiu",
      "Chih-Wei Chang",
      "Hui Mao",
      "Xiaofeng Yang"
    ],
    "published": "2025-05-06T13:02:40Z",
    "updated": "2025-09-04T17:50:32Z",
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2505.03498v2",
    "arxiv_url": "http://arxiv.org/abs/2505.03498v2",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2509.04378v1",
    "title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs",
    "summary": "Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.",
    "authors": [
      "Yilin Tao",
      "Jiashui Huang",
      "Huaze Xu",
      "Ling Shao"
    ],
    "published": "2025-09-04T16:40:15Z",
    "updated": "2025-09-04T16:40:15Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04378v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04378v1",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2503.20652v5",
    "title": "Imitating Radiological Scrolling: A Global-Local Attention Model for 3D   Chest CT Volumes Multi-Label Anomaly Classification",
    "summary": "The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.",
    "authors": [
      "Theo Di Piazza",
      "Carole Lazarus",
      "Olivier Nempont",
      "Loic Boussel"
    ],
    "published": "2025-03-26T15:47:50Z",
    "updated": "2025-09-04T13:19:09Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2503.20652v5",
    "arxiv_url": "http://arxiv.org/abs/2503.20652v5",
    "comment": "13 pages, 4 figures. Accepted for publication at MIDL 2025",
    "relevance_score": 6.5
  },
  {
    "id": "2509.04123v1",
    "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
    "summary": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.",
    "authors": [
      "Ayan Banerjee",
      "Josep Lladós",
      "Umapada Pal",
      "Anjan Dutta"
    ],
    "published": "2025-09-04T11:37:06Z",
    "updated": "2025-09-04T11:37:06Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04123v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04123v1",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2509.03951v1",
    "title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD   Detection",
    "summary": "The introduction of negative labels (NLs) has proven effective in enhancing\nOut-of-Distribution (OOD) detection. However, existing methods often lack an\nunderstanding of OOD images, making it difficult to construct an accurate\nnegative space. In addition, the presence of false negative labels\nsignificantly degrades their near-OOD performance. To address these issues, we\npropose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the\nunderstanding and reasoning capabilities of multimodal large language models\n(MLLMs). Specifically, we identify images likely to be OOD samples as negative\nimages and prompt the MLLM to describe these images, generating expressive\nnegative sentences that precisely characterize the OOD distribution and enhance\nfar-OOD detection. For the near-OOD setting, where OOD samples resemble the\nin-distribution (ID) subset, we first identify the subset of ID classes that\nare visually similar to negative images and then leverage the reasoning\ncapability of MLLMs to generate visually similar negative labels tailored to\nthis subset, effectively reducing false negatives and improving near-OOD\ndetection. To balance these two types of negative textual spaces, we design an\nadaptive weighted score that enables the method to handle different OOD task\nsettings (near-OOD and far-OOD) without relying on task-specific prior\nknowledge, making it highly adaptable in open environments. On the ImageNet\nbenchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a\nnew state-of-the-art. Furthermore, our method is training-free and zero-shot,\nenabling high scalability.",
    "authors": [
      "Zhu Wenjie",
      "Zhang Yabin",
      "Xin Jin",
      "Wenjun Zeng",
      "Lei Zhang"
    ],
    "published": "2025-09-04T07:26:20Z",
    "updated": "2025-09-04T07:26:20Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03951v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03951v1",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2509.03903v1",
    "title": "A Generative Foundation Model for Chest Radiography",
    "summary": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.",
    "authors": [
      "Yuanfeng Ji",
      "Dan Lin",
      "Xiyue Wang",
      "Lu Zhang",
      "Wenhui Zhou",
      "Chongjian Ge",
      "Ruihang Chu",
      "Xiaoli Yang",
      "Junhan Zhao",
      "Junsong Chen",
      "Xiangde Luo",
      "Sen Yang",
      "Jin Fang",
      "Ping Luo",
      "Ruijiang Li"
    ],
    "published": "2025-09-04T05:53:58Z",
    "updated": "2025-09-04T05:53:58Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03903v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03903v1",
    "comment": null,
    "relevance_score": 6.5
  },
  {
    "id": "2509.04059v1",
    "title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement   Learning",
    "summary": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation.",
    "authors": [
      "Zhilin Wang",
      "Zhe Yang",
      "Yun Luo",
      "Yafu Li",
      "Haoran Zhang",
      "Runzhe Zhan",
      "Derek F. Wong",
      "Jizhe Zhou",
      "Yu Cheng"
    ],
    "published": "2025-09-04T09:42:17Z",
    "updated": "2025-09-04T09:42:17Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04059v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04059v1",
    "comment": "11 pages",
    "relevance_score": 6.5
  },
  {
    "id": "2411.12736v2",
    "title": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs",
    "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.",
    "authors": [
      "Salma Kharrat",
      "Fares Fourati",
      "Marco Canini"
    ],
    "published": "2024-11-19T18:58:03Z",
    "updated": "2025-09-04T17:56:24Z",
    "categories": [
      "cs.CL",
      "cs.SY",
      "math.OC",
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2411.12736v2",
    "arxiv_url": "http://arxiv.org/abs/2411.12736v2",
    "comment": "Accepted at EMNLP 2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04310v1",
    "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation",
    "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
    "authors": [
      "Yunbo Long",
      "Liming Xu",
      "Lukas Beckenbauer",
      "Yuhan Liu",
      "Alexandra Brintrup"
    ],
    "published": "2025-09-04T15:23:58Z",
    "updated": "2025-09-04T15:23:58Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04310v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04310v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2312.03993v2",
    "title": "Style Transfer to Calvin and Hobbes comics using Stable Diffusion",
    "summary": "This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.",
    "authors": [
      "Asvin Kumar Venkataramanan",
      "Sloke Shrestha",
      "Sundar Sripada Venugopalaswamy Sriraman"
    ],
    "published": "2023-12-07T02:21:31Z",
    "updated": "2025-09-04T14:45:07Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2312.03993v2",
    "arxiv_url": "http://arxiv.org/abs/2312.03993v2",
    "comment": "Updated authorship",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04139v1",
    "title": "Enhancing Technical Documents Retrieval for RAG",
    "summary": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.",
    "authors": [
      "Songjiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kaiwen Xue",
      "Kwan-Ho Lin",
      "Yan-Ming Choi",
      "Vincent Ng",
      "Kin-Man Lam"
    ],
    "published": "2025-09-04T12:11:03Z",
    "updated": "2025-09-04T12:11:03Z",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04139v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04139v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04126v1",
    "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation",
    "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
    "authors": [
      "Yuan Zhao",
      "Liu Lin"
    ],
    "published": "2025-09-04T11:44:28Z",
    "updated": "2025-09-04T11:44:28Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04126v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04126v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2504.05774v2",
    "title": "Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation",
    "summary": "Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available.",
    "authors": [
      "Jianhua Liu",
      "Zhengyu Li",
      "Yanru Wu",
      "Jingge Wang",
      "Yang Tan",
      "Ruizhe Zhao",
      "Guan Wang",
      "Yang Li"
    ],
    "published": "2025-04-08T07:53:51Z",
    "updated": "2025-09-04T10:49:27Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2504.05774v2",
    "arxiv_url": "http://arxiv.org/abs/2504.05774v2",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.01221v2",
    "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for   Fine-tuning Domain Tasks Based on Data and Model Compression",
    "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.",
    "authors": [
      "Wei Huang",
      "Huang Wei",
      "Yinggui Wang"
    ],
    "published": "2025-09-01T08:06:49Z",
    "updated": "2025-09-04T09:30:16Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.01221v2",
    "arxiv_url": "http://arxiv.org/abs/2509.01221v2",
    "comment": "Accepted by EMNLP 2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04027v1",
    "title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via   Reinforcement Learning",
    "summary": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.",
    "authors": [
      "Zeyu Gan",
      "Hao Yi",
      "Yong Liu"
    ],
    "published": "2025-09-04T09:02:16Z",
    "updated": "2025-09-04T09:02:16Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04027v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04027v1",
    "comment": "Preprint Edition",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04011v1",
    "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware   Embeddings",
    "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
    "authors": [
      "Or Shachar",
      "Uri Katz",
      "Yoav Goldberg",
      "Oren Glickman"
    ],
    "published": "2025-09-04T08:42:23Z",
    "updated": "2025-09-04T08:42:23Z",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04011v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04011v1",
    "comment": "Findings of EMNLP 2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.03986v1",
    "title": "Promptception: How Sensitive Are Large Multimodal Models to Prompts?",
    "summary": "Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.",
    "authors": [
      "Mohamed Insaf Ismithdeen",
      "Muhammad Uzair Khattak",
      "Salman Khan"
    ],
    "published": "2025-09-04T08:13:06Z",
    "updated": "2025-09-04T08:13:06Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03986v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03986v1",
    "comment": "Accepted to EMNLP 2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04259v1",
    "title": "RL's Razor: Why Online Reinforcement Learning Forgets Less",
    "summary": "Comparison of fine-tuning models with reinforcement learning (RL) and\nsupervised fine-tuning (SFT) reveals that, despite similar performance at a new\ntask, RL preserves prior knowledge and capabilities significantly better. We\nfind that the degree of forgetting is determined by the distributional shift,\nmeasured as the KL-divergence between the fine-tuned and base policy evaluated\non the new task. Our analysis reveals that on-policy RL is implicitly biased\ntowards KL-minimal solutions among the many that solve the new task, whereas\nSFT can converge to distributions arbitrarily far from the base model. We\nvalidate these findings through experiments with large language models and\nrobotic foundation models and further provide theoretical justification for why\non-policy RL updates lead to a smaller KL change. We term this principle\n$\\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those\nclosest in KL to the original model.",
    "authors": [
      "Idan Shenfeld",
      "Jyothish Pari",
      "Pulkit Agrawal"
    ],
    "published": "2025-09-04T14:38:08Z",
    "updated": "2025-09-04T14:38:08Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04259v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04259v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04245v1",
    "title": "Synthetic Survival Data Generation for Heart Failure Prognosis Using   Deep Generative Models",
    "summary": "Background: Heart failure (HF) research is constrained by limited access to\nlarge, shareable datasets due to privacy regulations and institutional\nbarriers. Synthetic data generation offers a promising solution to overcome\nthese challenges while preserving patient confidentiality. Methods: We\ngenerated synthetic HF datasets from institutional data comprising 12,552\nunique patients using five deep learning models: tabular variational\nautoencoder (TVAE), normalizing flow, ADSGAN, SurvivalGAN, and tabular\ndenoising diffusion probabilistic models (TabDDPM). We comprehensively\nevaluated synthetic data utility through statistical similarity metrics,\nsurvival prediction using machine learning and privacy assessments. Results:\nSurvivalGAN and TabDDPM demonstrated high fidelity to the original dataset,\nexhibiting similar variable distributions and survival curves after applying\nhistogram equalization. SurvivalGAN (C-indices: 0.71-0.76) and TVAE (C-indices:\n0.73-0.76) achieved the strongest performance in survival prediction\nevaluation, closely matched real data performance (C-indices: 0.73-0.76).\nPrivacy evaluation confirmed protection against re-identification attacks.\nConclusions: Deep learning-based synthetic data generation can produce\nhigh-fidelity, privacy-preserving HF datasets suitable for research\napplications. This publicly available synthetic dataset addresses critical data\nsharing barriers and provides a valuable resource for advancing HF research and\npredictive modeling.",
    "authors": [
      "Chanon Puttanawarut",
      "Natcha Fongsrisin",
      "Porntep Amornritvanich",
      "Cholatid Ratanatharathorn",
      "Panu Looareesuwan"
    ],
    "published": "2025-09-04T14:17:58Z",
    "updated": "2025-09-04T14:17:58Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04245v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04245v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04213v1",
    "title": "Sailing Towards Zero-Shot State Estimation using Foundation Models   Combined with a UKF",
    "summary": "State estimation in control and systems engineering traditionally requires\nextensive manual system identification or data-collection effort. However,\ntransformer-based foundation models in other domains have reduced data\nrequirements by leveraging pre-trained generalist models. Ultimately,\ndeveloping zero-shot foundation models of system dynamics could drastically\nreduce manual deployment effort. While recent work shows that transformer-based\nend-to-end approaches can achieve zero-shot performance on unseen systems, they\nare limited to sensor models seen during training. We introduce the foundation\nmodel unscented Kalman filter (FM-UKF), which combines a transformer-based\nmodel of system dynamics with analytically known sensor models via an UKF,\nenabling generalization across varying dynamics without retraining for new\nsensor configurations. We evaluate FM-UKF on a new benchmark of container ship\nmodels with complex dynamics, demonstrating a competitive accuracy, effort, and\nrobustness trade-off compared to classical methods with approximate system\nknowledge and to an end-to-end approach. The benchmark and dataset are open\nsourced to further support future research in zero-shot state estimation via\nfoundation models.",
    "authors": [
      "Tobin Holtmann",
      "David Stenger",
      "Andres Posada-Moreno",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ],
    "published": "2025-09-04T13:38:54Z",
    "updated": "2025-09-04T13:38:54Z",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04213v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04213v1",
    "comment": "Accepted for publication at CDC2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04210v1",
    "title": "COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic   Obesity Management via Wrist-Worn Activity Monitoring",
    "summary": "Chronic obesity management requires continuous monitoring of energy balance\nbehaviors, yet traditional self-reported methods suffer from significant\nunderreporting and recall bias, and difficulty in integration with modern\ndigital health systems. This study presents COBRA (Chronic Obesity Behavioral\nRecognition Architecture), a novel deep learning framework for objective\nbehavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a\nhybrid D-Net architecture combining U-Net spatial modeling, multi-head\nself-attention mechanisms, and BiLSTM temporal processing to classify daily\nactivities into four obesity-relevant categories: Food Intake, Physical\nActivity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart\ndataset with 51 subjects performing 18 activities, COBRA's optimal\npreprocessing strategy combines spectral-temporal feature extraction, achieving\nhigh performance across multiple architectures. D-Net demonstrates 96.86%\noverall accuracy with category-specific F1-scores of 98.55% (Physical\nActivity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily\nLiving), outperforming state-of-the-art baselines by 1.18% in accuracy. The\nframework shows robust generalizability with low demographic variance (<3%),\nenabling scalable deployment for personalized obesity interventions and\ncontinuous lifestyle monitoring.",
    "authors": [
      "Zhengyang Shen",
      "Bo Gao",
      "Mayue Shi"
    ],
    "published": "2025-09-04T13:35:49Z",
    "updated": "2025-09-04T13:35:49Z",
    "categories": [
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04210v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04210v1",
    "comment": "19 pages, 4 figures. *Correspondence: m.shi16@imperial.ac.uk.\n  Accepted by the IUPESM World Congress on Medical Physics and Biomedical\n  Engineering 2025",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04403v1",
    "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety   Scenarios",
    "summary": "Multimodal large language models (MLLMs) are rapidly evolving, presenting\nincreasingly complex safety challenges. However, current dataset construction\nmethods, which are risk-oriented, fail to cover the growing complexity of\nreal-world multimodal safety scenarios (RMS). And due to the lack of a unified\nevaluation metric, their overall effectiveness remains unproven. This paper\nintroduces a novel image-oriented self-adaptive dataset construction method for\nRMS, which starts with images and end constructing paired text and guidance\nresponses. Using the image-oriented method, we automatically generate an RMS\ndataset comprising 35k image-text pairs with guidance responses. Additionally,\nwe introduce a standardized safety dataset evaluation metric: fine-tuning a\nsafety judge model and evaluating its capabilities on other safety\ndatasets.Extensive experiments on various tasks demonstrate the effectiveness\nof the proposed image-oriented pipeline. The results confirm the scalability\nand effectiveness of the image-oriented approach, offering a new perspective\nfor the construction of real-world multimodal safety datasets.",
    "authors": [
      "Jingen Qu",
      "Lijun Li",
      "Bo Zhang",
      "Yichen Yan",
      "Jing Shao"
    ],
    "published": "2025-09-04T17:13:59Z",
    "updated": "2025-09-04T17:13:59Z",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04403v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04403v1",
    "comment": "Accepted at EMNLP 2025 Findings",
    "relevance_score": 5.5
  },
  {
    "id": "2509.04376v1",
    "title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval   for Text-Based Person Anomaly Search",
    "summary": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.",
    "authors": [
      "Hao Ju",
      "Hu Zhang",
      "Zhedong Zheng"
    ],
    "published": "2025-09-04T16:34:46Z",
    "updated": "2025-09-04T16:34:46Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04376v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04376v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04324v1",
    "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent   Detection",
    "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.",
    "authors": [
      "Chen Hu",
      "Shan Luo",
      "Letizia Gionfrida"
    ],
    "published": "2025-09-04T15:42:36Z",
    "updated": "2025-09-04T15:42:36Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04324v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04324v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2409.06002v5",
    "title": "Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance",
    "summary": "Data augmentation is crucial for pixel-wise annotation tasks like semantic\nsegmentation, where labeling requires significant effort and intensive labor.\nTraditional methods, involving simple transformations such as rotations and\nflips, create new images but often lack diversity along key semantic dimensions\nand fail to alter high-level semantic properties. To address this issue,\ngenerative models have emerged as an effective solution for augmenting data by\ngenerating synthetic images. Controllable Generative models offer data\naugmentation methods for semantic segmentation tasks by using prompts and\nvisual references from the original image. However, these models face\nchallenges in generating synthetic images that accurately reflect the content\nand structure of the original image due to difficulties in creating effective\nprompts and visual references. In this work, we introduce an effective data\naugmentation pipeline for semantic segmentation using Controllable Diffusion\nmodel. Our proposed method includes efficient prompt generation using\nClass-Prompt Appending and Visual Prior Blending to enhance attention to\nlabeled classes in real images, allowing the pipeline to generate a precise\nnumber of augmented images while preserving the structure of\nsegmentation-labeled classes. In addition, we implement a class balancing\nalgorithm to ensure a balanced training dataset when merging the synthetic and\noriginal images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates\nits effectiveness in generating high-quality synthetic images for semantic\nsegmentation. Our code is available at\nhttps://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance.",
    "authors": [
      "Quang-Huy Che",
      "Duc-Tri Le",
      "Bich-Nga Pham",
      "Duc-Khai Lam",
      "Vinh-Tiep Nguyen"
    ],
    "published": "2024-09-09T19:01:14Z",
    "updated": "2025-09-04T09:32:46Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2409.06002v5",
    "arxiv_url": "http://arxiv.org/abs/2409.06002v5",
    "comment": "Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313",
    "relevance_score": 5.5
  },
  {
    "id": "2509.03897v1",
    "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
    "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.",
    "authors": [
      "Xiaofu Chen",
      "Israfel Salazar",
      "Yova Kementchedjhieva"
    ],
    "published": "2025-09-04T05:43:50Z",
    "updated": "2025-09-04T05:43:50Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03897v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03897v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.03312v2",
    "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
    "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
    "authors": [
      "Guibin Zhang",
      "Junhao Wang",
      "Junjie Chen",
      "Wangchunshu Zhou",
      "Kun Wang",
      "Shuicheng Yan"
    ],
    "published": "2025-09-03T13:42:14Z",
    "updated": "2025-09-04T17:49:20Z",
    "categories": [
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03312v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03312v2",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04373v1",
    "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature   of LLM Gender Biases",
    "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.",
    "authors": [
      "Bufan Gao",
      "Elisa Kreiss"
    ],
    "published": "2025-09-04T16:32:18Z",
    "updated": "2025-09-04T16:32:18Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04373v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04373v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.04077v1",
    "title": "Improving Narrative Classification and Explanation via Fine Tuned   Language Models",
    "summary": "Understanding covert narratives and implicit messaging is essential for\nanalyzing bias and sentiment. Traditional NLP methods struggle with detecting\nsubtle phrasing and hidden agendas. This study tackles two key challenges: (1)\nmulti-label classification of narratives and sub-narratives in news articles,\nand (2) generating concise, evidence-based explanations for dominant\nnarratives. We fine-tune a BERT model with a recall-oriented approach for\ncomprehensive narrative detection, refining predictions using a GPT-4o pipeline\nfor consistency. For narrative explanation, we propose a ReACT (Reasoning +\nActing) framework with semantic retrieval-based few-shot prompting, ensuring\ngrounded and relevant justifications. To enhance factual accuracy and reduce\nhallucinations, we incorporate a structured taxonomy table as an auxiliary\nknowledge base. Our results show that integrating auxiliary knowledge in\nprompts improves classification accuracy and justification reliability, with\napplications in media analysis, education, and intelligence gathering.",
    "authors": [
      "Rishit Tyagi",
      "Rahul Bouri",
      "Mohit Gupta"
    ],
    "published": "2025-09-04T10:12:31Z",
    "updated": "2025-09-04T10:12:31Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04077v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04077v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2508.19740v2",
    "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear   Hashing-based KV Cache Retrieval",
    "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
    "authors": [
      "Wenhao Li",
      "Yuxin Zhang",
      "Gen Luo",
      "Haiyuan Wan",
      "Ziyang Gong",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "published": "2025-08-27T10:11:27Z",
    "updated": "2025-09-04T09:08:29Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2508.19740v2",
    "arxiv_url": "http://arxiv.org/abs/2508.19740v2",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.03962v1",
    "title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting",
    "summary": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language.",
    "authors": [
      "Ulin Nuha",
      "Adam Jatowt"
    ],
    "published": "2025-09-04T07:41:23Z",
    "updated": "2025-09-04T07:41:23Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03962v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03962v1",
    "comment": null,
    "relevance_score": 5.5
  },
  {
    "id": "2509.03937v1",
    "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by   Self-Play Fine-Tuning",
    "summary": "Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.",
    "authors": [
      "Yuhao Zhang",
      "Shaoming Duan",
      "Jinhang Su",
      "Chuanyi Liu",
      "Peiyi Han"
    ],
    "published": "2025-09-04T06:55:46Z",
    "updated": "2025-09-04T06:55:46Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03937v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03937v1",
    "comment": "EMNLP 2025 Findings",
    "relevance_score": 5.5
  },
  {
    "id": "2509.03867v1",
    "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
    "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Chia-Yi Hsiao",
      "Zi Yan Chang",
      "Chi-Li Chen",
      "Tyler Loakman",
      "Chenghua Lin"
    ],
    "published": "2025-09-04T03:58:55Z",
    "updated": "2025-09-04T03:58:55Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03867v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03867v1",
    "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference",
    "relevance_score": 5.5
  },
  {
    "id": "2506.05997v2",
    "title": "Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation   via End-to-End Reinforcement Learning",
    "summary": "Recent advancements in robot navigation, particularly with end-to-end\nlearning approaches such as reinforcement learning (RL), have demonstrated\nstrong performance. However, successful navigation still depends on two key\ncapabilities: mapping and planning (explicitly or implicitly). Classical\napproaches rely on explicit mapping pipelines to register egocentric\nobservations into a coherent map. In contrast, end-to-end learning often\nachieves this implicitly -- through recurrent neural networks (RNNs) that fuse\ncurrent and historical observations into a latent space for planning. While\nexisting architectures, such as LSTM and GRU, can capture temporal\ndependencies, our findings reveal a critical limitation: their inability to\neffectively perform spatial memorization. This capability is essential for\nintegrating sequential observations from varying perspectives to build spatial\nrepresentations that support planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective\nmodification to existing RNNs -- that enhance spatial memorization. We further\nintroduce an attention-based network architecture integrated with SRUs,\nenabling long-range mapless navigation using a single forward-facing stereo\ncamera. We also employ regularization techniques to facilitate robust\nend-to-end recurrent training via RL. Experimental results show 23.5% overall\nimprovement in long-range navigation compared to existing RNNs. With SRU\nmemory, our method outperforms RL baselines -- one relying on explicit mapping\nand the other on stacked historical observations -- by 29.6% and 105.0%,\nrespectively, across diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer for deployment\nacross diverse and complex real-world environments.",
    "authors": [
      "Fan Yang",
      "Per Frivik",
      "David Hoeller",
      "Chen Wang",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-06-06T11:35:48Z",
    "updated": "2025-09-04T14:30:12Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2506.05997v2",
    "arxiv_url": "http://arxiv.org/abs/2506.05997v2",
    "comment": "22 pages",
    "relevance_score": 5.0
  },
  {
    "id": "2509.04442v1",
    "title": "Delta Activations: A Representation for Finetuned Large Language Models",
    "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
    "authors": [
      "Zhiqiu Xu",
      "Amish Sethi",
      "Mayur Naik",
      "Ser-Nam Lim"
    ],
    "published": "2025-09-04T17:59:06Z",
    "updated": "2025-09-04T17:59:06Z",
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04442v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04442v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04439v1",
    "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
    "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.",
    "authors": [
      "Matthew Ho",
      "Chen Si",
      "Zhaoxiang Feng",
      "Fangxu Yu",
      "Zhijian Liu",
      "Zhiting Hu",
      "Lianhui Qin"
    ],
    "published": "2025-09-04T17:54:19Z",
    "updated": "2025-09-04T17:54:19Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04439v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04439v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04419v1",
    "title": "Towards a Unified View of Large Language Model Post-Training",
    "summary": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
    "authors": [
      "Xingtai Lv",
      "Yuxin Zuo",
      "Youbang Sun",
      "Hongyi Liu",
      "Yuntian Wei",
      "Zhekai Chen",
      "Lixuan He",
      "Xuekai Zhu",
      "Kaiyan Zhang",
      "Bingning Wang",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-09-04T17:40:33Z",
    "updated": "2025-09-04T17:40:33Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04419v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04419v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.01185v2",
    "title": "Modular Techniques for Synthetic Long-Context Data Generation in   Language Model Training and Evaluation",
    "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.",
    "authors": [
      "Seganrasan Subramanian",
      "Abhigya Verma"
    ],
    "published": "2025-09-01T07:08:45Z",
    "updated": "2025-09-04T17:22:16Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.01185v2",
    "arxiv_url": "http://arxiv.org/abs/2509.01185v2",
    "comment": "26 pages, 4 figures",
    "relevance_score": 4.5
  },
  {
    "id": "2509.04343v1",
    "title": "Psychologically Enhanced AI Agents",
    "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
    "authors": [
      "Maciej Besta",
      "Shriram Chandran",
      "Robert Gerstenberger",
      "Mathis Lindner",
      "Marcin Chrapek",
      "Sebastian Hermann Martschat",
      "Taraneh Ghandi",
      "Patrick Iff",
      "Hubert Niewiadomski",
      "Piotr Nyczyk",
      "Jürgen Müller",
      "Torsten Hoefler"
    ],
    "published": "2025-09-04T16:03:03Z",
    "updated": "2025-09-04T16:03:03Z",
    "categories": [
      "cs.MA",
      "cs.CL",
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04343v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04343v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.02349v2",
    "title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity.",
    "authors": [
      "Lu Wang",
      "Hao Chen",
      "Siyu Wu",
      "Zhiyue Wu",
      "Hao Zhou",
      "Chengfeng Zhang",
      "Ting Wang",
      "Haodi Zhang"
    ],
    "published": "2025-09-02T14:15:22Z",
    "updated": "2025-09-04T14:25:57Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.02349v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02349v2",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2508.01700v2",
    "title": "DeepVIS: Bridging Natural Language and Data Visualization Through   Step-wise Reasoning",
    "summary": "Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.",
    "authors": [
      "Zhihao Shuai",
      "Boyan Li",
      "Siyu Yan",
      "Yuyu Luo",
      "Weikai Yang"
    ],
    "published": "2025-08-03T10:04:17Z",
    "updated": "2025-09-04T12:50:20Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.01700v2",
    "arxiv_url": "http://arxiv.org/abs/2508.01700v2",
    "comment": "IEEE VIS 2025 full paper",
    "relevance_score": 4.5
  },
  {
    "id": "2506.06052v2",
    "title": "CP-Bench: Evaluating Large Language Models for Constraint Modelling",
    "summary": "Constraint Programming (CP) is widely used to solve combinatorial problems,\nbut its core process, namely constraint modelling, requires significant\nexpertise and is considered to be a bottleneck for wider adoption. Aiming to\nalleviate this bottleneck, recent studies have explored using Large Language\nModels (LLMs) to transform combinatorial problem descriptions into executable\nconstraint models. However, the existing evaluation datasets for constraint\nmodelling are often limited to small, homogeneous, or domain-specific\ninstances, which do not capture the diversity of real-world scenarios. This\nwork addresses this gap by introducing CP-Bench, a novel benchmark that\nincludes a diverse set of well-known combinatorial problems sourced from the CP\ncommunity, structured explicitly for evaluating LLM-driven CP modelling. With\nthis dataset, and given the variety of constraint modelling frameworks, we\ncompare and evaluate the modelling capabilities of LLMs for three distinct\nconstraint modelling systems, which vary in abstraction level and underlying\nsyntax. Notably, the results show higher performance when modelling with a\nhigh-level Python-based framework. Additionally, we systematically evaluate the\nuse of prompt-based and inference-time compute methods across different LLMs,\nwhich further increase accuracy, reaching up to 70% on this highly challenging\nbenchmark.",
    "authors": [
      "Kostis Michailidis",
      "Dimos Tsouros",
      "Tias Guns"
    ],
    "published": "2025-06-06T12:56:02Z",
    "updated": "2025-09-04T09:10:05Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2506.06052v2",
    "arxiv_url": "http://arxiv.org/abs/2506.06052v2",
    "comment": "ECAI 25",
    "relevance_score": 4.5
  },
  {
    "id": "2509.03990v1",
    "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility   for Resource-Efficient LLM Agent",
    "summary": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions.",
    "authors": [
      "Chunlong Wu",
      "Zhibo Qu"
    ],
    "published": "2025-09-04T08:18:39Z",
    "updated": "2025-09-04T08:18:39Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03990v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03990v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.03972v1",
    "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through   a Korean Case Study",
    "summary": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.",
    "authors": [
      "Junghwan Lim",
      "Gangwon Jo",
      "Sungmin Lee",
      "Jiyoung Park",
      "Dongseok Kim",
      "Jihwan Kim",
      "Junhyeok Lee",
      "Wai Ting Cheung",
      "Dahye Choi",
      "Kibong Choi",
      "Jaeyeon Huh",
      "Beomgyu Kim",
      "Jangwoong Kim",
      "Taehyun Kim",
      "Haesol Lee",
      "Jeesoo Lee",
      "Dongpin Oh",
      "Changseok Song",
      "Daewon Suh"
    ],
    "published": "2025-09-04T07:56:24Z",
    "updated": "2025-09-04T07:56:24Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03972v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03972v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04377v1",
    "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient   Large Language Model Inference",
    "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
    "authors": [
      "Krishna Teja Chitty-Venkata",
      "Jie Ye",
      "Xian-He Sun",
      "Anthony Kougkas",
      "Murali Emani",
      "Venkatram Vishwanath",
      "Bogdan Nicolae"
    ],
    "published": "2025-09-04T16:40:01Z",
    "updated": "2025-09-04T16:40:01Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04377v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04377v1",
    "comment": "Preprint",
    "relevance_score": 4.5
  },
  {
    "id": "2509.04372v1",
    "title": "Connections between reinforcement learning with feedback,test-time   scaling, and diffusion guidance: An anthology",
    "summary": "In this note, we reflect on several fundamental connections among widely used\npost-training techniques. We clarify some intimate connections and equivalences\nbetween reinforcement learning with human feedback, reinforcement learning with\ninternal feedback, and test-time scaling (particularly soft best-of-$N$\nsampling), while also illuminating intrinsic links between diffusion guidance\nand test-time scaling. Additionally, we introduce a resampling approach for\nalignment and reward-directed diffusion models, sidestepping the need for\nexplicit reinforcement learning techniques.",
    "authors": [
      "Yuchen Jiao",
      "Yuxin Chen",
      "Gen Li"
    ],
    "published": "2025-09-04T16:29:38Z",
    "updated": "2025-09-04T16:29:38Z",
    "categories": [
      "cs.GL",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04372v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04372v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.02846v2",
    "title": "Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven   Inference-Time-Scaling Algorithm",
    "summary": "Partial Differential Equations (PDEs) are the bedrock for modern\ncomputational sciences and engineering, and inherently computationally\nexpensive. While PDE foundation models have shown much promise for simulating\nsuch complex spatio-temporal phenomena, existing models remain constrained by\nthe pretraining datasets and struggle with auto-regressive rollout performance,\nespecially in out-of-distribution (OOD) cases. Furthermore, they have\nsignificant compute and training data requirements which hamper their use in\nmany critical applications. Inspired by recent advances in ``thinking\"\nstrategies used in large language models (LLMs), we introduce the first\ntest-time computing (TTC) strategy for PDEs that utilizes computational\nresources during inference to achieve more accurate predictions with fewer\ntraining samples and smaller models. We accomplish this with two types of\nreward models that evaluate predictions of a stochastic based model for\nspatio-temporal consistency. We demonstrate this method on compressible\nEuler-equation simulations from the PDEGym benchmark and show that TTC captures\nimproved predictions relative to standard non-adaptive auto-regressive\ninference. This TTC framework marks a foundational step towards more advanced\nreasoning algorithms or PDE modeling, inluding building\nreinforcement-learning-based approaches, potentially transforming computational\nworkflows in physics and engineering.",
    "authors": [
      "Siddharth Mansingh",
      "James Amarel",
      "Ragib Arnab",
      "Arvind Mohan",
      "Kamaljeet Singh",
      "Gerd J. Kunde",
      "Nicolas Hengartner",
      "Benjamin Migliori",
      "Emily Casleton",
      "Nathan A. Debardeleben",
      "Ayan Biswas",
      "Diane Oyen",
      "Earl Lawrence"
    ],
    "published": "2025-09-02T21:31:32Z",
    "updated": "2025-09-04T15:40:37Z",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.02846v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02846v2",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2501.14701v2",
    "title": "An Unsupervised Natural Language Processing Pipeline for Assessing   Referral Appropriateness",
    "summary": "Objective: Assessing the appropriateness of diagnostic referrals is critical\nfor improving healthcare efficiency and reducing unnecessary procedures.\nHowever, this task becomes challenging when referral reasons are recorded only\nas free text rather than structured codes, like in the Italian NHS. To address\nthis gap, we propose a fully unsupervised Natural Language Processing (NLP)\npipeline capable of extracting and evaluating referral reasons without relying\non labelled datasets.\n  Methods: Our pipeline leverages Transformer-based embeddings pre-trained on\nItalian medical texts to cluster referral reasons and assess their alignment\nwith appropriateness guidelines. It operates in an unsupervised setting and is\ndesigned to generalize across different examination types. We analyzed two\ncomplete regional datasets from the Lombardy Region (Italy), covering all\nreferrals between 2019 and 2021 for venous echocolordoppler of the lower limbs\n(ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC;\nn=407,949; testing only). For both, a random sample of 1,000 referrals was\nmanually annotated to measure performance.\n  Results: The pipeline achieved high performance in identifying referral\nreasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and\nappropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96%\n(FEC)). At the regional level, the analysis identified relevant inappropriate\nreferral groups and variation across contexts, findings that informed a new\nLombardy Region resolution to reinforce guideline adherence.\n  Conclusions: This study presents a robust, scalable, unsupervised NLP\npipeline for assessing referral appropriateness in large, real-world datasets.\nIt demonstrates how such data can be effectively leveraged, providing public\nhealth authorities with a deployable AI tool to monitor practices and support\nevidence-based policy.",
    "authors": [
      "Vittorio Torri",
      "Annamaria Bottelli",
      "Michele Ercolanoni",
      "Olivia Leoni",
      "Francesca Ieva"
    ],
    "published": "2025-01-24T18:24:16Z",
    "updated": "2025-09-04T15:11:24Z",
    "categories": [
      "I.2.7; J.1; J.3",
      "68T50",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2501.14701v2",
    "arxiv_url": "http://arxiv.org/abs/2501.14701v2",
    "comment": "49 pages, 10 figures",
    "relevance_score": 4.5
  },
  {
    "id": "2311.16507v2",
    "title": "Straighter Flow Matching via a Diffusion-Based Coupling Prior",
    "summary": "Flow matching as a paradigm of generative model achieves notable success\nacross various domains. However, existing methods use either multi-round\ntraining or knowledge within minibatches, posing challenges in finding a\nfavorable coupling strategy for straightening trajectories to few-step\ngeneration. To address this issue, we propose a novel approach, Straighter\ntrajectories of Flow Matching (StraightFM). It straightens trajectories with\nthe coupling strategy from the entire distribution level. More specifically,\nduring training, StraightFM creates couplings of images and noise via one\ndiffusion model as a coupling prior to straighten trajectories for few-step\ngeneration. Our coupling strategy can also integrate with the existing coupling\ndirection from real data to noise, improving image quality in few-step\ngeneration. Experimental results on pixel space and latent space show that\nStraightFM yields attractive samples within 5 steps. Moreover, our\nunconditional StraightFM is seamlessly compatible with training-free multimodal\nconditional generation, maintaining high-quality image generation in few steps.",
    "authors": [
      "Siyu Xing",
      "Jie Cao",
      "Huaibo Huang",
      "Haichao Shi",
      "Xiao-Yu Zhang"
    ],
    "published": "2023-11-28T06:19:30Z",
    "updated": "2025-09-04T14:24:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2311.16507v2",
    "arxiv_url": "http://arxiv.org/abs/2311.16507v2",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04208v1",
    "title": "One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a   Model Zoo",
    "summary": "The proliferation of Time Series Foundation Models (TSFMs) has significantly\nadvanced zero-shot forecasting, enabling predictions for unseen time series\nwithout task-specific fine-tuning. Extensive research has confirmed that no\nsingle TSFM excels universally, as different models exhibit preferences for\ndistinct temporal patterns. This diversity suggests an opportunity: how to take\nadvantage of the complementary abilities of TSFMs. To this end, we propose\nZooCast, which characterizes each model's distinct forecasting strengths.\nZooCast can intelligently assemble current TSFMs into a model zoo that\ndynamically selects optimal models for different forecasting tasks. Our key\ninnovation lies in the One-Embedding-Fits-All paradigm that constructs a\nunified representation space where each model in the zoo is represented by a\nsingle embedding, enabling efficient similarity matching for all tasks.\nExperiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shot\nforecasting benchmark while maintaining the efficiency of a single TSFM. In\nreal-world scenarios with sequential model releases, the framework seamlessly\nadds new models for progressive accuracy gains with negligible overhead.",
    "authors": [
      "Hao-Nan Shi",
      "Ting-Ji Huang",
      "Lu Han",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ],
    "published": "2025-09-04T13:34:54Z",
    "updated": "2025-09-04T13:34:54Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04208v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04208v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04191v1",
    "title": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and   Runtime Logs Analysis",
    "summary": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance.",
    "authors": [
      "Omri Sgan Cohen",
      "Ehud Malul",
      "Yair Meidan",
      "Dudu Mimran",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published": "2025-09-04T13:13:57Z",
    "updated": "2025-09-04T13:13:57Z",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04191v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04191v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04448v1",
    "title": "TRUST-VL: An Explainable News Assistant for General Multimodal   Misinformation Detection",
    "summary": "Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.",
    "authors": [
      "Zehong Yan",
      "Peng Qi",
      "Wynne Hsu",
      "Mong Li Lee"
    ],
    "published": "2025-09-04T17:59:43Z",
    "updated": "2025-09-04T17:59:43Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04448v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04448v1",
    "comment": "EMNLP 2025; Project Homepage: https://yanzehong.github.io/trust-vl/",
    "relevance_score": 4.5
  },
  {
    "id": "2509.04446v1",
    "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing   with Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.",
    "authors": [
      "Kiymet Akdemir",
      "Jing Shi",
      "Kushal Kafle",
      "Brian Price",
      "Pinar Yanardag"
    ],
    "published": "2025-09-04T17:59:34Z",
    "updated": "2025-09-04T17:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04446v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04446v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04269v1",
    "title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D   Diffusion Models",
    "summary": "Accurate quantification of tau pathology via tau positron emission tomography\n(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).\nHowever, the high cost and limited availability of tau PET restrict its\nwidespread use. In contrast, structural magnetic resonance imaging (MRI) and\nplasma-based biomarkers provide non-invasive and widely available complementary\ninformation related to brain anatomy and disease progression. In this work, we\npropose a text-guided 3D diffusion model for 3D tau PET image synthesis,\nleveraging multimodal conditions from both structural MRI and plasma\nmeasurement. Specifically, the textual prompt is from the plasma p-tau217\nmeasurement, which is a key indicator of AD progression, while MRI provides\nanatomical structure constraints. The proposed framework is trained and\nevaluated using clinical AV1451 tau PET data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. Experimental results demonstrate that\nour approach can generate realistic, clinically meaningful 3D tau PET across a\nrange of disease stages. The proposed framework can help perform tau PET data\naugmentation under different settings, provide a non-invasive, cost-effective\nalternative for visualizing tau pathology, and support the simulation of\ndisease progression under varying plasma biomarker levels and cognitive\nconditions.",
    "authors": [
      "Yuxin Gong",
      "Se-in Jang",
      "Wei Shao",
      "Yi Su",
      "Kuang Gong"
    ],
    "published": "2025-09-04T14:45:50Z",
    "updated": "2025-09-04T14:45:50Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04269v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04269v1",
    "comment": "9 pages, 4 figures, submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences",
    "relevance_score": 4.5
  },
  {
    "id": "2403.05702v2",
    "title": "Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis   from 3D OCT Imaging",
    "summary": "Glaucoma, a leading cause of irreversible blindness, necessitates early\ndetection for accurate and timely intervention to prevent irreversible vision\nloss. In this study, we present a novel deep learning framework that leverages\nthe diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for\nautomated glaucoma detection. In this framework, we integrate a pre-trained\nVision Transformer on retinal data for rich slice-wise feature extraction and a\nbidirectional Gated Recurrent Unit for capturing inter-slice spatial\ndependencies. This dual-component approach enables comprehensive analysis of\nlocal nuances and global structural integrity, crucial for accurate glaucoma\ndiagnosis. Experimental results on a large dataset demonstrate the superior\nperformance of the proposed method over state-of-the-art ones, achieving an\nF1-score of 93.01%, Matthews Correlation Coefficient (MCC) of 69.33%, and AUC\nof 94.20%. The framework's ability to leverage the valuable information in 3D\nOCT data holds significant potential for enhancing clinical decision support\nsystems and improving patient outcomes in glaucoma management.",
    "authors": [
      "Mona Ashtari-Majlan",
      "David Masip"
    ],
    "published": "2024-03-08T22:25:15Z",
    "updated": "2025-09-04T09:22:57Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2403.05702v2",
    "arxiv_url": "http://arxiv.org/abs/2403.05702v2",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2411.06119v2",
    "title": "Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures   for On-Device Image Generation",
    "summary": "Vision Transformers and U-Net architectures have been widely adopted in the\nimplementation of Diffusion Models. However, each architecture presents\nspecific challenges while realizing them on-device. Vision Transformers require\npositional embedding to maintain correspondence between the tokens processed by\nthe transformer, although they offer the advantage of using fixed-size,\nreusable repetitive blocks following tokenization. The U-Net architecture lacks\nthese attributes, as it utilizes variable-sized intermediate blocks for\ndown-convolution and up-convolution in the noise estimation backbone for the\ndiffusion process. To address these issues, we propose an architecture that\nutilizes a fixed-size, reusable transformer block as a core structure, making\nit more suitable for hardware implementation. Our architecture is characterized\nby low complexity, token-free design, absence of positional embeddings,\nuniformity, and scalability, making it highly suitable for deployment on mobile\nand resource-constrained devices. The proposed model exhibit competitive and\nconsistent performance across both unconditional and conditional image\ngeneration tasks. The model achieved a state-of-the-art FID score of 1.6 on\nunconditional image generation with the CelebA.",
    "authors": [
      "Sanchar Palit",
      "Sathya Veera Reddy Dendi",
      "Mallikarjuna Talluri",
      "Raj Narayana Gadde"
    ],
    "published": "2024-11-09T08:58:57Z",
    "updated": "2025-09-04T07:02:34Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2411.06119v2",
    "arxiv_url": "http://arxiv.org/abs/2411.06119v2",
    "comment": "presented at IJCNN 2025 poster track",
    "relevance_score": 4.5
  },
  {
    "id": "2406.01359v3",
    "title": "R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code   Completion Abilities of Code Large Language Models",
    "summary": "Code completion models have made significant progress in recent years.\nRecently, repository-level code completion has drawn more attention in modern\nsoftware development, and several baseline methods and benchmarks have been\nproposed. However, existing repository-level code completion methods often fall\nshort of fully using the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies. Besides, the existing\nbenchmarks usually focus on limited code completion scenarios, which cannot\nreflect the repository-level code completion abilities well of existing\nmethods. To address these limitations, we propose the R2C2-Coder to enhance and\nbenchmark the real-world repository-level code completion abilities of code\nLarge Language Models, where the R2C2-Coder includes a code prompt construction\nmethod R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically,\nfirst, in R2C2-Enhance, we first construct the candidate retrieval pool and\nthen assemble the completion prompt by retrieving from the retrieval pool for\neach completion cursor position. Second, based on R2C2 -Enhance, we can\nconstruct a more challenging and diverse R2C2-Bench with training, validation\nand test splits, where a context perturbation strategy is proposed to simulate\nthe real-world repository-level code completion well. Extensive results on\nmultiple benchmarks demonstrate the effectiveness of our R2C2-Coder.",
    "authors": [
      "Ken Deng",
      "Jiaheng Liu",
      "He Zhu",
      "Congnan Liu",
      "Jingxin Li",
      "Jiakai Wang",
      "Peng Zhao",
      "Chenchen Zhang",
      "Yanan Wu",
      "Xueqiao Yin",
      "Yuanxing Zhang",
      "Zizheng Zhan",
      "Wenbo Su",
      "Bangyu Xiang",
      "Tiezheng Ge",
      "Bo Zheng"
    ],
    "published": "2024-06-03T14:24:29Z",
    "updated": "2025-09-04T16:26:35Z",
    "categories": [
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2406.01359v3",
    "arxiv_url": "http://arxiv.org/abs/2406.01359v3",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2501.04316v2",
    "title": "Small Changes, Large Consequences: Analyzing the Allocational Fairness   of LLMs in Hiring Contexts",
    "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making\nremains understudied in generative and retrieval settings. In this work, we\nexamine the allocational fairness of LLM-based hiring systems through two tasks\nthat reflect actual HR usage: resume summarization and applicant ranking. By\nconstructing a synthetic resume dataset with controlled perturbations and\ncurating job postings, we investigate whether model behavior differs across\ndemographic groups. Our findings reveal that generated summaries exhibit\nmeaningful differences more frequently for race than for gender perturbations.\nModels also display non-uniform retrieval selection patterns across demographic\ngroups and exhibit high ranking sensitivity to both gender and race\nperturbations. Surprisingly, retrieval models can show comparable sensitivity\nto both demographic and non-demographic changes, suggesting that fairness\nissues may stem from broader model brittleness. Overall, our results indicate\nthat LLM-based hiring systems, especially in the retrieval stage, can exhibit\nnotable biases that lead to discriminatory outcomes in real-world contexts.",
    "authors": [
      "Preethi Seshadri",
      "Hongyu Chen",
      "Sameer Singh",
      "Seraphina Goldfarb-Tarrant"
    ],
    "published": "2025-01-08T07:28:10Z",
    "updated": "2025-09-04T15:53:10Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2501.04316v2",
    "arxiv_url": "http://arxiv.org/abs/2501.04316v2",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.04292v1",
    "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow   Real Instructions?",
    "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
    "authors": [
      "Qinyan Zhang",
      "Xinping Lei",
      "Ruijie Miao",
      "Yu Fu",
      "Haojie Fan",
      "Le Chang",
      "Jiafan Hou",
      "Dingling Zhang",
      "Zhongfei Hou",
      "Ziqiang Yang",
      "Changxin Pu",
      "Fei Hu",
      "Jingkai Liu",
      "Mengyun Liu",
      "Yang Liu",
      "Xiang Gao",
      "Jiaheng Liu",
      "Tong Yang",
      "Zaiyuan Wang",
      "Ge Zhang",
      "Wenhao Huang"
    ],
    "published": "2025-09-04T15:03:02Z",
    "updated": "2025-09-04T15:03:02Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04292v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04292v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2501.11110v4",
    "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large   Language Models via a Multi-Paradigm Perspective",
    "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks.",
    "authors": [
      "Yiyao Yu",
      "Yuxiang Zhang",
      "Dongdong Zhang",
      "Xiao Liang",
      "Hengyuan Zhang",
      "Xingxing Zhang",
      "Ziyi Yang",
      "Mahmoud Khademi",
      "Hany Awadalla",
      "Junjie Wang",
      "Yujiu Yang",
      "Furu Wei"
    ],
    "published": "2025-01-19T16:53:26Z",
    "updated": "2025-09-04T14:01:21Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2501.11110v4",
    "arxiv_url": "http://arxiv.org/abs/2501.11110v4",
    "comment": "Accepted to ACL 2025 (Main)",
    "relevance_score": 4.5
  },
  {
    "id": "2505.14585v2",
    "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning",
    "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +8.58% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
    "authors": [
      "Wenbin Hu",
      "Haoran Li",
      "Huihao Jing",
      "Qi Hu",
      "Ziqian Zeng",
      "Sirui Han",
      "Heli Xu",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-05-20T16:40:09Z",
    "updated": "2025-09-04T11:31:24Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2505.14585v2",
    "arxiv_url": "http://arxiv.org/abs/2505.14585v2",
    "comment": "Accepted to EMNLP 2025 Main",
    "relevance_score": 4.5
  },
  {
    "id": "2509.04046v1",
    "title": "A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts",
    "summary": "Systemic Functional Grammar and its branch, Cardiff Grammar, have been widely\napplied to discourse analysis, semantic function research, and other tasks\nacross various languages and texts. However, an automatic annotation system\nbased on this theory for Chinese texts has not yet been developed, which\nsignificantly constrains the application and promotion of relevant theories. To\nfill this gap, this research introduces a functional syntax annotation model\nfor Chinese based on RoBERTa (Robustly Optimized BERT Pretraining Approach).\nThe study randomly selected 4,100 sentences from the People's Daily 2014 corpus\nand annotated them according to functional syntax theory to establish a dataset\nfor training. The study then fine-tuned the RoBERTa-Chinese wwm-ext model based\non the dataset to implement the named entity recognition task, achieving an F1\nscore of 0.852 on the test set that significantly outperforms other comparative\nmodels. The model demonstrated excellent performance in identifying core\nsyntactic elements such as Subject (S), Main Verb (M), and Complement (C).\nNevertheless, there remains room for improvement in recognizing entities with\nimbalanced label samples. As the first integration of functional syntax with\nattention-based NLP models, this research provides a new method for automated\nChinese functional syntax analysis and lays a solid foundation for subsequent\nstudies.",
    "authors": [
      "Han Xiaohui",
      "Zhang Yunlong",
      "Guo Yuxi"
    ],
    "published": "2025-09-04T09:27:40Z",
    "updated": "2025-09-04T09:27:40Z",
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04046v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04046v1",
    "comment": "The paper includes 10 pages, 6 tables, and 4 figures. This project is\n  completed with the assistance of National Center for Language Technology and\n  Digital Economy Research (No. GJLX20250002), and is funded by Heilongjiang\n  Language Research Committee Project Construction of an Adaptive Intelligent\n  Chinese Learning Platform for International Students in China (No. G2025Y003)",
    "relevance_score": 4.5
  },
  {
    "id": "2509.03020v2",
    "title": "Training LLMs to be Better Text Embedders through Bidirectional   Reconstruction",
    "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales.",
    "authors": [
      "Chang Su",
      "Dengliang Shi",
      "Siyuan Huang",
      "Jintao Du",
      "Changhua Meng",
      "Yu Cheng",
      "Weiqiang Wang",
      "Zhouhan Lin"
    ],
    "published": "2025-09-03T04:55:26Z",
    "updated": "2025-09-04T08:02:20Z",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03020v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03020v2",
    "comment": "accepted by EMNLP 2025 Main Conference",
    "relevance_score": 4.5
  },
  {
    "id": "2509.03934v1",
    "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented   Generation via Distribution Self-Alignment",
    "summary": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.",
    "authors": [
      "Yuqing Huang",
      "Rongyang Zhang",
      "Qimeng Wang",
      "Chengqiang Lu",
      "Yan Gao",
      "Yi Wu",
      "Yao Hu",
      "Xuyang Zhi",
      "Guiquan Liu",
      "Xin Li",
      "Hao Wang",
      "Enhong Chen"
    ],
    "published": "2025-09-04T06:50:47Z",
    "updated": "2025-09-04T06:50:47Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03934v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03934v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2509.03918v1",
    "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question   Answering",
    "summary": "Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.",
    "authors": [
      "Fengxiao Tang",
      "Yufeng Li",
      "Zongzong Wu",
      "Ming Zhao"
    ],
    "published": "2025-09-04T06:13:28Z",
    "updated": "2025-09-04T06:13:28Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03918v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03918v1",
    "comment": null,
    "relevance_score": 4.5
  },
  {
    "id": "2505.18102v5",
    "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
    "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.",
    "authors": [
      "Takashi Ishida",
      "Thanawat Lodkaew",
      "Ikko Yamane"
    ],
    "published": "2025-05-23T16:57:34Z",
    "updated": "2025-09-04T00:35:33Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ME"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2505.18102v5",
    "arxiv_url": "http://arxiv.org/abs/2505.18102v5",
    "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models",
    "relevance_score": 4.5
  },
  {
    "id": "2509.03898v1",
    "title": "Diffusion Generative Models Meet Compressed Sensing, with Applications   to Image Data and Financial Time Series",
    "summary": "This paper develops dimension reduction techniques for accelerating diffusion\nmodel inference in the context of synthetic data generation. The idea is to\nintegrate compressed sensing into diffusion models: (i) compress the data into\na latent space, (ii) train a diffusion model in the latent space, and (iii)\napply a compressed sensing algorithm to the samples generated in the latent\nspace, facilitating the efficiency of both model training and inference. Under\nsuitable sparsity assumptions on data, the proposed algorithm is proved to\nenjoy faster convergence by combining diffusion model inference with sparse\nrecovery. As a byproduct, we obtain an optimal value for the latent space\ndimension. We also conduct numerical experiments on a range of datasets,\nincluding image data (handwritten digits, medical images, and climate data) and\nfinancial time series for stress testing.",
    "authors": [
      "Zhengyi Guo",
      "Jiatu Li",
      "Wenpin Tang",
      "David D. Yao"
    ],
    "published": "2025-09-04T05:45:06Z",
    "updated": "2025-09-04T05:45:06Z",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.03898v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03898v1",
    "comment": null,
    "relevance_score": 4.0
  },
  {
    "id": "2509.03842v1",
    "title": "INGRID: Intelligent Generative Robotic Design Using Large Language   Models",
    "summary": "The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.",
    "authors": [
      "Guanglu Jia",
      "Ceng Zhang",
      "Gregory S. Chirikjian"
    ],
    "published": "2025-09-04T03:08:01Z",
    "updated": "2025-09-04T03:08:01Z",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.03842v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03842v1",
    "comment": "15 pages, 6 figures",
    "relevance_score": 4.0
  },
  {
    "id": "2509.04340v1",
    "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing   Clinical Notes",
    "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.",
    "authors": [
      "Kristina L. Kupferschmidt",
      "Kieran O'Doherty",
      "Joshua A. Skorburg"
    ],
    "published": "2025-09-04T15:59:45Z",
    "updated": "2025-09-04T15:59:45Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.04340v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04340v1",
    "comment": null,
    "relevance_score": 4.0
  },
  {
    "id": "2509.00616v2",
    "title": "TimeCopilot",
    "summary": "We introduce TimeCopilot, the first open-source agentic framework for\nforecasting that combines multiple Time Series Foundation Models (TSFMs) with\nLarge Language Models (LLMs) through a single unified API. TimeCopilot\nautomates the forecasting pipeline: feature analysis, model selection,\ncross-validation, and forecast generation, while providing natural language\nexplanations and supporting direct queries about the future. The framework is\nLLM-agnostic, compatible with both commercial and open-source models, and\nsupports ensembles across diverse forecasting families. Results on the\nlarge-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art\nprobabilistic forecasting performance at low cost. Our framework provides a\npractical foundation for reproducible, explainable, and accessible agentic\nforecasting systems.",
    "authors": [
      "Azul Garza",
      "Reneé Rosillo"
    ],
    "published": "2025-08-30T21:48:51Z",
    "updated": "2025-09-04T01:01:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.00616v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00616v2",
    "comment": null,
    "relevance_score": 4.0
  },
  {
    "id": "2503.08046v3",
    "title": "MultiConIR: Towards multi-condition Information Retrieval",
    "summary": "Multi-condition information retrieval (IR) presents a significant, yet\nunderexplored challenge for existing systems. This paper introduces MultiConIR,\na benchmark specifically designed to evaluate retrieval and reranking models\nunder nuanced multi-condition query scenarios across five diverse domains. We\nsystematically assess model capabilities through three critical tasks:\ncomplexity robustness, relevance monotonicity, and query format sensitivity.\nOur extensive experiments on 15 models reveal a critical vulnerability: most\nretrievers and rerankers exhibit severe performance degradation as query\ncomplexity increases. Key deficiencies include widespread failure to maintain\nrelevance monotonicity, and high sensitivity to query style and condition\nplacement. The superior performance of GPT-4o reveals the performance gap\nbetween IR systems and advanced LLM for handling sophisticated natural language\nqueries. Furthermore, this work delves into the factors contributing to\nreranker performance deterioration and examines how condition positioning\nwithin queries affects similarity assessment, providing crucial insights for\nadvancing IR systems towards complex search scenarios. The code and datasets\nare available at https://github.com/EIT-NLP/MultiConIR",
    "authors": [
      "Xuan Lu",
      "Sifan Liu",
      "Bochao Yin",
      "Yongqi Li",
      "Xinghao Chen",
      "Hui Su",
      "Yaohui Jin",
      "Wenjun Zeng",
      "Xiaoyu Shen"
    ],
    "published": "2025-03-11T05:02:03Z",
    "updated": "2025-09-04T06:11:48Z",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2503.08046v3",
    "arxiv_url": "http://arxiv.org/abs/2503.08046v3",
    "comment": "EMNLP 2025 Findings",
    "relevance_score": 4.0
  },
  {
    "id": "2509.04404v1",
    "title": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in   Resume Screening",
    "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.",
    "authors": [
      "Kyra Wilson",
      "Mattea Sim",
      "Anna-Maria Gueorguieva",
      "Aylin Caliskan"
    ],
    "published": "2025-09-04T17:16:26Z",
    "updated": "2025-09-04T17:16:26Z",
    "categories": [
      "cs.CL",
      "cs.CY",
      "K.4.2",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04404v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04404v1",
    "comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04398v1",
    "title": "IPA: An Information-Preserving Input Projection Framework for Efficient   Foundation Model Adaptation",
    "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen.",
    "authors": [
      "Yuan Yin",
      "Shashanka Venkataramanan",
      "Tuan-Hung Vu",
      "Andrei Bursuc",
      "Matthieu Cord"
    ],
    "published": "2025-09-04T17:10:01Z",
    "updated": "2025-09-04T17:10:01Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04398v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04398v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04362v1",
    "title": "Parking Availability Prediction via Fusing Multi-Source Data with A   Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer",
    "summary": "The rapid growth of private car ownership has worsened the urban parking\npredicament, underscoring the need for accurate and effective parking\navailability prediction to support urban planning and management. To address\nkey limitations in modeling spatio-temporal dependencies and exploiting\nmulti-source data for parking availability prediction, this study proposes a\nnovel approach with SST-iTransformer. The methodology leverages K-means\nclustering to establish parking cluster zones (PCZs), extracting and\nintegrating traffic demand characteristics from various transportation modes\n(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted\nparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates\nmasking-reconstruction-based pretext tasks for self-supervised spatio-temporal\nrepresentation learning, and features an innovative dual-branch attention\nmechanism: Series Attention captures long-term temporal dependencies via\npatching operations, while Channel Attention models cross-variate interactions\nthrough inverted dimensions. Extensive experiments using real-world data from\nChengdu, China, demonstrate that SST-iTransformer outperforms baseline deep\nlearning models (including Informer, Autoformer, Crossformer, and\niTransformer), achieving state-of-the-art performance with the lowest mean\nsquared error (MSE) and competitive mean absolute error (MAE). Comprehensive\nablation studies quantitatively reveal the relative importance of different\ndata sources: incorporating ride-hailing data provides the largest performance\ngains, followed by taxi, whereas fixed-route transit features (bus/metro)\ncontribute marginally. Spatial correlation analysis further confirms that\nexcluding historical data from correlated parking lots within PCZs leads to\nsubstantial performance degradation, underscoring the importance of modeling\nspatial dependencies.",
    "authors": [
      "Yin Huang",
      "Yongqi Dong",
      "Youhua Tang",
      "Li Li"
    ],
    "published": "2025-09-04T16:22:29Z",
    "updated": "2025-09-04T16:22:29Z",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04362v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04362v1",
    "comment": "25 pages, 5 figures, under review for journal publication",
    "relevance_score": 3.5
  },
  {
    "id": "2508.14723v2",
    "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
    "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
    "authors": [
      "Guangzhan Wang",
      "Hongyu Zhang",
      "Beijun Shen",
      "Xiaodong Gu"
    ],
    "published": "2025-08-20T14:05:18Z",
    "updated": "2025-09-04T15:58:17Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.14723v2",
    "arxiv_url": "http://arxiv.org/abs/2508.14723v2",
    "comment": "Accepted by EMNLP 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2508.13755v2",
    "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with   Adaptive Exploration",
    "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
    "authors": [
      "Zhicheng Yang",
      "Zhijiang Guo",
      "Yinya Huang",
      "Yongxin Wang",
      "Dongchun Xie",
      "Yiwei Wang",
      "Xiaodan Liang",
      "Jing Tang"
    ],
    "published": "2025-08-19T11:51:40Z",
    "updated": "2025-09-04T15:21:27Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.13755v2",
    "arxiv_url": "http://arxiv.org/abs/2508.13755v2",
    "comment": "16 pages, 14 figures",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04304v1",
    "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge   in Large Language Models",
    "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.",
    "authors": [
      "Juraj Vladika",
      "Mahdi Dhaini",
      "Florian Matthes"
    ],
    "published": "2025-09-04T15:17:50Z",
    "updated": "2025-09-04T15:17:50Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04304v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04304v1",
    "comment": "Accepted to Findings of EMNLP 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04303v1",
    "title": "HumAIne-Chatbot: Real-Time Personalized Conversational AI via   Reinforcement Learning",
    "summary": "Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.",
    "authors": [
      "Georgios Makridis",
      "Georgios Fragiadakis",
      "Jorge Oliveira",
      "Tomaz Saraiva",
      "Philip Mavrepis",
      "Georgios Fatouros",
      "Dimosthenis Kyriazis"
    ],
    "published": "2025-09-04T15:16:38Z",
    "updated": "2025-09-04T15:16:38Z",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04303v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04303v1",
    "comment": "11 pages, 4 figures, IEEE conference format",
    "relevance_score": 3.5
  },
  {
    "id": "2508.08193v2",
    "title": "Street-Level AI: Are Large Language Models Ready for Real-World   Judgments?",
    "summary": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
    "authors": [
      "Gaurab Pokharel",
      "Shafkat Farabi",
      "Patrick J. Fowler",
      "Sanmay Das"
    ],
    "published": "2025-08-11T17:12:55Z",
    "updated": "2025-09-04T14:42:06Z",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.08193v2",
    "arxiv_url": "http://arxiv.org/abs/2508.08193v2",
    "comment": "This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
    "relevance_score": 3.5
  },
  {
    "id": "2507.14904v2",
    "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D   Visual Grounding based on CLIP",
    "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.",
    "authors": [
      "Fan Li",
      "Zanyi Wang",
      "Zeyi Huang",
      "Guang Dai",
      "Jingdong Wang",
      "Mengmeng Wang"
    ],
    "published": "2025-07-20T10:28:06Z",
    "updated": "2025-09-04T13:42:44Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.14904v2",
    "arxiv_url": "http://arxiv.org/abs/2507.14904v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04183v1",
    "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn   Mental Health Counseling Sessions",
    "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
    "authors": [
      "Aishik Mandal",
      "Tanmoy Chakraborty",
      "Iryna Gurevych"
    ],
    "published": "2025-09-04T12:59:24Z",
    "updated": "2025-09-04T12:59:24Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04183v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04183v1",
    "comment": "25 pages, 29 figures",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04180v1",
    "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer   Vision",
    "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
    "authors": [
      "Safouane El Ghazouali",
      "Umberto Michelucci"
    ],
    "published": "2025-09-04T12:54:32Z",
    "updated": "2025-09-04T12:54:32Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04180v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04180v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2410.22381v2",
    "title": "Robust training of implicit generative models for multivariate and   heavy-tailed distributions with an invariant statistical loss",
    "summary": "Traditional implicit generative models are capable of learning highly complex\ndata distributions. However, their training involves distinguishing real data\nfrom synthetically generated data using adversarial discriminators, which can\nlead to unstable training dynamics and mode dropping issues. In this work, we\nbuild on the \\textit{invariant statistical loss} (ISL) method introduced in\n\\cite{de2024training}, and extend it to handle heavy-tailed and multivariate\ndata distributions.\n  The data generated by many real-world phenomena can only be properly\ncharacterised using heavy-tailed probability distributions, and traditional\nimplicit methods struggle to effectively capture their asymptotic behavior. To\naddress this problem, we introduce a generator trained with ISL, that uses\ninput noise from a generalised Pareto distribution (GPD). We refer to this\ngenerative scheme as Pareto-ISL for conciseness. Our experiments demonstrate\nthat Pareto-ISL accurately models the tails of the distributions while still\neffectively capturing their central characteristics.\n  The original ISL function was conceived for 1D data sets. When the actual\ndata is $n$-dimensional, a straightforward extension of the method was obtained\nby targeting the $n$ marginal distributions of the data. This approach is\ncomputationally infeasible and ineffective in high-dimensional spaces. To\novercome this, we extend the 1D approach using random projections and define a\nnew loss function suited for multivariate data, keeping problems tractable by\nadjusting the number of projections. We assess its performance in\nmultidimensional generative modeling and explore its potential as a pretraining\ntechnique for generative adversarial networks (GANs) to prevent mode collapse,\nreporting promising results and highlighting its robustness across various\nhyperparameter settings.",
    "authors": [
      "José Manuel de Frutos",
      "Manuel A. Vázquez",
      "Pablo Olmos",
      "Joaquín Míguez"
    ],
    "published": "2024-10-29T10:27:50Z",
    "updated": "2025-09-04T12:44:44Z",
    "categories": [
      "stat.ML",
      "cs.AI",
      "stat.CO",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2410.22381v2",
    "arxiv_url": "http://arxiv.org/abs/2410.22381v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04166v1",
    "title": "Crossing the Species Divide: Transfer Learning from Speech to Animal   Sounds",
    "summary": "Self-supervised speech models have demonstrated impressive performance in\nspeech processing, but their effectiveness on non-speech data remains\nunderexplored. We study the transfer learning capabilities of such models on\nbioacoustic detection and classification tasks. We show that models such as\nHuBERT, WavLM, and XEUS can generate rich latent representations of animal\nsounds across taxa. We analyze the models properties with linear probing on\ntime-averaged representations. We then extend the approach to account for the\neffect of time-wise information with other downstream architectures. Finally,\nwe study the implication of frequency range and noise on performance. Notably,\nour results are competitive with fine-tuned bioacoustic pre-trained models and\nshow the impact of noise-robust pre-training setups. These findings highlight\nthe potential of speech-based self-supervised learning as an efficient\nframework for advancing bioacoustic research.",
    "authors": [
      "Jules Cauzinille",
      "Marius Miron",
      "Olivier Pietquin",
      "Masato Hagiwara",
      "Ricard Marxer",
      "Arnaud Rey",
      "Benoit Favre"
    ],
    "published": "2025-09-04T12:39:05Z",
    "updated": "2025-09-04T12:39:05Z",
    "categories": [
      "68T07",
      "cs.CL",
      "cs.SD",
      "cs.AI",
      "I.5.4; I.2.6; H.5.5",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04166v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04166v1",
    "comment": "5 pages, 3 figures, uses dcase2025.sty, submitted to DCASE 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04152v1",
    "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
    "summary": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
    "authors": [
      "Benoît Ronval",
      "Pierre Dupont",
      "Siegfried Nijssen"
    ],
    "published": "2025-09-04T12:25:14Z",
    "updated": "2025-09-04T12:25:14Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04152v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04152v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.01909v2",
    "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for   Responsible Language Models",
    "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
    "authors": [
      "Ranjie Duan",
      "Jiexi Liu",
      "Xiaojun Jia",
      "Shiji Zhao",
      "Ruoxi Cheng",
      "Fengxiang Wang",
      "Cheng Wei",
      "Yong Xie",
      "Chang Liu",
      "Defeng Li",
      "Yinpeng Dong",
      "Yichi Zhang",
      "Yuefeng Chen",
      "Chongwen Wang",
      "Xingjun Ma",
      "Xingxing Wei",
      "Yang Liu",
      "Hang Su",
      "Jun Zhu",
      "Xinfeng Li",
      "Yitong Sun",
      "Jie Zhang",
      "Jinzhao Hu",
      "Sha Xu",
      "Yitong Yang",
      "Jialing Tao",
      "Hui Xue"
    ],
    "published": "2025-09-02T03:04:27Z",
    "updated": "2025-09-04T11:54:06Z",
    "categories": [
      "cs.SC",
      "cs.CL",
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.01909v2",
    "arxiv_url": "http://arxiv.org/abs/2509.01909v2",
    "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
    "relevance_score": 3.5
  },
  {
    "id": "2403.03726v3",
    "title": "Diffusion on language model encodings for protein sequence generation",
    "summary": "Protein sequence design has seen significant advances through discrete\ndiffusion and autoregressive approaches, yet the potential of continuous\ndiffusion remains underexplored. Here, we present DiMA, a latent diffusion\nframework that operates on protein language model representations. Through\nsystematic exploration of architectural choices and diffusion components, we\ndevelop a robust methodology that generalizes across multiple protein encoders\nranging from 8M to 3B parameters. We demonstrate that our framework achieves\nconsistently high performance across sequence-only (ESM-2, ESMc),\ndual-decodable (CHEAP), and multimodal (SaProt) representations using the same\narchitecture and training approach. We extensively evaluate existing methods\nalongside DiMA using multiple metrics across two protein modalities, covering\nquality, diversity, novelty, and distribution matching of generated proteins.\nDiMA consistently produces novel, high-quality and diverse protein sequences\nand achieves strong results compared to baselines such as autoregressive,\ndiscrete diffusion and flow matching language models. The model demonstrates\nversatile functionality, supporting conditional generation tasks including\nprotein family-generation, motif scaffolding and infilling, and fold-specific\nsequence design. This work provides a universal continuous diffusion framework\nfor protein sequence generation, offering both architectural insights and\npractical applicability across various protein design scenarios.",
    "authors": [
      "Viacheslav Meshchaninov",
      "Pavel Strashnov",
      "Andrey Shevtsov",
      "Fedor Nikolaev",
      "Nikita Ivanisenko",
      "Olga Kardymon",
      "Dmitry Vetrov"
    ],
    "published": "2024-03-06T14:15:20Z",
    "updated": "2025-09-04T11:13:05Z",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2403.03726v3",
    "arxiv_url": "http://arxiv.org/abs/2403.03726v3",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04083v1",
    "title": "Intermediate Languages Matter: Formal Languages and LLMs affect   Neurosymbolic Reasoning",
    "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.",
    "authors": [
      "Alexander Beiser",
      "David Penz",
      "Nysret Musliu"
    ],
    "published": "2025-09-04T10:25:50Z",
    "updated": "2025-09-04T10:25:50Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04083v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04083v1",
    "comment": "To appear in the proceedings of The Second Workshop on Knowledge\n  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025\n  Conference, Vienna, Austria - September 3rd, 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04078v1",
    "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging   Evaluation of Large Language Models",
    "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.",
    "authors": [
      "Jingjing Liu",
      "Zeming Liu",
      "Zihao Cheng",
      "Mengliang He",
      "Xiaoming Shi",
      "Yuhang Guo",
      "Xiangrong Zhu",
      "Yuanfang Guo",
      "Yunhong Wang",
      "Haifeng Wang"
    ],
    "published": "2025-09-04T10:13:21Z",
    "updated": "2025-09-04T10:13:21Z",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04078v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04078v1",
    "comment": "30 pages, 12 figures, EMNLP 2025 Findings",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04076v1",
    "title": "Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot",
    "summary": "We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.",
    "authors": [
      "Lennart Clasmeier",
      "Jan-Gerrit Habekost",
      "Connor Gäde",
      "Philipp Allgeuer",
      "Stefan Wermter"
    ],
    "published": "2025-09-04T10:11:51Z",
    "updated": "2025-09-04T10:11:51Z",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04076v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04076v1",
    "comment": "Submitted to ICANN 20255 Special Session on Neural Robotics",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04013v1",
    "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
    "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
    "authors": [
      "Riccardo Lunardi",
      "Vincenzo Della Mea",
      "Stefano Mizzaro",
      "Kevin Roitero"
    ],
    "published": "2025-09-04T08:43:27Z",
    "updated": "2025-09-04T08:43:27Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04013v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04013v1",
    "comment": "Accepted at ECAI 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04007v1",
    "title": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers",
    "summary": "Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design.",
    "authors": [
      "Jinyuan Li",
      "Yi Chu",
      "Yiwen Sun",
      "Mengchuan Zou",
      "Shaowei Cai"
    ],
    "published": "2025-09-04T08:38:42Z",
    "updated": "2025-09-04T08:38:42Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04007v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04007v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.03995v1",
    "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question   Answering with Large Language Models",
    "summary": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.",
    "authors": [
      "Zhaoyan Gong",
      "Juan Li",
      "Zhiqiang Liu",
      "Lei Liang",
      "Huajun Chen",
      "Wen Zhang"
    ],
    "published": "2025-09-04T08:25:01Z",
    "updated": "2025-09-04T08:25:01Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03995v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03995v1",
    "comment": "EMNLP 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.03985v1",
    "title": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language   Models",
    "summary": "In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks.",
    "authors": [
      "Chuhan Zhang",
      "Ye Zhang",
      "Bowen Shi",
      "Yuyou Gan",
      "Tianyu Du",
      "Shouling Ji",
      "Dazhan Deng",
      "Yingcai Wu"
    ],
    "published": "2025-09-04T08:12:06Z",
    "updated": "2025-09-04T08:12:06Z",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03985v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03985v1",
    "comment": "12 pages, 9 figures",
    "relevance_score": 3.5
  },
  {
    "id": "2508.08715v3",
    "title": "MultiGen: Child-Friendly Multilingual Speech Generator with LLMs",
    "summary": "Generative speech models have demonstrated significant potential in improving\nhuman-machine interactions, offering valuable real-world applications such as\nlanguage learning for children. However, achieving high-quality, child-friendly\nspeech generation remains challenging, particularly for low-resource languages\nacross diverse languages and cultural contexts. In this paper, we propose\nMultiGen, a multilingual speech generation model with child-friendly\ninteraction, leveraging LLM architecture for speech generation tailored for\nlow-resource languages. We propose to integrate age-appropriate multilingual\nspeech generation using LLM architectures, which can be used to facilitate\nyoung children's communication with AI systems through culturally relevant\ncontext in three low-resource languages: Singaporean accent Mandarin, Malay,\nand Tamil. Experimental results from both objective metrics and subjective\nevaluations demonstrate the superior performance of the proposed MultiGen\ncompared to baseline methods.",
    "authors": [
      "Xiaoxue Gao",
      "Huayun Zhang",
      "Nancy F. Chen"
    ],
    "published": "2025-08-12T07:58:48Z",
    "updated": "2025-09-04T07:56:00Z",
    "categories": [
      "cs.CL",
      "eess.SP",
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.08715v3",
    "arxiv_url": "http://arxiv.org/abs/2508.08715v3",
    "comment": "5 pages",
    "relevance_score": 3.5
  },
  {
    "id": "2505.05118v2",
    "title": "Enhancing Text2Cypher with Schema Filtering",
    "summary": "Knowledge graphs represent complex data using nodes, relationships, and\nproperties. Cypher, a powerful query language for graph databases, enables\nefficient modeling and querying. Recent advancements in large language models\nallow translation of natural language questions into Cypher queries -\nText2Cypher. A common approach is incorporating database schema into prompts.\nHowever, complex schemas can introduce noise, increase hallucinations, and\nraise computational costs. Schema filtering addresses these challenges by\nincluding only relevant schema elements, improving query generation while\nreducing token costs. This work explores various schema filtering methods for\nText2Cypher task and analyzes their impact on token length, performance, and\ncost. Results show that schema filtering effectively optimizes Text2Cypher,\nespecially for smaller models. Consistent with prior research, we find that\nlarger models benefit less from schema filtering due to their longer context\ncapabilities. However, schema filtering remains valuable for both larger and\nsmaller models in cost reduction.",
    "authors": [
      "Makbule Gulcin Ozsoy"
    ],
    "published": "2025-05-08T10:42:20Z",
    "updated": "2025-09-04T17:52:23Z",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2505.05118v2",
    "arxiv_url": "http://arxiv.org/abs/2505.05118v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2505.05122v2",
    "title": "Text2Cypher: Data Pruning using Hard Example Selection",
    "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.",
    "authors": [
      "Makbule Gulcin Ozsoy"
    ],
    "published": "2025-05-08T10:51:13Z",
    "updated": "2025-09-04T17:41:13Z",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2505.05122v2",
    "arxiv_url": "http://arxiv.org/abs/2505.05122v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04394v1",
    "title": "Transition Models: Rethinking the Generative Learning Objective",
    "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
    "authors": [
      "Zidong Wang",
      "Yiyuan Zhang",
      "Xiaoyu Yue",
      "Xiangyu Yue",
      "Yangguang Li",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "published": "2025-09-04T17:05:59Z",
    "updated": "2025-09-04T17:05:59Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04394v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04394v1",
    "comment": "The code is released at https://github.com/WZDTHU/TiM",
    "relevance_score": 3.5
  },
  {
    "id": "2504.14610v4",
    "title": "Imputation-free Learning of Tabular Data with Missing Values using   Incremental Feature Partitions in Transformer",
    "summary": "Tabular data sets with varying missing values are prepared for machine\nlearning using an arbitrary imputation strategy. Synthetic values generated by\nimputation models often raise concerns about data quality and the reliability\nof data-driven outcomes. To address these concerns, this article proposes an\nimputation-free incremental attention learning (IFIAL) method for tabular data.\nA pair of attention masks is derived and retrofitted to a transformer to\ndirectly streamline tabular data without imputing or initializing missing\nvalues. The proposed method incrementally learns partitions of overlapping and\nfixed-size feature sets to enhance the efficiency and performance of the\ntransformer. The average classification performance rank order across 17\ndiverse tabular data sets highlights the superiority of IFIAL over 11\nstate-of-the-art learning methods with or without missing value imputations.\nFurther experiments substantiate the robustness of IFIAL against varying\nmissing value types and rates compared to methods involving missing value\nimputation. Our analysis reveals that a feature partition size of half the\noriginal feature space is, both computationally and in terms of accuracy, the\nbest choice for the proposed incremental learning. The proposed method is one\nof the first solutions to enable deep attention learning of tabular data\nwithout requiring missing-value imputation. The source code for this paper is\npublicly available.",
    "authors": [
      "Manar D. Samad",
      "Kazi Fuad B. Akhter",
      "Shourav B. Rabbani",
      "Ibna Kowsar"
    ],
    "published": "2025-04-20T13:31:49Z",
    "updated": "2025-09-04T14:19:58Z",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2504.14610v4",
    "arxiv_url": "http://arxiv.org/abs/2504.14610v4",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04226v1",
    "title": "Rethinking the long-range dependency in Mamba/SSM and transformer models",
    "summary": "Long-range dependency is one of the most desired properties of recent\nsequence models such as state-space models (particularly Mamba) and transformer\nmodels. New model architectures are being actively developed and benchmarked\nfor prediction tasks requiring long-range dependency. However, the capability\nof modeling long-range dependencies of these models has not been investigated\nfrom a theoretical perspective, which hinders a systematic improvement on this\naspect. In this work, we mathematically define long-range dependency using the\nderivative of hidden states with respect to past inputs and compare the\ncapability of SSM and transformer models of modeling long-range dependency\nbased on this definition. We showed that the long-range dependency of SSM\ndecays exponentially with the sequence length, which aligns with the\nexponential decay of memory function in RNN. But the attention mechanism used\nin transformers is more flexible and is not constrained to exponential decay,\nwhich could in theory perform better at modeling long-range dependency with\nsufficient training data, computing resources, and proper training. To combine\nthe flexibility of long-range dependency of attention mechanism and computation\nefficiency of SSM, we propose a new formulation for hidden state update in SSM\nand prove its stability under a standard Gaussian distribution of the input\ndata.",
    "authors": [
      "Cong Ma",
      "Kayvan Najarian"
    ],
    "published": "2025-09-04T13:56:47Z",
    "updated": "2025-09-04T13:56:47Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04226v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04226v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04193v1",
    "title": "DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval",
    "summary": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of\nthe same category across diverse domains without relying on annotations.\nExisting UCIR methods, which align cross-domain features for the entire image,\noften struggle with the domain gap, as the object features critical for\nretrieval are frequently entangled with domain-specific styles. To address this\nchallenge, we propose DUDE, a novel UCIR method building upon feature\ndisentanglement. In brief, DUDE leverages a text-to-image generative model to\ndisentangle object features from domain-specific styles, thus facilitating\nsemantical image retrieval. To further achieve reliable alignment of the\ndisentangled object features, DUDE aligns mutual neighbors from within domains\nto across domains in a progressive manner. Extensive experiments demonstrate\nthat DUDE achieves state-of-the-art performance across three benchmark datasets\nover 13 domains. The code will be released.",
    "authors": [
      "Ruohong Yang",
      "Peng Hu",
      "Yunfan Li",
      "Xi Peng"
    ],
    "published": "2025-09-04T13:15:16Z",
    "updated": "2025-09-04T13:15:16Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04193v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04193v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2507.03690v2",
    "title": "Plugging Attention into Power Grids: Towards Transparent Forecasting",
    "summary": "Reliable prediction of electricity demand plays a key role in safeguarding\ngrid stability and guiding generation decisions, a need that grows with the\ndecentralization and complexity of modern systems. While classical approaches\nsuch as Generalized Additive Models (GAMs) remain widely used, they often fail\nto capture the spatial dependencies inherent in energy networks. Graph Neural\nNetworks (GNNs) offer a principled framework to incorporate this structure by\ndirectly leveraging graph topologies. In this work, we evaluate a broad set of\nGNN architectures -- including GCN, GraphSAGE, ChebConv, TAG, APPNP,\nTransformerConv, and Graph Attention Networks (GAT and GATv2) -- on two\nreal-world electricity consumption datasets from France and the UK. Our results\nshow that simpler models such as GCN, SAGE, or APPNP often outperform more\ncomplex alternatives in low-data regimes, while GAT ranks among the strongest\narchitectures in our benchmarks, combining high accuracy with valuable\ninterpretability. We perform a temporal analysis of attention weights,\nrevealing evolving patterns of regional interaction linked to seasonal and\nmeteorological variability. These results highlight that, although attention is\nnot universally superior, it provides valuable explanatory power when spatial\ndependencies are prominent. Additionally, we demonstrate that ensemble-based\nexpert aggregation strategies, particularly bottom-up combinations,\nsignificantly improve robustness and yield state-of-the-art performance across\nboth datasets. These findings highlight the dual promise of GNNs for accurate\nand interpretable forecasting, and suggest that architectural simplicity\ncoupled with ensemble methods can provide a practical path forward for\ntransparent energy analytics.",
    "authors": [
      "Eloi Campagne",
      "Itai Zehavi",
      "Yvenn Amara-Ouali",
      "Yannig Goude",
      "Argyris Kalogeratos"
    ],
    "published": "2025-07-04T16:18:18Z",
    "updated": "2025-09-04T12:05:09Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.03690v2",
    "arxiv_url": "http://arxiv.org/abs/2507.03690v2",
    "comment": "16 pages, ECML PKDD 2025 Workshop paper",
    "relevance_score": 3.5
  },
  {
    "id": "2508.01426v2",
    "title": "UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting",
    "summary": "Recent advancements in deep learning have led to the development of\nFoundation Models (FMs) for weather forecasting, yet their ability to predict\nextreme weather events remains limited. Existing approaches either focus on\ngeneral weather conditions or specialize in specific-type extremes, neglecting\nthe real-world atmospheric patterns of diversified extreme events. In this\nwork, we identify two key characteristics of extreme events: (1) the spectral\ndisparity against normal weather regimes, and (2) the hierarchical drivers and\ngeographic blending of diverse extremes. Along this line, we propose\nUniExtreme, a universal extreme weather forecasting foundation model that\nintegrates (1) an Adaptive Frequency Modulation (AFM) module that captures\nregion-wise spectral differences between normal and extreme weather, through\nlearnable Beta-distribution filters and multi-granularity spectral aggregation,\nand (2) an Event Prior Augmentation (EPA) module which incorporates\nregion-specific extreme event priors to resolve hierarchical extreme diversity\nand composite extreme schema, via a dual-level memory fusion network. Extensive\nexperiments demonstrate that UniExtreme outperforms state-of-the-art baselines\nin both extreme and general weather forecasting, showcasing superior\nadaptability across diverse extreme scenarios.",
    "authors": [
      "Hang Ni",
      "Weijia Zhang",
      "Hao Liu"
    ],
    "published": "2025-08-02T16:20:19Z",
    "updated": "2025-09-04T11:24:09Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2508.01426v2",
    "arxiv_url": "http://arxiv.org/abs/2508.01426v2",
    "comment": "35 pages, 80 figures, submitted to ACM KDD 2026 conference",
    "relevance_score": 3.5
  },
  {
    "id": "2411.00515v2",
    "title": "Zero-shot Generalization in Inventory Management: Train, then Estimate   and Decide",
    "summary": "Deploying deep reinforcement learning (DRL) in real-world inventory\nmanagement presents challenges, including dynamic environments and uncertain\nproblem parameters, e.g. demand and lead time distributions. These challenges\nhighlight a research gap, suggesting a need for a unifying framework to model\nand solve sequential decision-making under parameter uncertainty. We address\nthis by exploring an underexplored area of DRL for inventory management:\ntraining generally capable agents (GCAs) under zero-shot generalization (ZSG).\nHere, GCAs are advanced DRL policies designed to handle a broad range of\nsampled problem instances with diverse inventory challenges. ZSG refers to the\nability to successfully apply learned policies to unseen instances with unknown\nparameters without retraining.\n  We propose a unifying Super-Markov Decision Process formulation and the\nTrain, then Estimate and Decide (TED) framework to train and deploy a GCA\ntailored to inventory management applications. The TED framework consists of\nthree phases: training a GCA on varied problem instances, continuously\nestimating problem parameters during deployment, and making decisions based on\nthese estimates. Applied to periodic review inventory problems with lost sales,\ncyclic demand patterns, and stochastic lead times, our trained agent, the\nGenerally Capable Lost Sales Network (GC-LSN) consistently outperforms\nwell-known traditional policies when problem parameters are known. Moreover,\nunder conditions where demand and/or lead time distributions are initially\nunknown and must be estimated, we benchmark against online learning methods\nthat provide worst-case performance guarantees. Our GC-LSN policy, paired with\nthe Kaplan-Meier estimator, is demonstrated to complement these methods by\nproviding superior empirical performance.",
    "authors": [
      "Tarkan Temizöz",
      "Christina Imdahl",
      "Remco Dijkman",
      "Douniel Lamghari-Idrissi",
      "Willem van Jaarsveld"
    ],
    "published": "2024-11-01T11:20:05Z",
    "updated": "2025-09-04T10:16:00Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2411.00515v2",
    "arxiv_url": "http://arxiv.org/abs/2411.00515v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04063v1",
    "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA   Flow Models",
    "summary": "Vision-Language-Action (VLA) models based on flow matching have shown\nexcellent performance in general-purpose robotic manipulation tasks. However,\nthe action accuracy of these models on complex downstream tasks is\nunsatisfactory. One important reason is that these models rely solely on the\npost-training paradigm of imitation learning, which makes it difficult to have\na deeper understanding of the distribution properties of data quality, which is\nexactly what Reinforcement Learning (RL) excels at. In this paper, we\ntheoretically propose an offline RL post-training objective for VLA flow models\nand induce an efficient and feasible offline RL fine-tuning algorithm --\nAdaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted\nscaling factor in the VLA flow model loss, we construct a principled\nbias-variance trade-off objective function to optimally control the impact of\nRL signal on flow loss. ARFM adaptively balances RL advantage preservation and\nflow loss gradient variance control, resulting in a more stable and efficient\nfine-tuning process. Extensive simulation and real-world experimental results\nshow that ARFM exhibits excellent generalization, robustness, few-shot\nlearning, and continuous learning performance.",
    "authors": [
      "Hongyin Zhang",
      "Shiyuan Zhang",
      "Junxi Jin",
      "Qixin Zeng",
      "Yifan Qiao",
      "Hongchao Lu",
      "Donglin Wang"
    ],
    "published": "2025-09-04T09:48:43Z",
    "updated": "2025-09-04T09:48:43Z",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04063v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04063v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2411.10438v4",
    "title": "MARS: Unleashing the Power of Variance Reduction for Training Large   Models",
    "summary": "Training deep neural networks--and more recently, large models demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin. The\nimplementation of MARS is available at https://github.com/AGI-Arena/MARS.",
    "authors": [
      "Huizhuo Yuan",
      "Yifeng Liu",
      "Shuang Wu",
      "Xun Zhou",
      "Quanquan Gu"
    ],
    "published": "2024-11-15T18:57:39Z",
    "updated": "2025-09-04T09:37:05Z",
    "categories": [
      "math.OC",
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2411.10438v4",
    "arxiv_url": "http://arxiv.org/abs/2411.10438v4",
    "comment": "35 pages, 19 figures, 12 tables",
    "relevance_score": 3.5
  },
  {
    "id": "2509.03335v2",
    "title": "EvolveSignal: A Large Language Model Powered Coding Agent for   Discovering Traffic Signal Control Algorithms",
    "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
    "authors": [
      "Leizhen Wang",
      "Peibo Duan",
      "Hao Wang",
      "Yue Wang",
      "Jian Xu",
      "Nan Zheng",
      "Zhenliang Ma"
    ],
    "published": "2025-09-03T14:10:56Z",
    "updated": "2025-09-04T09:25:05Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.03335v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03335v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.01875v2",
    "title": "RadioDiff-Loc: Diffusion Model Enhanced Scattering Congnition for NLoS   Localization with Sparse Radio Map Estimation",
    "summary": "Accurate localization of non-cooperative signal sources in non-line-of-sight\n(NLoS) environments remains a critical challenge with a wide range of\napplications, including autonomous navigation, industrial automation, and\nemergency response. In such settings, traditional positioning techniques\nrelying on line-of-sight (LoS) or cooperative signaling fail due to severe\nmultipath propagation and unknown transmit power. This paper proposes a novel\ngenerative inference framework for NLoS localization based on conditional\ndiffusion models. By leveraging the physical insight that diffracted\nelectromagnetic energy concentrates near building edges, we develop a sampling\nstrategy that collects sparse received signal strength (RSS) measurements at\nthe geometric vertices of obstacles--locations that maximize Fisher information\nand mutual information with respect to the unknown source. To overcome the lack\nof known transmission power, we normalize all sampled RSS values relative to\nthe maximum observed intensity, enabling the construction of a power-invariant\nradio map (RM). A conditional diffusion model is trained to reconstruct the\nfull RM based on environmental layout and sparse RSS observations. Localization\nis then achieved by identifying the brightest point on the generated RM.\nMoreover, the proposed framework is compatible with existing RSS-based\nlocalization algorithms, enabling a dual-driven paradigm that fuses physical\nknowledge and data-driven inference for improved accuracy. Extensive\ntheoretical analysis and empirical validation demonstrate that our approach\nachieves high localization accuracy with significantly reduced sampling cost,\noffering a scalable and physically grounded solution for non-cooperative NLoS\nemitter localization.",
    "authors": [
      "Xiucheng Wang",
      "Qiming Zhang",
      "Nan Cheng"
    ],
    "published": "2025-09-02T01:43:23Z",
    "updated": "2025-09-04T09:23:45Z",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.01875v2",
    "arxiv_url": "http://arxiv.org/abs/2509.01875v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04326v1",
    "title": "Efficient Odd-One-Out Anomaly Detection",
    "summary": "The recently introduced odd-one-out anomaly detection task involves\nidentifying the odd-looking instances within a multi-object scene. This problem\npresents several challenges for modern deep learning models, demanding spatial\nreasoning across multiple views and relational reasoning to understand context\nand generalize across varying object categories and layouts. We argue that\nthese challenges must be addressed with efficiency in mind. To this end, we\npropose a DINO-based model that reduces the number of parameters by one third\nand shortens training time by a factor of three compared to the current\nstate-of-the-art, while maintaining competitive performance. Our experimental\nevaluation also introduces a Multimodal Large Language Model baseline,\nproviding insights into its current limitations in structured visual reasoning\ntasks. The project page can be found at\nhttps://silviochito.github.io/EfficientOddOneOut/",
    "authors": [
      "Silvio Chito",
      "Paolo Rabino",
      "Tatiana Tommasi"
    ],
    "published": "2025-09-04T15:44:37Z",
    "updated": "2025-09-04T15:44:37Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04326v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04326v1",
    "comment": "Accepted at ICIAP 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04268v1",
    "title": "Differential Morphological Profile Neural Networks for Semantic   Segmentation",
    "summary": "Semantic segmentation of overhead remote sensing imagery enables applications\nin mapping, urban planning, and disaster response. State-of-the-art\nsegmentation networks are typically developed and tuned on ground-perspective\nphotographs and do not directly address remote sensing challenges such as\nextreme scale variation, foreground-background imbalance, and large image\nsizes. We explore the incorporation of the differential morphological profile\n(DMP), a multi-scale shape extraction method based on grayscale morphology,\ninto modern segmentation networks. Prior studies have shown that the DMP can\nprovide critical shape information to Deep Neural Networks to enable superior\ndetection and classification performance in overhead imagery. In this work, we\nextend prior DMPNet work beyond classification and object detection by\nintegrating DMP features into three state-of-the-art convolutional and\ntransformer semantic segmentation architectures. We utilize both direct input,\nwhich adapts the input stem of feature extraction architectures to accept DMP\nchannels, and hybrid architectures, a dual-stream design that fuses RGB and DMP\nencoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP\ndifferentials and structuring element shapes to more effectively provide shape\ninformation to the model. Our results show that while non-DMP models generally\noutperform the direct-input variants, hybrid DMP consistently outperforms\ndirect-input and is capable of surpassing a non-DMP model on mIoU, F1, and\nRecall.",
    "authors": [
      "David Huangal",
      "J. Alex Hurt"
    ],
    "published": "2025-09-04T14:44:18Z",
    "updated": "2025-09-04T14:44:18Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04268v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04268v1",
    "comment": "14 pages, 7 figures",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04145v1",
    "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network   Weight Space Diffusion",
    "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.",
    "authors": [
      "Dongliang Cao",
      "Guoxing Sun",
      "Marc Habermann",
      "Florian Bernard"
    ],
    "published": "2025-09-04T12:15:55Z",
    "updated": "2025-09-04T12:15:55Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04145v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04145v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2505.07611v2",
    "title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A   Comprehensive Review of Methods, Datasets, and Future Directions",
    "summary": "Traffic accident prediction and detection are critical for enhancing road\nsafety, and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning. This paper reviews 147\nrecent studies, focusing on the application of supervised, unsupervised, and\nhybrid deep learning models for accident prediction, alongside the use of\nreal-world and synthetic datasets. Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatio-temporal\nfeature-based prediction, scene understanding, and multi modal data fusion.\nWhile these methods demonstrate significant potential, challenges such as data\nscarcity, limited generalization to complex scenarios, and real-time\nperformance constraints remain prevalent. This review highlights opportunities\nfor future research, including the integration of multi modal data fusion,\nself-supervised learning, and Transformer-based architectures to enhance\nprediction accuracy and scalability. By synthesizing existing advancements and\nidentifying critical gaps, this paper provides a foundational reference for\ndeveloping robust and adaptive Vision-TAA systems, contributing to road safety\nand traffic management.",
    "authors": [
      "Ruonan Lin",
      "Tao Tang",
      "Yongtai Liu",
      "Wenye Zhou",
      "Xin Yang",
      "Hao Zheng",
      "Jianpu Lin",
      "Yi Zhang"
    ],
    "published": "2025-05-12T14:34:22Z",
    "updated": "2025-09-04T09:28:05Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2505.07611v2",
    "arxiv_url": "http://arxiv.org/abs/2505.07611v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.03999v1",
    "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy   Representation",
    "summary": "Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.",
    "authors": [
      "Han Huang",
      "Han Sun",
      "Ningzhong Liu",
      "Huiyu Zhou",
      "Jiaquan Shen"
    ],
    "published": "2025-09-04T08:27:54Z",
    "updated": "2025-09-04T08:27:54Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03999v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03999v1",
    "comment": "14 pages, accepted by PRCV2025",
    "relevance_score": 3.5
  },
  {
    "id": "2504.09885v2",
    "title": "Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated   Piano Hand Motion Synthesis",
    "summary": "Automating the synthesis of coordinated bimanual piano performances poses\nsignificant challenges, particularly in capturing the intricate choreography\nbetween the hands while preserving their distinct kinematic signatures. In this\npaper, we propose a dual-stream neural framework designed to generate\nsynchronized hand gestures for piano playing from audio input, addressing the\ncritical challenge of modeling both hand independence and coordination. Our\nframework introduces two key innovations: (i) a decoupled diffusion-based\ngeneration framework that independently models each hand's motion via\ndual-noise initialization, sampling distinct latent noise for each while\nleveraging a shared positional condition, and (ii) a Hand-Coordinated\nAsymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise\nto highlight asymmetric hand-specific features, while adaptively enhancing\ninter-hand coordination during denoising. Comprehensive evaluations demonstrate\nthat our framework outperforms existing state-of-the-art methods across\nmultiple metrics. Our project is available at\nhttps://monkek123king.github.io/S2C_page/.",
    "authors": [
      "Zihao Liu",
      "Mingwen Ou",
      "Zunnan Xu",
      "Jiaqi Huang",
      "Haonan Han",
      "Ronghui Li",
      "Xiu Li"
    ],
    "published": "2025-04-14T05:17:41Z",
    "updated": "2025-09-04T08:25:53Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2504.09885v2",
    "arxiv_url": "http://arxiv.org/abs/2504.09885v2",
    "comment": "15 pages, 7 figures, Accepted to ACMMM 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2508.13238v2",
    "title": "DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool   Interleaved Vision-Language Model",
    "summary": "Recent advances in large vision-language models (LVLMs) have enabled a new\nparadigm of end-to-end document image parsing, excelling in Optical Character\nRecognition (OCR) tasks such as text, table, and formula recognition. However,\ngenerative LVLMs, similarly to large language models (LLMs), are prone to\nhallucinations--generating words that do not exist in input images.\nFurthermore, LVLMs are designed for general purposes and tend to be less\neffective on OCR tasks compared to expert models that are trained on\ndomain-specific datasets. In this paper, we propose DianJin-OCR-R1, a\nreasoning-enhanced framework designed to address these limitations through\ntraining reasoning-and-tool interleaved VLMs. Given a recognition instruction,\nour DianJin-OCR-R1 model first recognizes the content in the input image by its\nown OCR capabilities, and then calls other tools (i.e., other expert models) to\nobtain their results as references, finally \"looks again\" the image and\nrethinks about the reasoning process to provide the final recognized content.\nSince architectures of expert models are tailored for specific OCR tasks, which\nmakes them less prone to hallucinations, their results can help VLMs mitigate\nhallucinations. We evaluate our model on ReST and OmniDocBench, and\nexperimental results show that our DianJin-OCR-R1 models consistently\noutperform their non-reasoning counterparts and expert OCR models, which proves\nthe effectiveness of our method. Additionally, the results indicate that\nenhancing expert models, which are typically small and easy to iterate, enable\nperformance improvements for VLMs.",
    "authors": [
      "Qian Chen",
      "Xianyin Zhang",
      "Lifan Guo",
      "Feng Chen",
      "Chi Zhang"
    ],
    "published": "2025-08-18T03:28:57Z",
    "updated": "2025-09-04T08:05:29Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.13238v2",
    "arxiv_url": "http://arxiv.org/abs/2508.13238v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.03961v1",
    "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for   Remote Sensing Change Detection",
    "summary": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
    "authors": [
      "Yijun Zhou",
      "Yikui Zhai",
      "Zilu Ying",
      "Tingfeng Xian",
      "Wenlve Zhou",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "Xudong Jia",
      "Hongsheng Zhang",
      "C. L. Philip Chen"
    ],
    "published": "2025-09-04T07:39:18Z",
    "updated": "2025-09-04T07:39:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03961v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03961v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2502.06289v2",
    "title": "Is an Ultra Large Natural Image-Based Foundation Model Superior to a   Retina-Specific Model for Detecting Ocular and Systemic Diseases?",
    "summary": "The advent of foundation models (FMs) is transforming medical domain. In\nophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4\nmillion natural images and 1.6 million retinal images, has demonstrated high\nadaptability across clinical applications. Conversely, DINOv2, a\ngeneral-purpose vision FM pre-trained on 142 million natural images, has shown\npromise in non-medical domains. However, its applicability to clinical tasks\nremains underexplored. To address this, we conducted head-to-head evaluations\nby fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular\ndisease detection and systemic disease prediction tasks, across eight\nstandardized open-source ocular datasets, as well as the Moorfields AlzEye and\nthe UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting\ndiabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,\nall P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In\nglaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,\nP<0.001). Conversely, RETFound achieved superior performance over all DINOv2\nmodels in predicting heart failure, myocardial infarction, and ischaemic stroke\n(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even\nwith 10% of the fine-tuning data. These findings showcase the distinct\nscenarios where general-purpose and domain-specific FMs excel, highlighting the\nimportance of aligning FM selection with task-specific requirements to optimise\nclinical performance.",
    "authors": [
      "Qingshan Hou",
      "Yukun Zhou",
      "Jocelyn Hui Lin Goh",
      "Ke Zou",
      "Samantha Min Er Yew",
      "Sahana Srinivasan",
      "Meng Wang",
      "Thaddaeus Lo",
      "Xiaofeng Lei",
      "Siegfried K. Wagner",
      "Mark A. Chia",
      "Dawei Yang",
      "Hongyang Jiang",
      "An Ran Ran",
      "Rui Santos",
      "Gabor Mark Somfai",
      "Juan Helen Zhou",
      "Haoyu Chen",
      "Qingyu Chen",
      "Carol Y. Cheung",
      "Pearse A. Keane",
      "Yih Chung Tham"
    ],
    "published": "2025-02-10T09:31:39Z",
    "updated": "2025-09-04T07:35:48Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2502.06289v2",
    "arxiv_url": "http://arxiv.org/abs/2502.06289v2",
    "comment": "Accepted by Ophthalmology Science and is currently in press",
    "relevance_score": 3.5
  },
  {
    "id": "2503.21080v4",
    "title": "EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming   in Debt Recovery",
    "summary": "Large language model-based chatbots have enhanced engagement in financial\nnegotiations, but their overreliance on passive empathy introduces critical\nrisks in credit collection. While empathy-driven approaches preserve client\nsatisfaction in benign cases, they fail catastrophically against dishonest\ndebtors--individuals who exploit conciliatory tactics to manipulate terms or\nevade repayment. Blindly prioritizing \"customer experience\" in such scenarios\nleads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic\nexploitation. To address this, we propose EQ-Knight, an LLM agent that\ndynamically optimizes emotional strategy to defend creditor interests. Unlike\nnaive empathy-centric bots, EQ-Knight integrates emotion memory and\ngame-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and\npredict debtor emotional states. By analyzing both real-time and historical\nemotional cues, EQ-Knight strategically counters negative emotions (e.g.,\naggression, feigned distress) while preserving productive debtor relationships.\nExperiments demonstrate EQ-Knight's superiority over conventional LLM\nnegotiators: it achieves a 32\\% reduction in concession losses without\ncompromising recovery rates, particularly in adversarial cases where debtors\nweaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce\nconcessions. For credit agencies, EQ-Knight transforms LLMs from high-risk\n\"people-pleasers\" into strategic emotion-defenders--balancing emotional\nintelligence with tactical rigor to enforce accountability and deter\nexploitation.",
    "authors": [
      "Yunbo Long",
      "Yuhan Liu",
      "Liming Xu",
      "Alexandra Brintrup"
    ],
    "published": "2025-03-27T01:41:34Z",
    "updated": "2025-09-04T15:06:27Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2503.21080v4",
    "arxiv_url": "http://arxiv.org/abs/2503.21080v4",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2503.09454v4",
    "title": "Explicit Learning and the LLM in Machine Translation",
    "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book, a process we term \"explicit learning.\" To\nrigorously assess this ability, we design controlled translation experiments\nbetween English and constructed languages generated, through specific\ncryptographic means, from Latin or French. Contrary to previous studies, our\nresults demonstrate that LLMs do possess a measurable capacity for explicit\nlearning. This ability, however, diminishes as the complexity of the linguistic\nphenomena to be learned increases. Supervised fine-tuning on ad hoc chains of\nthought significantly enhances LLM performance but struggles to generalize to\ntypologically novel or more complex linguistic features. These findings point\nto the need for more diverse training sets and alternative fine-tuning\nstrategies to further improve explicit learning by LLMs, benefiting\nlow-resource languages typically described in grammar books but lacking\nextensive corpora.",
    "authors": [
      "Malik Marmonier",
      "Rachel Bawden",
      "Benoît Sagot"
    ],
    "published": "2025-03-12T14:57:08Z",
    "updated": "2025-09-04T12:20:43Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2503.09454v4",
    "arxiv_url": "http://arxiv.org/abs/2503.09454v4",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2508.00454v2",
    "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
    "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n\"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to\nassess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
    "authors": [
      "Yuqi Tang",
      "Kehua Feng",
      "Yunfeng Wang",
      "Zhiwen Chen",
      "Chengfei Lv",
      "Gang Yu",
      "Qiang Zhang",
      "Keyan Ding"
    ],
    "published": "2025-08-01T09:26:01Z",
    "updated": "2025-09-04T11:57:46Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2508.00454v2",
    "arxiv_url": "http://arxiv.org/abs/2508.00454v2",
    "comment": "15 pages, 2 pages, under review",
    "relevance_score": 3.5
  },
  {
    "id": "2506.09556v2",
    "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for   Speech Emotion Recognition in Naturalistic Conditions",
    "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.",
    "authors": [
      "Georgios Chatzichristodoulou",
      "Despoina Kosmopoulou",
      "Antonios Kritikos",
      "Anastasia Poulopoulou",
      "Efthymios Georgiou",
      "Athanasios Katsamanis",
      "Vassilis Katsouros",
      "Alexandros Potamianos"
    ],
    "published": "2025-06-11T09:41:23Z",
    "updated": "2025-09-04T10:18:13Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2506.09556v2",
    "arxiv_url": "http://arxiv.org/abs/2506.09556v2",
    "comment": "Interspeech 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2509.04072v1",
    "title": "LibriQuote: A Speech Dataset of Fictional Character Utterances for   Expressive Zero-Shot Speech Synthesis",
    "summary": "Text-to-speech (TTS) systems have recently achieved more expressive and\nnatural speech synthesis by scaling to large speech datasets. However, the\nproportion of expressive speech in such large-scale corpora is often unclear.\nBesides, existing expressive speech corpora are typically smaller in scale and\nprimarily used for benchmarking TTS systems. In this paper, we introduce the\nLibriQuote dataset, an English corpus derived from read audiobooks, designed\nfor both fine-tuning and benchmarking expressive zero-shot TTS system. The\ntraining dataset includes 12.7K hours of read, non-expressive speech and 5.3K\nhours of mostly expressive speech drawn from character quotations. Each\nutterance in the expressive subset is supplemented with the context in which it\nwas written, along with pseudo-labels of speech verbs and adverbs used to\ndescribe the quotation (\\textit{e.g. ``he whispered softly''}). Additionally,\nwe provide a challenging 7.5 hour test set intended for benchmarking TTS\nsystems: given a neutral reference speech as input, we evaluate system's\nability to synthesize an expressive utterance while preserving reference\ntimbre. We validate qualitatively the test set by showing that it covers a wide\nrange of emotions compared to non-expressive speech, along with various\naccents. Extensive subjective and objective evaluations show that fine-tuning a\nbaseline TTS system on LibriQuote significantly improves its synthesized speech\nintelligibility, and that recent systems fail to synthesize speech as\nexpressive and natural as the ground-truth utterances. The dataset and\nevaluation code are freely available. Audio samples can be found at\nhttps://libriquote.github.io/.",
    "authors": [
      "Gaspard Michel",
      "Elena V. Epure",
      "Christophe Cerisara"
    ],
    "published": "2025-09-04T10:05:06Z",
    "updated": "2025-09-04T10:05:06Z",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04072v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04072v1",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2508.20038v3",
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to   Enhance LLM Safety Guardrail to Potential Attacks",
    "summary": "Despite advances in improving large language model (LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.",
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ],
    "published": "2025-08-27T16:44:03Z",
    "updated": "2025-09-04T09:23:46Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2508.20038v3",
    "arxiv_url": "http://arxiv.org/abs/2508.20038v3",
    "comment": "EMNLP 2025 findings",
    "relevance_score": 3.5
  },
  {
    "id": "2305.06166v3",
    "title": "Mitigating Bias in Text Classification via Prompt-Based Text   Transformation",
    "summary": "The presence of specific linguistic signals particular to a certain sub-group\ncan become highly salient to language models during training. In automated\ndecision-making settings, this may lead to biased outcomes when models rely on\ncues that correlate with protected characteristics. We investigate whether\nprompting ChatGPT to rewrite text using simplification, neutralisation,\nlocalisation, and formalisation can reduce demographic signals while preserving\nmeaning. Experimental results show a statistically significant drop in location\nclassification accuracy across multiple models after transformation, suggesting\nreduced reliance on group-specific language. At the same time, sentiment\nanalysis and rating prediction tasks confirm that the core meaning of the\nreviews remains greatly intact. These results suggest that prompt-based\nrewriting offers a practical and generalisable approach for mitigating bias in\ntext classification.",
    "authors": [
      "Charmaine Barker",
      "Dimitar Kazakov"
    ],
    "published": "2023-05-09T13:10:23Z",
    "updated": "2025-09-04T08:56:39Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2305.06166v3",
    "arxiv_url": "http://arxiv.org/abs/2305.06166v3",
    "comment": "This version corrects an error in the model specification",
    "relevance_score": 3.5
  },
  {
    "id": "2503.16561v3",
    "title": "FutureGen: A RAG-based Approach to Generate the Future Work of   Scientific Article",
    "summary": "The Future Work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from a scientific article.\nTo enrich the generation process with broader insights and reduce the chance of\nmissing important research directions, we use context from related papers using\nRAG. We experimented with various Large Language Models (LLMs) integrated into\nRetrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism\nto enhance the quality of the generated content and introduce an LLM-as-a-judge\nframework for robust evaluation, assessing key aspects such as novelty,\nhallucination, and feasibility. Our results demonstrate that the RAG-based\napproach using GPT-4o mini, combined with an LLM feedback mechanism,\noutperforms other methods based on both qualitative and quantitative\nevaluations. Moreover, we conduct a human evaluation to assess the LLM as an\nextractor, generator, and feedback provider.",
    "authors": [
      "Ibrahim Al Azher",
      "Miftahul Jannat Mokarrama",
      "Zhishuai Guo",
      "Sagnik Ray Choudhury",
      "Hamed Alhoori"
    ],
    "published": "2025-03-20T06:14:02Z",
    "updated": "2025-09-04T08:12:41Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2503.16561v3",
    "arxiv_url": "http://arxiv.org/abs/2503.16561v3",
    "comment": "12 pages, 6 figures, Accepted for publication at the Workshop on AI\n  Principles in Science Communication (Ai4SC'25), held in conjunction with the\n  IEEE eScience Conference 2025",
    "relevance_score": 3.5
  },
  {
    "id": "2503.23746v2",
    "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A   New Large Graph Model",
    "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.",
    "authors": [
      "Dizhan Xue",
      "Shengsheng Qian",
      "Chuanrui Hu",
      "Changsheng Xu"
    ],
    "published": "2025-03-31T05:53:15Z",
    "updated": "2025-09-04T05:10:37Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2503.23746v2",
    "arxiv_url": "http://arxiv.org/abs/2503.23746v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2407.01085v5",
    "title": "Explaining Length Bias in LLM-Based Preference Evaluations",
    "summary": "The use of large language models (LLMs) as judges, particularly in preference\ncomparisons, has become widespread, but this reveals a notable bias towards\nlonger responses, undermining the reliability of such evaluations. To better\nunderstand such bias, we propose to decompose the preference evaluation metric,\nspecifically the win rate, into two key components: desirability and\ninformation mass, where the former is length-independent and related to\ntrustworthiness such as correctness, toxicity, and consistency, and the latter\nis length-dependent and represents the amount of information in the response.\nWe empirically demonstrated the decomposition through controlled experiments\nand found that response length impacts evaluations by influencing information\nmass. To derive a reliable evaluation metric that assesses content quality\nwithout being confounded by response length, we propose AdapAlpaca, a simple\nyet effective adjustment to win rate measurement. Specifically, AdapAlpaca\nensures a fair comparison of response quality by aligning the lengths of\nreference and test model responses under equivalent length intervals.",
    "authors": [
      "Zhengyu Hu",
      "Linxin Song",
      "Jieyu Zhang",
      "Zheyuan Xiao",
      "Tianfu Wang",
      "Zhengyu Chen",
      "Nicholas Jing Yuan",
      "Jianxun Lian",
      "Kaize Ding",
      "Hui Xiong"
    ],
    "published": "2024-07-01T08:37:41Z",
    "updated": "2025-09-04T02:14:46Z",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2407.01085v5",
    "arxiv_url": "http://arxiv.org/abs/2407.01085v5",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2502.17882v2",
    "title": "Science Across Languages: Assessing LLM Multilingual Translation of   Scientific Papers",
    "summary": "Scientific research is inherently global. However, the vast majority of\nacademic journals are published exclusively in English, creating barriers for\nnon-native-English-speaking researchers. In this study, we leverage large\nlanguage models (LLMs) to translate published scientific articles while\npreserving their native JATS XML formatting, thereby developing a practical,\nautomated approach for implementation by academic journals. Using our approach,\nwe translate articles across multiple scientific disciplines into 28 languages.\nTo evaluate translation accuracy, we introduce a novel question-and-answer (QA)\nbenchmarking method, in which an LLM generates comprehension-based questions\nfrom the original text and then answers them based on the translated text. Our\nbenchmark results show an average performance of 95.9%, showing that the key\nscientific details are accurately conveyed. In a user study, we translate the\nscientific papers of 15 researchers into their native languages, finding that\nthe authors consistently found the translations to accurately capture the\noriginal information in their articles. Interestingly, a third of the authors\nfound many technical terms \"overtranslated,\" expressing a preference to keep\nterminology more familiar in English untranslated. Finally, we demonstrate how\nin-context learning techniques can be used to align translations with\ndomain-specific preferences such as mitigating overtranslation, highlighting\nthe adaptability and utility of LLM-driven scientific translation. The code and\ntranslated articles are available at https://hankleid.github.io/ProjectMundo.",
    "authors": [
      "Hannah Calzi Kleidermacher",
      "James Zou"
    ],
    "published": "2025-02-25T06:08:48Z",
    "updated": "2025-09-04T01:28:51Z",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2502.17882v2",
    "arxiv_url": "http://arxiv.org/abs/2502.17882v2",
    "comment": null,
    "relevance_score": 3.5
  },
  {
    "id": "2509.04356v1",
    "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic   Avatars",
    "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.",
    "authors": [
      "Atikkhan Faridkhan Nilgar",
      "Kristof Van Laerhoven",
      "Ayub Kinoti"
    ],
    "published": "2025-09-04T16:18:04Z",
    "updated": "2025-09-04T16:18:04Z",
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04356v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04356v1",
    "comment": null,
    "relevance_score": 3.0
  },
  {
    "id": "2509.04254v1",
    "title": "MuMTAffect: A Multimodal Multitask Affective Framework for Personality   and Emotion Recognition from Physiological Signals",
    "summary": "We present MuMTAffect, a novel Multimodal Multitask Affective Embedding\nNetwork designed for joint emotion classification and personality prediction\n(re-identification) from short physiological signal segments. MuMTAffect\nintegrates multiple physiological modalities pupil dilation, eye gaze, facial\naction units, and galvanic skin response using dedicated, transformer-based\nencoders for each modality and a fusion transformer to model cross-modal\ninteractions. Inspired by the Theory of Constructed Emotion, the architecture\nexplicitly separates core affect encoding (valence/arousal) from higher-level\nconceptualization, thereby grounding predictions in contemporary affective\nneuroscience. Personality trait prediction is leveraged as an auxiliary task to\ngenerate robust, user-specific affective embeddings, significantly enhancing\nemotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset,\ndemonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin\nresponse substantially improve arousal classification, while pupil and gaze\ndata enhance valence discrimination. The inherent modularity of MuMTAffect\nallows effortless integration of additional modalities, ensuring scalability\nand adaptability. Extensive experiments and ablation studies underscore the\nefficacy of our multimodal multitask approach in creating personalized,\ncontext-aware affective computing systems, highlighting pathways for further\nadvancements in cross-subject generalisation.",
    "authors": [
      "Meisam Jamshidi Seikavandi",
      "Fabricio Batista Narcizo",
      "Ted Vucurevich",
      "Andrew Burke Dittberner",
      "Paolo Burelli"
    ],
    "published": "2025-09-04T14:30:02Z",
    "updated": "2025-09-04T14:30:02Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.04254v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04254v1",
    "comment": null,
    "relevance_score": 3.0
  },
  {
    "id": "2504.13392v2",
    "title": "POET: Supporting Prompting Creativity and Personalization with Automated   Expansion of Text-to-Image Generation",
    "summary": "State-of-the-art visual generative AI tools hold immense potential to assist\nusers in the early ideation stages of creative tasks -- offering the ability to\ngenerate (rather than search for) novel and unprecedented (instead of existing)\nimages of considerable quality that also adhere to boundless combinations of\nuser specifications. However, many large-scale text-to-image systems are\ndesigned for broad applicability, yielding conventional output that may limit\ncreative exploration. They also employ interaction methods that may be\ndifficult for beginners. Given that creative end users often operate in\ndiverse, context-specific ways that are often unpredictable, more variation and\npersonalization are necessary. We introduce POET, a real-time interactive tool\nthat (1) automatically discovers dimensions of homogeneity in text-to-image\ngenerative models, (2) expands these dimensions to diversify the output space\nof generated images, and (3) learns from user feedback to personalize\nexpansions. An evaluation with 28 users spanning four creative task domains\ndemonstrated POET's ability to generate results with higher perceived diversity\nand help users reach satisfaction in fewer prompts during creative tasks,\nthereby prompting them to deliberate and reflect more on a wider range of\npossible produced results during the co-creative process. Focusing on visual\ncreativity, POET offers a first glimpse of how interaction techniques of future\ntext-to-image generation tools may support and align with more pluralistic\nvalues and the needs of end users during the ideation stages of their work.",
    "authors": [
      "Evans Xu Han",
      "Alice Qian Zhang",
      "Haiyi Zhu",
      "Hong Shen",
      "Paul Pu Liang",
      "Jane Hsieh"
    ],
    "published": "2025-04-18T00:54:36Z",
    "updated": "2025-09-04T03:45:36Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2504.13392v2",
    "arxiv_url": "http://arxiv.org/abs/2504.13392v2",
    "comment": null,
    "relevance_score": 3.0
  },
  {
    "id": "2509.03236v2",
    "title": "OneSearch: A Preliminary Exploration of the Unified End-to-End   Generative Framework for E-commerce Search",
    "summary": "Traditional e-commerce search systems employ multi-stage cascading\narchitectures (MCA) that progressively filter items through recall,\npre-ranking, and ranking stages. While effective at balancing computational\nefficiency with business conversion, these systems suffer from fragmented\ncomputation and optimization objective collisions across stages, which\nultimately limit their performance ceiling. To address these, we propose\n\\textbf{OneSearch}, the first industrial-deployed end-to-end generative\nframework for e-commerce search. This framework introduces three key\ninnovations: (1) a Keyword-enhanced Hierarchical Quantization Encoding (KHQE)\nmodule, to preserve both hierarchical semantics and distinctive item attributes\nwhile maintaining strong query-item relevance constraints; (2) a multi-view\nuser behavior sequence injection strategy that constructs behavior-driven user\nIDs and incorporates both explicit short-term and implicit long-term sequences\nto model user preferences comprehensively; and (3) a Preference-Aware Reward\nSystem (PARS) featuring multi-stage supervised fine-tuning and adaptive\nreward-weighted ranking to capture fine-grained user preferences. Extensive\noffline evaluations on large-scale industry datasets demonstrate OneSearch's\nsuperior performance for high-quality recall and ranking. The rigorous online\nA/B tests confirm its ability to enhance relevance in the same exposure\nposition, achieving statistically significant improvements: +1.67\\% item CTR,\n+2.40\\% buyer, and +3.22\\% order volume. Furthermore, OneSearch reduces\noperational expenditure by 75.40\\% and improves Model FLOPs Utilization from\n3.26\\% to 27.32\\%. The system has been successfully deployed across multiple\nsearch scenarios in Kuaishou, serving millions of users, generating tens of\nmillions of PVs daily.",
    "authors": [
      "Ben Chen",
      "Xian Guo",
      "Siyuan Wang",
      "Zihan Liang",
      "Yue Lv",
      "Yufei Ma",
      "Xinlong Xiao",
      "Bowen Xue",
      "Xuxin Zhang",
      "Ying Yang",
      "Huangyu Dai",
      "Xing Xu",
      "Tong Zhao",
      "Mingcan Peng",
      "Xiaoyang Zheng",
      "Chao Wang",
      "Qihang Zhao",
      "Zhixin Zhai",
      "Yang Zhao",
      "Bochao Liu",
      "Jingshan Lv",
      "Jing Chen",
      "Xiao Liang",
      "Yuqing Ding",
      "Chenyi Lei",
      "Wenwu Ou",
      "Han Li",
      "Kun Gai"
    ],
    "published": "2025-09-03T11:50:04Z",
    "updated": "2025-09-04T14:41:26Z",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2509.03236v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03236v2",
    "comment": null,
    "relevance_score": 3.0
  },
  {
    "id": "2507.12964v5",
    "title": "Demographic-aware fine-grained classification of pediatric wrist   fractures",
    "summary": "Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. Computer vision presents a promising\navenue, contingent upon the availability of extensive datasets, a notable\nchallenge in medical imaging. Therefore, reliance solely on one modality, such\nas images, proves inadequate, especially in an era of diverse and plentiful\ndata types. This study addresses the problem using a multifaceted approach:\nframing it as a fine-grained recognition task, fusing patient metadata with\nX-rays, and leveraging weights from a separate fine-grained dataset rather than\nfrom a coarse-grained dataset like ImageNet. Unlike prior work, this is the\nfirst application of metadata integration for wrist pathology recognition. Our\nresults show that combining fine-grained transformer approach, fine-grained\npre-training, and metadata integration improves diagnostic accuracy by 2% on\nsmall custom curated dataset and over 10% on a larger fracture dataset.",
    "authors": [
      "Ammar Ahmed",
      "Ali Shariq Imran",
      "Zenun Kastrati",
      "Sher Muhammad Daudpota"
    ],
    "published": "2025-07-17T10:03:57Z",
    "updated": "2025-09-04T16:55:10Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.12964v5",
    "arxiv_url": "http://arxiv.org/abs/2507.12964v5",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04379v1",
    "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
    "summary": "Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.",
    "authors": [
      "Jimin Xu",
      "Bosheng Qin",
      "Tao Jin",
      "Zhou Zhao",
      "Zhenhui Ye",
      "Jun Yu",
      "Fei Wu"
    ],
    "published": "2025-09-04T16:40:44Z",
    "updated": "2025-09-04T16:40:44Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04379v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04379v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.00167v2",
    "title": "Pilot Study on Generative AI and Critical Thinking in Higher Education   Classrooms",
    "summary": "Generative AI (GAI) tools have seen rapid adoption in educational settings,\nyet their role in fostering critical thinking remains underexplored. While\nprevious studies have examined GAI as a tutor for specific lessons or as a tool\nfor completing assignments, few have addressed how students critically evaluate\nthe accuracy and appropriateness of GAI-generated responses. This pilot study\ninvestigates students' ability to apply structured critical thinking when\nassessing Generative AI outputs in introductory Computational and Data Science\ncourses. Given that GAI tools often produce contextually flawed or factually\nincorrect answers, we designed learning activities that require students to\nanalyze, critique, and revise AI-generated solutions. Our findings offer\ninitial insights into students' ability to engage critically with GAI content\nand lay the groundwork for more comprehensive studies in future semesters.",
    "authors": [
      "W. F. Lamberti",
      "S. R. Lawrence",
      "D. White",
      "S. Kim",
      "S. Abdullah"
    ],
    "published": "2025-08-29T18:07:11Z",
    "updated": "2025-09-04T15:59:28Z",
    "categories": [
      "stat.AP",
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.00167v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00167v2",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04288v1",
    "title": "Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery   Systems with Data-Driven Formal Verification",
    "summary": "Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell.",
    "authors": [
      "Rudi Coppola",
      "Hovsep Touloujian",
      "Pierfrancesco Ombrini",
      "Manuel Mazo Jr"
    ],
    "published": "2025-09-04T15:01:03Z",
    "updated": "2025-09-04T15:01:03Z",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04288v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04288v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2405.17527v5",
    "title": "Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE   Solvers",
    "summary": "Deep models have recently emerged as promising tools to solve partial\ndifferential equations (PDEs), known as neural PDE solvers. While neural\nsolvers trained from either simulation data or physics-informed loss can solve\nPDEs reasonably well, they are mainly restricted to a few instances of PDEs,\ne.g. a certain equation with a limited set of coefficients. This limits their\ngeneralization to diverse PDEs, preventing them from being practical surrogate\nmodels of numerical solvers. In this paper, we present Unisolver, a novel\nTransformer model trained on diverse data and conditioned on diverse PDEs,\naiming towards a universal neural PDE solver capable of solving a wide scope of\nPDEs. Instead of purely scaling up data and parameters, Unisolver stems from\nthe theoretical analysis of the PDE-solving process. Inspired by the\nmathematical structure of PDEs that a PDE solution is fundamentally governed by\na series of PDE components such as equation symbols and boundary conditions, we\ndefine a complete set of PDE components and flexibly embed them as domain-wise\nand point-wise deep conditions for Transformer PDE solvers. Integrating\nphysical insights with recent Transformer advances, Unisolver achieves\nconsistent state-of-the-art on three challenging large-scale benchmarks,\nshowing impressive performance and generalizability. Code is available at\nhttps://github.com/thuml/Unisolver.",
    "authors": [
      "Hang Zhou",
      "Yuezhou Ma",
      "Haixu Wu",
      "Haowen Wang",
      "Mingsheng Long"
    ],
    "published": "2024-05-27T15:34:35Z",
    "updated": "2025-09-04T14:38:34Z",
    "categories": [
      "cs.NA",
      "math.NA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2405.17527v5",
    "arxiv_url": "http://arxiv.org/abs/2405.17527v5",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04260v1",
    "title": "An Empirical Study of Vulnerabilities in Python Packages and Their   Detection",
    "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.",
    "authors": [
      "Haowei Quan",
      "Junjie Wang",
      "Xinzhe Li",
      "Terry Yue Zhuo",
      "Xiao Chen",
      "Xiaoning Du"
    ],
    "published": "2025-09-04T14:38:28Z",
    "updated": "2025-09-04T14:38:28Z",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04260v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04260v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2501.13958v2",
    "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large   Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.",
    "authors": [
      "Qinggang Zhang",
      "Shengyuan Chen",
      "Yuanchen Bei",
      "Zheng Yuan",
      "Huachi Zhou",
      "Zijin Hong",
      "Hao Chen",
      "Yilin Xiao",
      "Chuang Zhou",
      "Yi Chang",
      "Xiao Huang"
    ],
    "published": "2025-01-21T06:25:21Z",
    "updated": "2025-09-04T14:24:19Z",
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2501.13958v2",
    "arxiv_url": "http://arxiv.org/abs/2501.13958v2",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04250v1",
    "title": "How many patients could we save with LLM priors?",
    "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.",
    "authors": [
      "Shota Arai",
      "David Selby",
      "Andrew Vargo",
      "Sebastian Vollmer"
    ],
    "published": "2025-09-04T14:23:35Z",
    "updated": "2025-09-04T14:23:35Z",
    "categories": [
      "stat.AP",
      "cs.ET",
      "stat.ME",
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04250v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04250v1",
    "comment": "9 pages, 4 figures",
    "relevance_score": 2.5
  },
  {
    "id": "2407.15161v4",
    "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via   Flow Variational Inference",
    "summary": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/).",
    "authors": [
      "Qian Feng",
      "Jianxiang Feng",
      "Zhaopeng Chen",
      "Rudolph Triebel",
      "Alois Knoll"
    ],
    "published": "2024-07-21T13:33:08Z",
    "updated": "2025-09-04T14:07:56Z",
    "categories": [
      "cs.AI",
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2407.15161v4",
    "arxiv_url": "http://arxiv.org/abs/2407.15161v4",
    "comment": "First two authors contributed equally, whose ordering decided via\n  coin-tossing. Accepted for CoRL 2025",
    "relevance_score": 2.5
  },
  {
    "id": "2411.03562v2",
    "title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level   Kaggle Data Science Performance",
    "summary": "Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.",
    "authors": [
      "Antoine Grosnit",
      "Alexandre Maraval",
      "Refinath S N",
      "Zichao Zhao",
      "James Dora",
      "Giuseppe Paolo",
      "Albert Thomas",
      "Jonas Gonzalez",
      "Abhineet Kumar",
      "Khyati Khandelwal",
      "Abdelhakim Benechehab",
      "Hamza Cherkaoui",
      "Youssef Attia El-Hili",
      "Kun Shao",
      "Jianye Hao",
      "Jun Yao",
      "Balázs Kégl",
      "Jun Wang"
    ],
    "published": "2024-11-05T23:55:23Z",
    "updated": "2025-09-04T14:07:26Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2411.03562v2",
    "arxiv_url": "http://arxiv.org/abs/2411.03562v2",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2508.08524v3",
    "title": "StreetViewAI: Making Street View Accessible Using Context-Aware   Multimodal AI",
    "summary": "Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work.",
    "authors": [
      "Jon E. Froehlich",
      "Alexander Fiannaca",
      "Nimer Jaber",
      "Victor Tsaran",
      "Shaun Kane"
    ],
    "published": "2025-08-11T23:30:39Z",
    "updated": "2025-09-04T13:56:50Z",
    "categories": [
      "cs.AI",
      "H.5; I.2",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.08524v3",
    "arxiv_url": "http://arxiv.org/abs/2508.08524v3",
    "comment": "Accepted to UIST'25 v2. Fixed a missing word in the PDF v3. Fixed a\n  typo in an author's name",
    "relevance_score": 2.5
  },
  {
    "id": "2507.22627v2",
    "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text   Pairing",
    "summary": "Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.",
    "authors": [
      "Federico Girella",
      "Davide Talon",
      "Ziyue Liu",
      "Zanxi Ruan",
      "Yiming Wang",
      "Marco Cristani"
    ],
    "published": "2025-07-30T12:48:29Z",
    "updated": "2025-09-04T13:26:56Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.22627v2",
    "arxiv_url": "http://arxiv.org/abs/2507.22627v2",
    "comment": "Accepted at ICCV25 (Oral). Project page:\n  https://intelligolabs.github.io/lots/",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04154v1",
    "title": "Attention as an Adaptive Filter",
    "summary": "We introduce Adaptive Filter Attention (AFA), a novel attention mechanism\nthat incorporates a learnable dynamics model directly into the computation of\nattention weights. Rather than comparing queries and keys directly, we model\nthe input sequence as discrete observations of a linear stochastic differential\nequation (SDE). By imposing a linear dynamics model with simultaneously\ndiagonalizable state matrices and noise covariances, we can make use of a\nclosed-form solution to the differential Lyapunov equation to efficiently\npropagate pairwise uncertainties through the dynamics. Attention naturally\narises as the maximum likelihood solution for this linear SDE, with attention\nweights corresponding to robust residual-based reweightings of the propagated\npairwise precisions. Imposing an additional constraint on the state matrix's\neigenvalues leads to a simplified variant with the same computational and\nmemory complexity as standard attention. In the limit of vanishing dynamics and\nprocess noise, and using a small-angle approximation, we recover ordinary\ndot-product attention.",
    "authors": [
      "Peter Racioppo"
    ],
    "published": "2025-09-04T12:29:14Z",
    "updated": "2025-09-04T12:29:14Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04154v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04154v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2411.00265v3",
    "title": "Quantifying Calibration Error in Neural Networks Through Evidence-Based   Theory",
    "summary": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.",
    "authors": [
      "Koffi Ismael Ouattara",
      "Ioannis Krontiris",
      "Theo Dimitrakos",
      "Frank Kargl"
    ],
    "published": "2024-10-31T23:54:21Z",
    "updated": "2025-09-04T12:15:53Z",
    "categories": [
      "math.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2411.00265v3",
    "arxiv_url": "http://arxiv.org/abs/2411.00265v3",
    "comment": "This is the preprint of the paper accepted to Fusion 2025 (28th\n  International Conference on Information Fusion, Rio de Janeiro, Brazil, July\n  7-10, 2025). The published version is available at\n  https://doi.org/10.23919/FUSION65864.2025.11124121",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04100v1",
    "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning",
    "summary": "This paper explores the combination of Reinforcement Learning (RL) and\nsearch-based path planners to speed up the optimization of flight paths for\nairliners, where in case of emergency a fast route re-calculation can be\ncrucial. The fundamental idea is to train an RL Agent to pre-compute\nnear-optimal paths based on location and atmospheric data and use those at\nruntime to constrain the underlying path planning solver and find a solution\nwithin a certain distance from the initial guess. The approach effectively\nreduces the size of the solver's search space, significantly speeding up route\noptimization. Although global optimality is not guaranteed, empirical results\nconducted with Airbus aircraft's performance models show that fuel consumption\nremains nearly identical to that of an unconstrained solver, with deviations\ntypically within 1%. At the same time, computation speed can be improved by up\nto 50% as compared to using a conventional solver alone.",
    "authors": [
      "Alberto Luise",
      "Michele Lombardi",
      "Florent Teichteil Koenigsbuch"
    ],
    "published": "2025-09-04T11:01:43Z",
    "updated": "2025-09-04T11:01:43Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04100v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04100v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2406.13923v2",
    "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal   Documents",
    "summary": "Recent advancements in large multimodal models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. To address these issues, we\nintroduce PIN (Paired and INterleaved multimodal documents), a novel data\nformat designed to foster a deeper integration of visual and textual knowledge.\nThe PIN format uniquely combines semantically rich Markdown files, which\npreserve fine-grained textual structures, with holistic overall images that\ncapture the complete document layout. Following this format, we construct and\nrelease two large-scale, open-source datasets: PIN-200M (~200 million\ndocuments) and PIN-14M (~14 million), compiled from diverse web and scientific\nsources in both English and Chinese. To maximize usability, we provide detailed\nstatistical analyses and equip the datasets with quality signals, enabling\nresearchers to easily filter and select data for specific tasks. Our work\nprovides the community with a versatile data format and substantial resources,\noffering a foundation for new research in pre-training strategies and the\ndevelopment of more powerful knowledge-intensive LMMs.",
    "authors": [
      "Junjie Wang",
      "Yuxiang Zhang",
      "Minghao Liu",
      "Yin Zhang",
      "Yatai Ji",
      "Weihao Xuan",
      "Nie Lin",
      "Kang Zhu",
      "Zhiqiang Lin",
      "Yiming Ren",
      "Chunyang Jiang",
      "Yiyao Yu",
      "Zekun Wang",
      "Tiezhen Wang",
      "Wenhao Huang",
      "Jie Fu",
      "Qunshu Liu",
      "Yujiu Yang",
      "Ge Zhang",
      "Ruibin Yuan",
      "Bei Chen",
      "Wenhu Chen"
    ],
    "published": "2024-06-20T01:43:08Z",
    "updated": "2025-09-04T10:10:23Z",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2406.13923v2",
    "arxiv_url": "http://arxiv.org/abs/2406.13923v2",
    "comment": "Technical report v1.0",
    "relevance_score": 2.5
  },
  {
    "id": "2507.15269v2",
    "title": "Conditional Video Generation for High-Efficiency Video Compression",
    "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.",
    "authors": [
      "Fangqiu Yi",
      "Jingyu Xu",
      "Jiawei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "published": "2025-07-21T06:16:27Z",
    "updated": "2025-09-04T09:36:19Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.15269v2",
    "arxiv_url": "http://arxiv.org/abs/2507.15269v2",
    "comment": "Critical methodology flaws invalidate key results",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04430v1",
    "title": "Unveiling the Role of Data Uncertainty in Tabular Deep Learning",
    "summary": "Recent advancements in tabular deep learning have demonstrated exceptional\npractical performance, yet the field often lacks a clear understanding of why\nthese techniques actually succeed. To address this gap, our paper highlights\nthe importance of the concept of data uncertainty for explaining the\neffectiveness of the recent tabular DL methods. In particular, we reveal that\nthe success of many beneficial design choices in tabular DL, such as numerical\nfeature embeddings, retrieval-augmented models and advanced ensembling\nstrategies, can be largely attributed to their implicit mechanisms for managing\nhigh data uncertainty. By dissecting these mechanisms, we provide a unifying\nunderstanding of the recent performance improvements. Furthermore, the insights\nderived from this data-uncertainty perspective directly allowed us to develop\nmore effective numerical feature embeddings as an immediate practical outcome\nof our analysis. Overall, our work paves the way to foundational understanding\nof the benefits introduced by modern tabular methods that results in the\nconcrete advancements of existing techniques and outlines future research\ndirections for tabular DL.",
    "authors": [
      "Nikolay Kartashev",
      "Ivan Rubachev",
      "Artem Babenko"
    ],
    "published": "2025-09-04T17:49:59Z",
    "updated": "2025-09-04T17:49:59Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04430v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04430v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04232v1",
    "title": "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit   Objectives and Privacy Budget Allocation",
    "summary": "Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially\nprivate deep learning by injecting noise into partitioned gradient vectors.\nHowever, existing methods often rely on heuristic noise allocation strategies,\nlacking a rigorous understanding of their theoretical grounding in connecting\nnoise allocation to formal privacy-utility tradeoffs. In this paper, we present\na unified analytical framework that systematically connects layer-wise noise\ninjection strategies with their implicit optimization objectives and associated\nprivacy budget allocations. Our analysis reveals that several existing\napproaches optimize ill-posed objectives -- either ignoring inter-layer\nsignal-to-noise ratio (SNR) consistency or leading to inefficient use of the\nprivacy budget. In response, we propose a SNR-Consistent noise allocation\nstrategy that unifies both aspects, yielding a noise allocation scheme that\nachieves better signal preservation and more efficient privacy budget\nutilization. Extensive experiments in both centralized and federated learning\nsettings demonstrate that our method consistently outperforms existing\nallocation strategies, achieving better privacy-utility tradeoffs. Our\nframework not only offers diagnostic insights into prior methods but also\nprovides theoretical guidance for designing adaptive and effective noise\ninjection schemes in deep models.",
    "authors": [
      "Qifeng Tan",
      "Shusen Yang",
      "Xuebin Ren",
      "Yikai Zhang"
    ],
    "published": "2025-09-04T14:09:46Z",
    "updated": "2025-09-04T14:09:46Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04232v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04232v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04185v1",
    "title": "Set Block Decoding is a Language Model Inference Accelerator",
    "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
    "authors": [
      "Itai Gat",
      "Heli Ben-Hamu",
      "Marton Havasi",
      "Daniel Haziza",
      "Jeremy Reizenstein",
      "Gabriel Synnaeve",
      "David Lopez-Paz",
      "Brian Karrer",
      "Yaron Lipman"
    ],
    "published": "2025-09-04T13:02:39Z",
    "updated": "2025-09-04T13:02:39Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04185v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04185v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04178v1",
    "title": "Comment on \"A Note on Over-Smoothing for Graph Neural Networks\"",
    "summary": "We comment on Cai and Wang (2020, arXiv:2006.13318), who analyze\nover-smoothing in GNNs via Dirichlet energy. We show that under mild spectral\nconditions (including with Leaky-ReLU), the Dirichlet energy of node embeddings\ndecreases exponentially with depth; we further extend the result to spectral\npolynomial filters and provide a short proof for the Leaky-ReLU case.\nExperiments on edge deletion and weight amplification illustrate when Dirichlet\nenergy increases, hinting at practical ways to relieve over-smoothing.",
    "authors": [
      "Razi Hasson",
      "Reuven Guetta"
    ],
    "published": "2025-09-04T12:53:37Z",
    "updated": "2025-09-04T12:53:37Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04178v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04178v1",
    "comment": "Comment on arXiv:2006.13318 (Cai & Wang, 2020). Revisits their\n  Dirichlet-energy analysis of over-smoothing and extends it to Leaky-ReLU and\n  spectral polynomial filters; includes Proposition 7.1 and a new proof of\n  Lemma 3.3 for Leaky-ReLU. 7 pages",
    "relevance_score": 2.5
  },
  {
    "id": "2504.08821v2",
    "title": "Probabilistic QoS Metric Forecasting in Delay-Tolerant Networks Using   Conditional Diffusion Models on Latent Dynamics",
    "summary": "Active QoS metric prediction, commonly employed in the maintenance and\noperation of DTN, could enhance network performance regarding latency,\nthroughput, energy consumption, and dependability. Naturally formulated as a\nmultivariate time series forecasting problem, it attracts substantial research\nefforts. Traditional mean regression methods for time series forecasting cannot\ncapture the data complexity adequately, resulting in deteriorated performance\nin operational tasks in DTNs such as routing. This paper formulates the\nprediction of QoS metrics in DTN as a probabilistic forecasting problem on\nmultivariate time series, where one could quantify the uncertainty of forecasts\nby characterizing the distribution of these samples. The proposed approach\nhires diffusion models and incorporates the latent temporal dynamics of\nnon-stationary and multi-mode data into them. Extensive experiments demonstrate\nthe efficacy of the proposed approach by showing that it outperforms the\npopular probabilistic time series forecasting methods.",
    "authors": [
      "Jianhua Liu",
      "Zheng Liu",
      "Yu Xiang",
      "Yanwen Qu"
    ],
    "published": "2025-04-09T15:40:02Z",
    "updated": "2025-09-04T10:49:22Z",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2504.08821v2",
    "arxiv_url": "http://arxiv.org/abs/2504.08821v2",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04444v1",
    "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision",
    "summary": "Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama",
    "authors": [
      "Xin Lin",
      "Xian Ge",
      "Dizhe Zhang",
      "Zhaoliang Wan",
      "Xianshun Wang",
      "Xiangtai Li",
      "Wenjie Jiang",
      "Bo Du",
      "Dacheng Tao",
      "Ming-Hsuan Yang",
      "Lu Qi"
    ],
    "published": "2025-09-04T17:59:10Z",
    "updated": "2025-09-04T17:59:10Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04444v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04444v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04434v1",
    "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
    "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
    "authors": [
      "Hyunsoo Cha",
      "Byungjun Kim",
      "Hanbyul Joo"
    ],
    "published": "2025-09-04T17:53:03Z",
    "updated": "2025-09-04T17:53:03Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04434v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04434v1",
    "comment": "Project Page: https://hyunsoocha.github.io/durian",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04406v1",
    "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
    "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
    "authors": [
      "Zanwei Zhou",
      "Taoran Yi",
      "Jiemin Fang",
      "Chen Yang",
      "Lingxi Xie",
      "Xinggang Wang",
      "Wei Shen",
      "Qi Tian"
    ],
    "published": "2025-09-04T17:24:31Z",
    "updated": "2025-09-04T17:24:31Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04406v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04406v1",
    "comment": "Project page: https://github.com/Zanue/MDT-dist",
    "relevance_score": 2.5
  },
  {
    "id": "2505.02980v2",
    "title": "Completing Spatial Transcriptomics Data for Gene Expression Prediction   Benchmarking",
    "summary": "Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.",
    "authors": [
      "Daniela Ruiz",
      "Paula Cárdenas",
      "Leonardo Manrique",
      "Daniela Vega",
      "Gabriel M. Mejia",
      "Pablo Arbeláez"
    ],
    "published": "2025-05-05T19:17:29Z",
    "updated": "2025-09-04T15:06:07Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2505.02980v2",
    "arxiv_url": "http://arxiv.org/abs/2505.02980v2",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2407.13027",
    "relevance_score": 2.5
  },
  {
    "id": "2508.18733v3",
    "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from   Vector Drawings",
    "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.",
    "authors": [
      "Feiwei Qin",
      "Shichao Lu",
      "Junhao Hou",
      "Changmiao Wang",
      "Meie Fang",
      "Ligang Liu"
    ],
    "published": "2025-08-26T07:01:58Z",
    "updated": "2025-09-04T14:22:42Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.18733v3",
    "arxiv_url": "http://arxiv.org/abs/2508.18733v3",
    "comment": "Accepted to ACM MM 2025",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04117v1",
    "title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset",
    "summary": "Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.",
    "authors": [
      "Mustafa Sakhai",
      "Kaung Sithu",
      "Min Khant Soe Oke",
      "Maciej Wielgosz"
    ],
    "published": "2025-09-04T11:30:29Z",
    "updated": "2025-09-04T11:30:29Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04117v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04117v1",
    "comment": "12 pages, 8 figures, 3 tables; dataset descriptor paper introducing\n  DVS-PedX (synthetic-and-real event-based pedestrian dataset with baselines)\n  External URL: https://doi.org/10.5281/zenodo.17030898",
    "relevance_score": 2.5
  },
  {
    "id": "2411.15537v4",
    "title": "MUNBa: Machine Unlearning via Nash Bargaining",
    "summary": "Machine Unlearning (MU) aims to selectively erase harmful behaviors from\nmodels while retaining the overall utility of the model. As a multi-task\nlearning problem, MU involves balancing objectives related to forgetting\nspecific concepts/data and preserving general performance. A naive integration\nof these forgetting and preserving objectives can lead to gradient conflicts\nand dominance, impeding MU algorithms from reaching optimal solutions. To\naddress the gradient conflict and dominance issue, we reformulate MU as a\ntwo-player cooperative game, where the two players, namely, the forgetting\nplayer and the preservation player, contribute via their gradient proposals to\nmaximize their overall gain and balance their contributions. To this end,\ninspired by the Nash bargaining theory, we derive a closed-form solution to\nguide the model toward the Pareto stationary point. Our formulation of MU\nguarantees an equilibrium solution, where any deviation from the final state\nwould lead to a reduction in the overall objectives for both players, ensuring\noptimality in each objective. We evaluate our algorithm's effectiveness on a\ndiverse set of tasks across image classification and image generation.\nExtensive experiments with ResNet, vision-language model CLIP, and\ntext-to-image diffusion models demonstrate that our method outperforms\nstate-of-the-art MU algorithms, achieving a better trade-off between forgetting\nand preserving. Our results also highlight improvements in forgetting\nprecision, preservation of generalization, and robustness against adversarial\nattacks.",
    "authors": [
      "Jing Wu",
      "Mehrtash Harandi"
    ],
    "published": "2024-11-23T12:18:28Z",
    "updated": "2025-09-04T11:00:46Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2411.15537v4",
    "arxiv_url": "http://arxiv.org/abs/2411.15537v4",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2309.16494v3",
    "title": "Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization",
    "summary": "Recently, deep learning-based methods have dominated image dehazing domain. A\nmulti-receptive-field non-local network (MRFNLN) consisting of the multi-stream\nfeature attention block (MSFAB) and the cross non-local block (CNLB) is\npresented in this paper to further enhance the performance. We start with\nextracting richer features for dehazing. Specifically, a multi-stream feature\nextraction (MSFE) sub-block, which contains three parallel convolutions with\ndifferent receptive fields (i.e., $1\\times 1$, $3\\times 3$, $5\\times 5$), is\ndesigned for extracting multi-scale features. Following MSFE, an attention\nsub-block is employed to make the model adaptively focus on important\nchannels/regions. These two sub-blocks constitute our MSFAB. Then, we design a\ncross non-local block (CNLB), which can capture long-range dependencies beyond\nthe query. Instead of the same input source of query branch, the key and value\nbranches are enhanced by fusing more preceding features. CNLB is\ncomputation-friendly by leveraging a spatial pyramid down-sampling (SPDS)\nstrategy to reduce the computation and memory consumption without sacrificing\nthe performance. Last but not least, a novel detail-focused contrastive\nregularization (DFCR) is presented by emphasizing the low-level details and\nignoring the high-level semantic information in a representation space\nspecially designed for dehazing. Comprehensive experimental results demonstrate\nthat the proposed MRFNLN model outperforms recent state-of-the-art dehazing\nmethods with less than 1.5 Million parameters.",
    "authors": [
      "Zewei He",
      "Zixuan Chen",
      "Jinlei Li",
      "Ziqian Lu",
      "Xuecheng Sun",
      "Hao Luo",
      "Zhe-Ming Lu",
      "Evangelos K. Markakis"
    ],
    "published": "2023-09-28T14:59:16Z",
    "updated": "2025-09-04T09:03:13Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2309.16494v3",
    "arxiv_url": "http://arxiv.org/abs/2309.16494v3",
    "comment": "submitted to the IEEE Journal for possible publication",
    "relevance_score": 2.5
  },
  {
    "id": "2508.18826v2",
    "title": "SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation",
    "summary": "Recent studies have shown that Machine Learning (ML) models can exhibit bias\nin real-world scenarios, posing significant challenges in ethically sensitive\ndomains such as healthcare. Such bias can negatively affect model fairness,\nmodel generalization abilities and further risks amplifying social\ndiscrimination. There is a need to remove biases from trained models. Existing\ndebiasing approaches often necessitate access to original training data and\nneed extensive model retraining; they also typically exhibit trade-offs between\nmodel fairness and discriminative performance. To address these challenges, we\npropose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that\nefficiently improves fairness while preserving discriminative performance with\nmuch less debiasing costs. Notably, SWiFT requires only a small external\ndataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to\nfirst find the relative, and yet distinct, contributions of model parameters to\nboth bias and predictive performance. Then, a two-step fine-tuning process\nupdates each parameter with different gradient flows defined by its\ncontribution. Extensive experiments with three bias sensitive attributes\n(gender, skin tone, and age) across four dermatological and two chest X-ray\ndatasets demonstrate that SWiFT can consistently reduce model bias while\nachieving competitive or even superior diagnostic accuracy under common\nfairness and accuracy metrics, compared to the state-of-the-art. Specifically,\nwe demonstrate improved model generalization ability as evidenced by superior\nperformance on several out-of-distribution (OOD) datasets.",
    "authors": [
      "Junyu Yan",
      "Feng Chen",
      "Yuyang Xue",
      "Yuning Du",
      "Konstantinos Vilouras",
      "Sotirios A. Tsaftaris",
      "Steven McDonagh"
    ],
    "published": "2025-08-26T09:03:18Z",
    "updated": "2025-09-04T08:59:58Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.18826v2",
    "arxiv_url": "http://arxiv.org/abs/2508.18826v2",
    "comment": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2025:015",
    "relevance_score": 2.5
  },
  {
    "id": "2508.19762v3",
    "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions",
    "summary": "Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nanthropogenic and environmental stressors. Scalable, automated monitoring in\nagricultural environments remains an open challenge due to the difficulty of\ndetecting small, fast-moving, and often camouflaged insects. To address this,\nwe present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator\nimages collected under real field conditions. BuzzSet contains 7,856 manually\nverified images with more than 8,000 annotated instances across three classes:\nhoneybees, bumblebees, and unidentified insects. Initial annotations were\nproduced using a YOLOv12 model trained on external data and refined through\nhuman verification with open-source tools. All images were preprocessed into\n256 x 256 tiles to improve the detection of small insects. We provide baselines\nusing the RF-DETR transformer-based object detector. The model achieves strong\nclassification accuracy with F1 scores of 0.94 and 0.92 for honeybees and\nbumblebees, with minimal confusion between these categories. The unidentified\nclass remains more difficult due to label ambiguity and fewer samples, yet\nstill contributes insights for robustness evaluation. Overall detection\nperformance (mAP at 0.50 of 0.559) illustrates the challenging nature of the\ndataset and its potential to drive advances in small object detection under\nrealistic ecological conditions. Future work focuses on expanding the dataset\nto version 2.0 with additional annotations and evaluating further detection\nstrategies. BuzzSet establishes a benchmark for ecological computer vision,\nwith the primary challenge being reliable detection of insects frequently\ncamouflaged within natural vegetation, highlighting an open problem for future\nresearch.",
    "authors": [
      "Ahmed Emam",
      "Mohamed Elbassiouny",
      "Julius Miller",
      "Patrick Donworth",
      "Sabine Seidel",
      "Ribana Roscher"
    ],
    "published": "2025-08-27T10:40:15Z",
    "updated": "2025-09-04T08:58:39Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.19762v3",
    "arxiv_url": "http://arxiv.org/abs/2508.19762v3",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2505.20789v3",
    "title": "Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models",
    "summary": "Inverse problems (IPs) involve reconstructing signals from noisy\nobservations. Recently, diffusion models (DMs) have emerged as a powerful\nframework for solving IPs, achieving remarkable reconstruction performance.\nHowever, existing DM-based methods frequently encounter issues such as heavy\ncomputational demands and suboptimal convergence. In this work, building upon\nthe idea of the recent work DMPlug, we propose two novel methods, DMILO and\nDMILO-PGD, to address these challenges. Our first method, DMILO, employs\nintermediate layer optimization (ILO) to alleviate the memory burden inherent\nin DMPlug. Additionally, by introducing sparse deviations, we expand the range\nof DMs, enabling the exploration of underlying signals that may lie outside the\nrange of the diffusion model. We further propose DMILO-PGD, which integrates\nILO with projected gradient descent (PGD), thereby reducing the risk of\nsuboptimal convergence. We provide an intuitive theoretical analysis of our\napproaches under appropriate conditions and validate their superiority through\nextensive experiments on diverse image datasets, encompassing both linear and\nnonlinear IPs. Our results demonstrate significant performance gains over\nstate-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD\nin addressing common challenges in DM-based IP solvers.",
    "authors": [
      "Yang Zheng",
      "Wen Li",
      "Zhaoqiang Liu"
    ],
    "published": "2025-05-27T06:49:02Z",
    "updated": "2025-09-04T07:37:08Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2505.20789v3",
    "arxiv_url": "http://arxiv.org/abs/2505.20789v3",
    "comment": "ICML 2025",
    "relevance_score": 2.5
  },
  {
    "id": "2502.12065v3",
    "title": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical   Definitions",
    "summary": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions: a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalization, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we augment LLMs'\nformalizations through relevant contextual elements from formal mathematical\nlibraries. Our findings reveal that definitions present a greater challenge\ncompared to existing benchmarks, such as miniF2F. In particular, we found that\nLLMs still struggle with self-correction, and aligning with relevant\nmathematical libraries. At the same time, structured refinement methods and\ndefinition grounding strategies yield notable improvements of up to 16% on\nself-correction capabilities and 43% on the reduction of undefined errors,\nhighlighting promising directions for enhancing LLM-based autoformalization in\nreal-world scenarios.",
    "authors": [
      "Lan Zhang",
      "Marco Valentino",
      "Andre Freitas"
    ],
    "published": "2025-02-17T17:34:48Z",
    "updated": "2025-09-04T12:43:14Z",
    "categories": [
      "cs.CL",
      "cs.FL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2502.12065v3",
    "arxiv_url": "http://arxiv.org/abs/2502.12065v3",
    "comment": "EMNLP 2025 Camera-Ready Version",
    "relevance_score": 2.5
  },
  {
    "id": "2509.04104v1",
    "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue",
    "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
    "authors": [
      "Keara Schaaij",
      "Roel Boumans",
      "Tibor Bosse",
      "Iris Hendrickx"
    ],
    "published": "2025-09-04T11:07:27Z",
    "updated": "2025-09-04T11:07:27Z",
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04104v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04104v1",
    "comment": "Accepted for TSD 2025",
    "relevance_score": 2.5
  },
  {
    "id": "2504.12311v4",
    "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer",
    "summary": "Prompt tuning has emerged as a lightweight strategy for adapting foundation\nmodels to downstream tasks, particularly for resource-constrained systems. As\npre-trained prompts become valuable assets, combining multiple source prompts\noffers a promising approach to enhance generalization for new tasks by\nleveraging complementary knowledge. However, naive aggregation often overlooks\ndifferent source prompts have different contribution potential to the target\ntask. To address this, we propose HGPrompt, a dynamic framework that learns\noptimal ensemble weights. These weights are optimized by jointly maximizing an\ninformation-theoretic metric for transferability and minimizing gradient\nconflicts via a novel regularization strategy. Specifically, we propose a\ndifferentiable prompt transferability metric to captures the discriminability\nof prompt-induced features on the target task. Meanwhile, HGPrompt match the\ngradient variances with respect to different source prompts based on Hessian\nand Fisher Information, ensuring stable and coherent knowledge transfer while\nsuppressing gradient conflicts among them. Extensive experiments on the\nlarge-scale VTAB benchmark demonstrate the state-of-the-art performance of\nHGPrompt, validating its effectiveness in learning an optimal ensemble for\neffective multi-source prompt transfer.",
    "authors": [
      "Jianhua Liu",
      "Liwen Cao",
      "Yanru Wu",
      "Zijie Zhao",
      "Yang Li"
    ],
    "published": "2025-04-09T08:40:21Z",
    "updated": "2025-09-04T10:49:13Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2504.12311v4",
    "arxiv_url": "http://arxiv.org/abs/2504.12311v4",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.03940v1",
    "title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based   Role-Playing Agents",
    "summary": "Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.",
    "authors": [
      "Weihao Wu",
      "Liang Cao",
      "Xinyu Wu",
      "Zhiwei Lin",
      "Rui Niu",
      "Jingbei Li",
      "Zhiyong Wu"
    ],
    "published": "2025-09-04T07:03:46Z",
    "updated": "2025-09-04T07:03:46Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03940v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03940v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.03932v1",
    "title": "Decoding the Poetic Language of Emotion in Korean Modern Poetry:   Insights from a Human-Labeled Dataset and AI Modeling",
    "summary": "This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset\nfor computational emotion analysis in modern Korean poetry. Despite remarkable\nprogress in text-based emotion classification using large language models,\npoetry-particularly Korean poetry-remains underexplored due to its figurative\nlanguage and cultural specificity. We built a multi-label emotion dataset of\n7,662 entries, including 7,007 line-level entries from 483 poems and 615\nwork-level entries, annotated with 44 fine-grained emotion categories from five\ninfluential Korean poets. A state-of-the-art Korean language model fine-tuned\non this dataset significantly outperformed previous models, achieving 0.60\nF1-micro compared to 0.34 from models trained on general corpora. The KPoEM\nmodel, trained through sequential fine-tuning-first on general corpora and then\non the KPoEM dataset-demonstrates not only an enhanced ability to identify\ntemporally and culturally specific emotional expressions, but also a strong\ncapacity to preserve the core sentiments of modern Korean poetry. This study\nbridges computational methods and literary analysis, presenting new\npossibilities for the quantitative exploration of poetic emotions through\nstructured data that faithfully retains the emotional and cultural nuances of\nKorean literature.",
    "authors": [
      "Iro Lim",
      "Haein Ji",
      "Byungjun Kim"
    ],
    "published": "2025-09-04T06:45:39Z",
    "updated": "2025-09-04T06:45:39Z",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03932v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03932v1",
    "comment": "30 pages, 13 tables, 2 figures, Digital Humanities and Social\n  Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies,\n  University of Pennsylvania, Philadelphia, USA",
    "relevance_score": 2.5
  },
  {
    "id": "2509.03888v1",
    "title": "False Sense of Security: Why Probing-based Malicious Input Detection   Fails to Generalize",
    "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.",
    "authors": [
      "Cheng Wang",
      "Zeming Wei",
      "Qin Liu",
      "Muhao Chen"
    ],
    "published": "2025-09-04T05:15:55Z",
    "updated": "2025-09-04T05:15:55Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03888v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03888v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.03829v1",
    "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio   Deepfake Detection via Attention Aggregation",
    "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.",
    "authors": [
      "Huhong Xian",
      "Rui Liu",
      "Berrak Sisman",
      "Haizhou Li"
    ],
    "published": "2025-09-04T02:33:00Z",
    "updated": "2025-09-04T02:33:00Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03829v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03829v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.03809v1",
    "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long   Document-Level Machine Translation",
    "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.",
    "authors": [
      "Jiaxin Guo",
      "Daimeng Wei",
      "Yuanchang Luo",
      "Xiaoyu Chen",
      "Zhanglin Wu",
      "Huan Yang",
      "Hengchao Shang",
      "Zongyao Li",
      "Zhiqiang Rao",
      "Jinlong Yang",
      "Hao Yang"
    ],
    "published": "2025-09-04T01:50:20Z",
    "updated": "2025-09-04T01:50:20Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03809v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03809v1",
    "comment": "under preview",
    "relevance_score": 2.5
  },
  {
    "id": "2509.03787v1",
    "title": "Evaluating the Robustness of Retrieval-Augmented Generation to   Adversarial Evidence in the Health Domain",
    "summary": "Retrieval augmented generation (RAG) systems provide a method for factually\ngrounding the responses of a Large Language Model (LLM) by providing retrieved\nevidence, or context, as support. Guided by this context, RAG systems can\nreduce hallucinations and expand the ability of LLMs to accurately answer\nquestions outside the scope of their training data. Unfortunately, this design\nintroduces a critical vulnerability: LLMs may absorb and reproduce\nmisinformation present in retrieved evidence. This problem is magnified if\nretrieved evidence contains adversarial material explicitly intended to\npromulgate misinformation. This paper presents a systematic evaluation of RAG\nrobustness in the health domain and examines alignment between model outputs\nand ground-truth answers. We focus on the health domain due to the potential\nfor harm caused by incorrect responses, as well as the availability of\nevidence-based ground truth for many common health-related questions. We\nconduct controlled experiments using common health questions, varying both the\ntype and composition of the retrieved documents (helpful, harmful, and\nadversarial) as well as the framing of the question by the user (consistent,\nneutral, and inconsistent). Our findings reveal that adversarial documents\nsubstantially degrade alignment, but robustness can be preserved when helpful\nevidence is also present in the retrieval pool. These findings offer actionable\ninsights for designing safer RAG systems in high-stakes domains by highlighting\nthe need for retrieval safeguards. To enable reproducibility and facilitate\nfuture research, all experimental results are publicly available in our github\nrepository.\n  https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL",
    "authors": [
      "Shakiba Amirshahi",
      "Amin Bigdeli",
      "Charles L. A. Clarke",
      "Amira Ghenai"
    ],
    "published": "2025-09-04T00:45:58Z",
    "updated": "2025-09-04T00:45:58Z",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03787v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03787v1",
    "comment": null,
    "relevance_score": 2.5
  },
  {
    "id": "2509.04244v1",
    "title": "Integrating Pruning with Quantization for Efficient Deep Neural Networks   Compression",
    "summary": "Deep Neural Networks (DNNs) have achieved significant advances in a wide\nrange of applications. However, their deployment on resource-constrained\ndevices remains a challenge due to the large number of layers and parameters,\nwhich result in considerable computational and memory demands. To address this\nissue, pruning and quantization are two widely used compression techniques,\ncommonly applied individually in most studies to reduce model size and enhance\nprocessing speed. Nevertheless, combining these two techniques can yield even\ngreater compression benefits. Effectively integrating pruning and quantization\nto harness their complementary advantages poses a challenging task, primarily\ndue to their potential impact on model accuracy and the complexity of jointly\noptimizing both processes. In this paper, we propose two approaches that\nintegrate similarity-based filter pruning with Adaptive Power-of-Two (APoT)\nquantization to achieve higher compression efficiency while preserving model\naccuracy. In the first approach, pruning and quantization are applied\nsimultaneously during training. In the second approach, pruning is performed\nfirst to remove less important parameters, followed by quantization of the\npruned model using low-bit representations. Experimental results demonstrate\nthat our proposed approaches achieve effective model compression with minimal\naccuracy degradation, making them well-suited for deployment on devices with\nlimited computational resources.",
    "authors": [
      "Sara Makenali",
      "Babak Rokh",
      "Ali Azarpeyvand"
    ],
    "published": "2025-09-04T14:17:28Z",
    "updated": "2025-09-04T14:17:28Z",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE",
    "pdf_url": "http://arxiv.org/pdf/2509.04244v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04244v1",
    "comment": null,
    "relevance_score": 2.0
  },
  {
    "id": "2509.03910v1",
    "title": "An invertible generative model for forward and inverse problems",
    "summary": "We formulate the inverse problem in a Bayesian framework and aim to train a\ngenerative model that allows us to simulate (i.e., sample from the likelihood)\nand do inference (i.e., sample from the posterior). We review the use of\ntriangular normalizing flows for conditional sampling in this context and show\nhow to combine two such triangular maps (an upper and a lower one) in to one\ninvertible mapping that can be used for simulation and inference. We work out\nseveral useful properties of this invertible generative model and propose a\npossible training loss for training the map directly. We illustrate the\nworkings of this new approach to conditional generative modeling numerically on\na few stylized examples.",
    "authors": [
      "Tristan van Leeuwen",
      "Christoph Brune",
      "Marcello Carioni"
    ],
    "published": "2025-09-04T06:04:10Z",
    "updated": "2025-09-04T06:04:10Z",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.03910v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03910v1",
    "comment": null,
    "relevance_score": 2.0
  },
  {
    "id": "2507.01044v2",
    "title": "Asymptotic convexity of wide and shallow neural networks",
    "summary": "For a simple model of shallow and wide neural networks, we show that the\nepigraph of its input-output map as a function of the network parameters\napproximates epigraph of a. convex function in a precise sense. This leads to a\nplausible explanation of their observed good performance.",
    "authors": [
      "Vivek Borkar",
      "Parthe Pandit"
    ],
    "published": "2025-06-23T20:26:14Z",
    "updated": "2025-09-04T03:48:14Z",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "68T07"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2507.01044v2",
    "arxiv_url": "http://arxiv.org/abs/2507.01044v2",
    "comment": "5 pages",
    "relevance_score": 2.0
  },
  {
    "id": "2509.04069v1",
    "title": "Solving Robotics Tasks with Prior Demonstration via   Exploration-Efficient Deep Reinforcement Learning",
    "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.",
    "authors": [
      "Chengyandan Shen",
      "Christoffer Sloth"
    ],
    "published": "2025-09-04T10:02:32Z",
    "updated": "2025-09-04T10:02:32Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04069v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04069v1",
    "comment": null,
    "relevance_score": 2.0
  },
  {
    "id": "2506.14317v3",
    "title": "ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in   Cluttered Scenes",
    "summary": "Dexterous grasping in cluttered scenes presents significant challenges due to\ndiverse object geometries, occlusions, and potential collisions. Existing\nmethods primarily focus on single-object grasping or grasp-pose prediction\nwithout interaction, which are insufficient for complex, cluttered scenes.\nRecent vision-language-action models offer a potential solution but require\nextensive real-world demonstrations, making them costly and difficult to scale.\nTo address these limitations, we revisit the sim-to-real transfer pipeline and\ndevelop key techniques that enable zero-shot deployment in reality while\nmaintaining robust generalization. We propose ClutterDexGrasp, a two-stage\nteacher-student framework for closed-loop target-oriented dexterous grasping in\ncluttered scenes. The framework features a teacher policy trained in simulation\nusing clutter density curriculum learning, incorporating both a geometry and\nspatially-embedded scene representation and a novel comprehensive safety\ncurriculum, enabling general, dynamic, and safe grasping behaviors. Through\nimitation learning, we distill the teacher's knowledge into a student 3D\ndiffusion policy (DP3) that operates on partial point cloud observations. To\nthe best of our knowledge, this represents the first zero-shot sim-to-real\nclosed-loop system for target-oriented dexterous grasping in cluttered scenes,\ndemonstrating robust performance across diverse objects and layouts. More\ndetails and videos are available at https://clutterdexgrasp.github.io/.",
    "authors": [
      "Zeyuan Chen",
      "Qiyang Yan",
      "Yuanpei Chen",
      "Tianhao Wu",
      "Jiyao Zhang",
      "Zihan Ding",
      "Jinzhou Li",
      "Yaodong Yang",
      "Hao Dong"
    ],
    "published": "2025-06-17T08:50:49Z",
    "updated": "2025-09-04T05:12:41Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2506.14317v3",
    "arxiv_url": "http://arxiv.org/abs/2506.14317v3",
    "comment": "Accepted at CoRL 2025",
    "relevance_score": 2.0
  },
  {
    "id": "2509.04088v1",
    "title": "Spiking Neural Network Decoders of Finger Forces from High-Density   Intramuscular Microelectrode Arrays",
    "summary": "Restoring naturalistic finger control in assistive technologies requires the\ncontinuous decoding of motor intent with high accuracy, efficiency, and\nrobustness. Here, we present a spike-based decoding framework that integrates\nspiking neural networks (SNNs) with motor unit activity extracted from\nhigh-density intramuscular microelectrode arrays. We demonstrate simultaneous\nand proportional decoding of individual finger forces from motor unit spike\ntrains during isometric contractions at 15% of maximum voluntary contraction\nusing SNNs. We systematically evaluated alternative SNN decoder configurations\nand compared two possible input modalities: physiologically grounded motor unit\nspike trains and spike-encoded intramuscular EMG signals. Through this\ncomparison, we quantified trade-offs between decoding accuracy, memory\nfootprint, and robustness to input errors. The results showed that shallow SNNs\ncan reliably decode finger-level motor intent with competitive accuracy and\nminimal latency, while operating with reduced memory requirements and without\nthe need for external preprocessing buffers. This work provides a practical\nblueprint for integrating SNNs into finger-level force decoding systems,\ndemonstrating how the choice of input representation can be strategically\ntailored to meet application-specific requirements for accuracy, robustness,\nand memory efficiency.",
    "authors": [
      "Farah Baracat",
      "Agnese Grison",
      "Dario Farina",
      "Giacomo Indiveri",
      "Elisa Donati"
    ],
    "published": "2025-09-04T10:43:43Z",
    "updated": "2025-09-04T10:43:43Z",
    "categories": [
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.04088v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04088v1",
    "comment": null,
    "relevance_score": 2.0
  },
  {
    "id": "2509.04330v1",
    "title": "Temporal Interest-Driven Multimodal Personalized Content Generation",
    "summary": "With the dynamic evolution of user interests and the increasing multimodal\ndemands in internet applications, personalized content generation strategies\nbased on static interest preferences struggle to meet practical application\nrequirements. The proposed TIMGen (Temporal Interest-driven Multimodal\nGeneration) model addresses this challenge by modeling the long-term temporal\nevolution of users' interests and capturing dynamic interest representations\nwith strong temporal dependencies. This model also supports the fusion of\nmultimodal features, such as text, images, video, and audio, and delivers\ncustomized content based on multimodal preferences. TIMGen jointly learns\ntemporal dependencies and modal preferences to obtain a unified interest\nrepresentation, which it then generates to meet users' personalized content\nneeds. TIMGen overcomes the shortcomings of personalized content recommendation\nmethods based on static preferences, enabling flexible and dynamic modeling of\nusers' multimodal interests, better understanding and capturing their interests\nand preferences. It can be extended to a variety of practical application\nscenarios, including e-commerce, advertising, online education, and precision\nmedicine, providing insights for future research.",
    "authors": [
      "Tian Miao"
    ],
    "published": "2025-09-04T15:49:26Z",
    "updated": "2025-09-04T15:49:26Z",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2509.04330v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04330v1",
    "comment": null,
    "relevance_score": 2.0
  },
  {
    "id": "2509.04215v1",
    "title": "PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music",
    "summary": "Solo piano music, despite being a single-instrument medium, possesses\nsignificant expressive capabilities, conveying rich semantic information across\ngenres, moods, and styles. However, current general-purpose music\nrepresentation models, predominantly trained on large-scale datasets, often\nstruggle to captures subtle semantic distinctions within homogeneous solo piano\nmusic. Furthermore, existing piano-specific representation models are typically\nunimodal, failing to capture the inherently multimodal nature of piano music,\nexpressed through audio, symbolic, and textual modalities. To address these\nlimitations, we propose PianoBind, a piano-specific multimodal joint embedding\nmodel. We systematically investigate strategies for multi-source training and\nmodality utilization within a joint embedding framework optimized for capturing\nfine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano\ndatasets. Our experimental results demonstrate that PianoBind learns multimodal\nrepresentations that effectively capture subtle nuances of piano music,\nachieving superior text-to-music retrieval performance on in-domain and\nout-of-domain piano datasets compared to general-purpose music joint embedding\nmodels. Moreover, our design choices offer reusable insights for multimodal\nrepresentation learning with homogeneous datasets beyond piano music.",
    "authors": [
      "Hayeon Bang",
      "Eunjin Choi",
      "Seungheon Doh",
      "Juhan Nam"
    ],
    "published": "2025-09-04T13:43:53Z",
    "updated": "2025-09-04T13:43:53Z",
    "categories": [
      "cs.SD",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2509.04215v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04215v1",
    "comment": "Accepted for publication at the 26th International Society for Music\n  Information Retrieval Conference (ISMIR 2025)",
    "relevance_score": 2.0
  },
  {
    "id": "2509.04449v1",
    "title": "ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset",
    "summary": "We present ChronoGraph, a graph-structured multivariate time series\nforecasting dataset built from real-world production microservices. Each node\nis a service that emits a multivariate stream of system-level performance\nmetrics, capturing CPU, memory, and network usage patterns, while directed\nedges encode dependencies between services. The primary task is forecasting\nfuture values of these signals at the service level. In addition, ChronoGraph\nprovides expert-annotated incident windows as anomaly labels, enabling\nevaluation of anomaly detection methods and assessment of forecast robustness\nduring operational disruptions. Compared to existing benchmarks from industrial\ncontrol systems or traffic and air-quality domains, ChronoGraph uniquely\ncombines (i) multivariate time series, (ii) an explicit, machine-readable\ndependency graph, and (iii) anomaly labels aligned with real incidents. We\nreport baseline results spanning forecasting models, pretrained time-series\nfoundation models, and standard anomaly detectors. ChronoGraph offers a\nrealistic benchmark for studying structure-aware forecasting and incident-aware\nevaluation in microservice systems.",
    "authors": [
      "Adrian Catalin Lutu",
      "Ioana Pintilie",
      "Elena Burceanu",
      "Andrei Manolache"
    ],
    "published": "2025-09-04T17:59:52Z",
    "updated": "2025-09-04T17:59:52Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04449v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04449v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.02175v2",
    "title": "Understanding Space Is Rocket Science -- Only Top Reasoning Models Can   Solve Spatial Understanding Tasks",
    "summary": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed to be very easy for humans and hard for\nthe current generation of VLMs, and this is empirically verified. Our results\nshow a striking lack of spatial relation understanding in open source and\nfrontier commercial VLMs and a surprisingly high performance of reasoning\nmodels. Additionally, we perform a disentanglement analysis to separate the\ncontributions of object localization and spatial reasoning in\nchain-of-thought-based models and find that the performance on the benchmark is\nbottlenecked by spatial reasoning and not object localization capabilities. We\nrelease the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience",
    "authors": [
      "Nils Hoehing",
      "Mayug Maniparambil",
      "Ellen Rushe",
      "Noel E. O'Connor",
      "Anthony Ventresque"
    ],
    "published": "2025-09-02T10:32:58Z",
    "updated": "2025-09-04T16:38:44Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.02175v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02175v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04345v1",
    "title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds",
    "summary": "Speech generation systems can produce remarkably realistic vocalisations that\nare often indistinguishable from human speech, posing significant authenticity\nchallenges. Although numerous deepfake detection methods have been developed,\ntheir effectiveness in real-world environments remains unrealiable due to the\ndomain shift between training and test samples arising from diverse human\nspeech and fast evolving speech synthesis systems. This is not adequately\naddressed by current datasets, which lack real-world application challenges\nwith diverse and up-to-date audios in both real and deep-fake categories. To\nfill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,\nhighly diverse deepfake audio dataset for comprehensive evaluation and robust\ndevelopment of generalised models for deepfake audio detection. It consists of\nover 4,500 hours of synthetic audio generated by 11 recent TTS models and 10\nvocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio\nclips, making it the largest deepfake audio dataset by scale. Through extensive\nexperiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods\ntrained on existing datasets struggle to generalise to novel deepfake audio\nsamples and suffer from high false positive rates on unseen human voice,\nunderscoring the need for a comprehensive dataset; and ii) these methods\ntrained on AUDETER achieve highly generalised detection performance and\nsignificantly reduce detection error rate by 44.1% to 51.6%, achieving an error\nrate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild\ndataset, paving the way for training generalist deepfake audio detectors.\nAUDETER is available on GitHub.",
    "authors": [
      "Qizhou Wang",
      "Hanxun Huang",
      "Guansong Pang",
      "Sarah Erfani",
      "Christopher Leckie"
    ],
    "published": "2025-09-04T16:03:44Z",
    "updated": "2025-09-04T16:03:44Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04345v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04345v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04317v1",
    "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment   Changes",
    "summary": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.",
    "authors": [
      "Isidoro Tamassia",
      "Wendelin Böhmer"
    ],
    "published": "2025-09-04T15:38:37Z",
    "updated": "2025-09-04T15:38:37Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04317v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04317v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04239v1",
    "title": "Evaluating Quality of Gaming Narratives Co-created with AI",
    "summary": "This paper proposes a structured methodology to evaluate AI-generated game\nnarratives, leveraging the Delphi study structure with a panel of narrative\ndesign experts. Our approach synthesizes story quality dimensions from\nliterature and expert insights, mapping them into the Kano model framework to\nunderstand their impact on player satisfaction. The results can inform game\ndevelopers on prioritizing quality aspects when co-creating game narratives\nwith generative AI.",
    "authors": [
      "Arturo Valdivia",
      "Paolo Burelli"
    ],
    "published": "2025-09-04T14:13:42Z",
    "updated": "2025-09-04T14:13:42Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04239v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04239v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2508.00401v2",
    "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent   Cooperation",
    "summary": "Theory of Mind (ToM) -- the ability to understand that others can have\ndiffering knowledge and goals -- enables agents to reason about others' beliefs\nwhile planning their own actions. We present a novel approach to multi-agent\ncooperation by implementing ToM within active inference. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication. In\nour framework, ToM-equipped agents maintain distinct representations of their\nown and others' beliefs and goals. ToM agents then use an extended and adapted\nversion of the sophisticated inference tree-based planning algorithm to\nsystematically explore joint policy spaces through recursive reasoning. We\nevaluate our approach through collision avoidance and foraging simulations.\nResults suggest that ToM agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour and considering them when planning their own actions. Our\napproach shows potential for generalisable and scalable multi-agent systems\nwhile providing computational insights into ToM mechanisms.",
    "authors": [
      "Riddhi J. Pitliya",
      "Ozan Çatal",
      "Toon Van de Maele",
      "Corrado Pezzato",
      "Tim Verbelen"
    ],
    "published": "2025-08-01T08:02:35Z",
    "updated": "2025-09-04T13:51:30Z",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.00401v2",
    "arxiv_url": "http://arxiv.org/abs/2508.00401v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.00215v2",
    "title": "First Order Model-Based RL through Decoupled Backpropagation",
    "summary": "There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.",
    "authors": [
      "Joseph Amigo",
      "Rooholla Khorrambakht",
      "Elliot Chane-Sane",
      "Nicolas Mansard",
      "Ludovic Righetti"
    ],
    "published": "2025-08-29T19:55:25Z",
    "updated": "2025-09-04T12:46:04Z",
    "categories": [
      "cs.AI",
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.00215v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00215v2",
    "comment": "CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04156v1",
    "title": "YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind   Turbine Components",
    "summary": "Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.",
    "authors": [
      "Serhii Svystun",
      "Pavlo Radiuk",
      "Oleksandr Melnychenko",
      "Oleg Savenko",
      "Anatoliy Sachenko"
    ],
    "published": "2025-09-04T12:32:04Z",
    "updated": "2025-09-04T12:32:04Z",
    "categories": [
      "I.2.10; I.4.8; I.5.4; I.2.9",
      "cs.AI",
      "cs.CV",
      "cs.RO",
      "68T07, 68T45, 68U10, 68T40"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04156v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04156v1",
    "comment": "The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland",
    "relevance_score": 1.5
  },
  {
    "id": "2508.16117v2",
    "title": "Extending FKG.in: Towards a Food Claim Traceability Network",
    "summary": "The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we\nhave been incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions.",
    "authors": [
      "Saransh Kumar Gupta",
      "Rizwan Gulzar Mir",
      "Lipika Dey",
      "Partha Pratim Das",
      "Anirban Sen",
      "Ramesh Jain"
    ],
    "published": "2025-08-22T06:18:51Z",
    "updated": "2025-09-04T11:54:35Z",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.16117v2",
    "arxiv_url": "http://arxiv.org/abs/2508.16117v2",
    "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop",
    "relevance_score": 1.5
  },
  {
    "id": "2412.05248v3",
    "title": "Enhancing FKG.in: automating Indian food composition analysis",
    "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
    "authors": [
      "Saransh Kumar Gupta",
      "Lipika Dey",
      "Partha Pratim Das",
      "Geeta Trilok-Kumar",
      "Ramesh Jain"
    ],
    "published": "2024-12-06T18:27:15Z",
    "updated": "2025-09-04T11:42:04Z",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2412.05248v3",
    "arxiv_url": "http://arxiv.org/abs/2412.05248v3",
    "comment": "15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
    "relevance_score": 1.5
  },
  {
    "id": "2506.20790v2",
    "title": "Stochastic Parameter Decomposition",
    "summary": "A key step in reverse engineering neural networks is to decompose them into\nsimpler parts that can be studied in relative isolation. Linear parameter\ndecomposition -- a framework that has been proposed to resolve several issues\nwith current decomposition methods -- decomposes neural network parameters into\na sum of sparsely used vectors in parameter space. However, the current main\nmethod in this framework, Attribution-based Parameter Decomposition (APD), is\nimpractical on account of its computational cost and sensitivity to\nhyperparameters. In this work, we introduce \\textit{Stochastic Parameter\nDecomposition} (SPD), a method that is more scalable and robust to\nhyperparameters than APD, which we demonstrate by decomposing models that are\nslightly larger and more complex than was possible to decompose with APD. We\nalso show that SPD avoids other issues, such as shrinkage of the learned\nparameters, and better identifies ground truth mechanisms in toy models. By\nbridging causal mediation analysis and network decomposition methods, this\ndemonstration opens up new research possibilities in mechanistic\ninterpretability by removing barriers to scaling linear parameter decomposition\nmethods to larger models. We release a library for running SPD and reproducing\nour experiments at https://github.com/goodfire-ai/spd/tree/spd-paper.",
    "authors": [
      "Lucius Bushnaq",
      "Dan Braun",
      "Lee Sharkey"
    ],
    "published": "2025-06-25T19:26:31Z",
    "updated": "2025-09-04T11:41:34Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2506.20790v2",
    "arxiv_url": "http://arxiv.org/abs/2506.20790v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04125v1",
    "title": "Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker",
    "summary": "In the game of poker, being unpredictable, or bluffing, is an essential\nskill. When humans play poker, they bluff. However, most works on\ncomputer-poker focus on performance metrics such as win rates, while bluffing\nis overlooked. In this paper we study whether two popular algorithms, DQN\n(based on reinforcement learning) and CFR (based on game theory), exhibit\nbluffing behavior in Leduc Hold'em, a simplified version of poker. We designed\nan experiment where we let the DQN and CFR agent play against each other while\nwe log their actions. We find that both DQN and CFR exhibit bluffing behavior,\nbut they do so in different ways. Although both attempt to perform bluffs at\ndifferent rates, the percentage of successful bluffs (where the opponent folds)\nis roughly the same. This suggests that bluffing is an essential aspect of the\ngame, not of the algorithm. Future work should look at different bluffing\nstyles and at the full game of poker. Code at\nhttps://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.",
    "authors": [
      "Tarik Zaciragic",
      "Aske Plaat",
      "K. Joost Batenburg"
    ],
    "published": "2025-09-04T11:40:24Z",
    "updated": "2025-09-04T11:40:24Z",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04125v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04125v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04118v1",
    "title": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural   Video Coding",
    "summary": "Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.",
    "authors": [
      "Junqi Liao",
      "Yaojun Wu",
      "Chaoyi Lin",
      "Zhipin Deng",
      "Li Li",
      "Dong Liu",
      "Xiaoyan Sun"
    ],
    "published": "2025-09-04T11:31:12Z",
    "updated": "2025-09-04T11:31:12Z",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04118v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04118v1",
    "comment": "9 pages, 8 figures, Accepted to ACMMM 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2410.18970v4",
    "title": "WASP: A Weight-Space Approach to Detecting Learned Spuriousness",
    "summary": "It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.",
    "authors": [
      "Cristian Daniel Păduraru",
      "Antonio Bărbălau",
      "Radu Filipescu",
      "Andrei Liviu Nicolicioiu",
      "Elena Burceanu"
    ],
    "published": "2024-10-24T17:59:16Z",
    "updated": "2025-09-04T11:06:55Z",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2410.18970v4",
    "arxiv_url": "http://arxiv.org/abs/2410.18970v4",
    "comment": "under review",
    "relevance_score": 1.5
  },
  {
    "id": "2503.14048v2",
    "title": "Beyond holography: the entropic quantum gravity foundations of image   processing",
    "summary": "Recently, thanks to the development of artificial intelligence (AI) there is\nincreasing scientific attention in establishing the connections between\ntheoretical physics and AI. Traditionally, these connections have been focusing\nmostly on the relation between string theory and image processing and involve\nimportant theoretical paradigms such as holography. Recently G. Bianconi has\nformulated the Gravity from Entropy (GfE) approach to quantum gravity in which\ngravity is derived from the geometric quantum relative entropy (GQRE) between\ntwo metrics associated with the Lorentzian spacetime. Here it is demonstrated\nthat the famous Perona-Malik algorithm for image processing is the gradient\nflow of the GfE action in its simple warm-up scenario. Specifically, this\nalgorithm is the outcome of the minimization of the GQRE between two Euclidean\nmetrics: the one of the support of the image and the one induced by the image.\nAs the Perona-Malik algorithm is known to preserve sharp contours, this implies\nthat the GfE action, does not in general lead to uniform images upon iteration\nof the gradient flow dynamics as it would be intuitively expected from entropic\nactions maximising classical entropies. Rather, the outcome of the minimization\nof the GQRE is compatible with the preservation of complex structures. These\nresults provide the geometrical and information theory foundations for the\nPerona-Malik algorithm and might contribute to establish deeper connections\nbetween GfE, machine learning and brain research.",
    "authors": [
      "Ginestra Bianconi"
    ],
    "published": "2025-03-18T09:06:33Z",
    "updated": "2025-09-04T08:38:38Z",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI",
      "gr-qc",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2503.14048v2",
    "arxiv_url": "http://arxiv.org/abs/2503.14048v2",
    "comment": "(7 pages, 1 figure)",
    "relevance_score": 1.5
  },
  {
    "id": "2509.03973v1",
    "title": "SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for   Histopathology Whole Slide Image Classification",
    "summary": "We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.",
    "authors": [
      "Yu Bai",
      "Zitong Yu",
      "Haowen Tian",
      "Xijing Wang",
      "Shuo Yan",
      "Lin Wang",
      "Honglin Li",
      "Xitong Ling",
      "Bo Zhang",
      "Zheng Zhang",
      "Wufan Wang",
      "Hui Gao",
      "Xiangyang Gong",
      "Wendong Wang"
    ],
    "published": "2025-09-04T07:58:52Z",
    "updated": "2025-09-04T07:58:52Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03973v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03973v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04450v1",
    "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual   Try-On from a Single Image -- Technical Preview",
    "summary": "We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.",
    "authors": [
      "Jun-Kun Chen",
      "Aayush Bansal",
      "Minh Phuoc Vo",
      "Yu-Xiong Wang"
    ],
    "published": "2025-09-04T17:59:55Z",
    "updated": "2025-09-04T17:59:55Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04450v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04450v1",
    "comment": "Project Page: https://immortalco.github.io/VirtualFittingRoom/",
    "relevance_score": 1.5
  },
  {
    "id": "2509.02565v2",
    "title": "Understanding sparse autoencoder scaling in the presence of feature   manifolds",
    "summary": "Sparse autoencoders (SAEs) model the activations of a neural network as\nlinear combinations of sparsely occurring directions of variation (latents).\nThe ability of SAEs to reconstruct activations follows scaling laws w.r.t. the\nnumber of latents. In this work, we adapt a capacity-allocation model from the\nneural scaling literature (Brill, 2024) to understand SAE scaling, and in\nparticular, to understand how \"feature manifolds\" (multi-dimensional features)\ninfluence scaling behavior. Consistent with prior work, the model recovers\ndistinct scaling regimes. Notably, in one regime, feature manifolds have the\npathological effect of causing SAEs to learn far fewer features in data than\nthere are latents in the SAE. We provide some preliminary discussion on whether\nor not SAEs are in this pathological regime in the wild.",
    "authors": [
      "Eric J. Michaud",
      "Liv Gorton",
      "Tom McGrath"
    ],
    "published": "2025-09-02T17:59:50Z",
    "updated": "2025-09-04T17:55:36Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.02565v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02565v2",
    "comment": "13 pages, 8 figures, short workshop submission",
    "relevance_score": 1.5
  },
  {
    "id": "2402.04915v3",
    "title": "Moco: A Learnable Meta Optimizer for Combinatorial Optimization",
    "summary": "Relevant combinatorial optimization problems (COPs) are often NP-hard. While\nthey have been tackled mainly via handcrafted heuristics in the past, advances\nin neural networks have motivated the development of general methods to learn\nheuristics from data. Many approaches utilize a neural network to directly\nconstruct a solution, but are limited in further improving based on already\nconstructed solutions at inference time. Our approach, Moco, defines a\nlightweight solution construction procedure, guided by a single continuous\nvector $\\theta$ (called heatmap) and learns a neural network to update $\\theta$\nfor a single instance of a COP at inference time. The update is based on\nvarious features of the current search state. The training procedure is budget\naware, targeting the overall best solution found during the entire search. Moco\nis a fully learnable meta optimizer not utilizing problem specific heuristics\nor requiring optimal solutions for training. We test Moco on the Traveling\nSalesman Problem (TSP) and Maximum Independent Set (MIS) and show that it\nsignificantly improves over other heatmap based methods.",
    "authors": [
      "Tim Dernedde",
      "Daniela Thyssens",
      "Sören Dittrich",
      "Maximilian Stubbemann",
      "Lars Schmidt-Thieme"
    ],
    "published": "2024-02-07T14:41:17Z",
    "updated": "2025-09-04T16:15:03Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2402.04915v3",
    "arxiv_url": "http://arxiv.org/abs/2402.04915v3",
    "comment": "20 pages, 2 figures. A prior version was published in Advances in\n  Knowledge Discovery and Data Mining. PAKDD 2025. Lecture Notes in Computer\n  Science, vol 15872. Springer, Singapore",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04322v1",
    "title": "Characteristic Energy Behavior Profiling of Non-Residential Buildings",
    "summary": "Due to the threat of changing climate and extreme weather events, the\ninfrastructure of the United States Army installations is at risk. More than\never, climate resilience measures are needed to protect facility assets that\nsupport critical missions and help generate readiness. As most of the Army\ninstallations within the continental United States rely on commercial energy\nand water sources, resilience to the vulnerabilities within independent energy\nresources (electricity grids, natural gas pipelines, etc) along with a baseline\nunderstanding of energy usage within installations must be determined. This\npaper will propose a data-driven behavioral model to determine behavior\nprofiles of energy usage on installations. These profiles will be used 1) to\ncreate a baseline assessment of the impact of unexpected disruptions on energy\nsystems and 2) to benchmark future resiliency measures. In this methodology,\nindividual building behavior will be represented with models that can\naccurately analyze, predict, and cluster multimodal data collected from energy\nusage of non-residential buildings. Due to the nature of Army installation\nenergy usage data, similarly structured open access data will be used to\nillustrate this methodology.",
    "authors": [
      "Haley Dozier",
      "Althea Henslee"
    ],
    "published": "2025-09-04T15:41:35Z",
    "updated": "2025-09-04T15:41:35Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04322v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04322v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2507.05972v2",
    "title": "Generalized and Unified Equivalences between Hardness and Pseudoentropy",
    "summary": "Pseudoentropy characterizations provide a quantitatively precise\ndemonstration of the close relationship between computational hardness and\ncomputational randomness. We prove a unified pseudoentropy characterization\nthat generalizes and strengthens previous results for both uniform and\nnon-uniform models of computation. Our characterization holds for a general\nfamily of entropy notions that encompasses the common notions of Shannon\nentropy and min entropy as special cases. Moreover, we show that the\ncharacterizations for different entropy notions can be simultaneously achieved\nby a single, universal function that simultaneously witnesses computational\nhardness and computational randomness. A key technical insight of our work is\nthat the notion of weight-restricted calibration from the recent literature on\nalgorithm fairness, along with standard computational indistinguishability\n(known as multiaccuracy in the fairness literature), suffices for proving\npseudoentropy characterizations for general entropy notions. This demonstrates\nthe power of weight-restricted calibration to enhance the classic\nComplexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009)\nand Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to\nachieve an exponential improvement in the complexity dependency on the alphabet\nsize compared to the pseudoentropy characterizations by Casacuberta, Dwork, and\nVadhan (2024) based on the much stronger notion of multicalibration. We show\nthat the exponential dependency on the alphabet size is inevitable for\nmulticalibration as well as for the weaker notion of calibrated multiaccuracy.",
    "authors": [
      "Lunjia Hu",
      "Salil Vadhan"
    ],
    "published": "2025-07-08T13:27:03Z",
    "updated": "2025-09-04T15:11:20Z",
    "categories": [
      "cs.CC",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.05972v2",
    "arxiv_url": "http://arxiv.org/abs/2507.05972v2",
    "comment": "Accepted to TCC 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2506.05138v2",
    "title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT   Systems",
    "summary": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.",
    "authors": [
      "Pavle Vasiljevic",
      "Milica Matic",
      "Miroslav Popovic"
    ],
    "published": "2025-06-05T15:22:04Z",
    "updated": "2025-09-04T15:08:01Z",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2506.05138v2",
    "arxiv_url": "http://arxiv.org/abs/2506.05138v2",
    "comment": "6 pages, 4 algorithms, 5 figures, 2 tables",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04290v1",
    "title": "An Interactive Framework for Finding the Optimal Trade-off in   Differential Privacy",
    "summary": "Differential privacy (DP) is the standard for privacy-preserving analysis,\nand introduces a fundamental trade-off between privacy guarantees and model\nperformance. Selecting the optimal balance is a critical challenge that can be\nframed as a multi-objective optimization (MOO) problem where one first\ndiscovers the set of optimal trade-offs (the Pareto front) and then learns a\ndecision-maker's preference over them. While a rich body of work on interactive\nMOO exists, the standard approach -- modeling the objective functions with\ngeneric surrogates and learning preferences from simple pairwise feedback -- is\ninefficient for DP because it fails to leverage the problem's unique structure:\na point on the Pareto front can be generated directly by maximizing accuracy\nfor a fixed privacy level. Motivated by this property, we first derive the\nshape of the trade-off theoretically, which allows us to model the Pareto front\ndirectly and efficiently. To address inefficiency in preference learning, we\nreplace pairwise comparisons with a more informative interaction. In\nparticular, we present the user with hypothetical trade-off curves and ask them\nto pick their preferred trade-off. Our experiments on differentially private\nlogistic regression and deep transfer learning across six real-world datasets\nshow that our method converges to the optimal privacy-accuracy trade-off with\nsignificantly less computational cost and user interaction than baselines.",
    "authors": [
      "Yaohong Yang",
      "Aki Rehn",
      "Sammie Katt",
      "Antti Honkela",
      "Samuel Kaski"
    ],
    "published": "2025-09-04T15:02:10Z",
    "updated": "2025-09-04T15:02:10Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04290v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04290v1",
    "comment": "20 pages, 12 figures",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04169v1",
    "title": "Privacy Risks in Time Series Forecasting: User- and Record-Level   Membership Inference",
    "summary": "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models.",
    "authors": [
      "Nicolas Johansson",
      "Tobias Olsson",
      "Daniel Nilsson",
      "Johan Östman",
      "Fazeleh Hoseini"
    ],
    "published": "2025-09-04T12:43:45Z",
    "updated": "2025-09-04T12:43:45Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04169v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04169v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2407.03951v3",
    "title": "Uncertainty-Guided Likelihood Tree Search",
    "summary": "Tree search is a fundamental tool for planning, as many sequential\ndecision-making problems can be framed as searching over tree-structured\nspaces. We propose an uncertainty-guided tree search algorithm for settings\nwhere the reward function is a log-likelihood function of the paths. Due to the\ncombinatorial explosion of the tree size, the set of paths for which one can\nobtain rewards is sparse, particularly when the likelihood is obtained through\nexpensive evaluations, such as by querying a large language model. We address\nthis challenge by deriving an probabilistic search heuristic based on\nregularity assumptions for the likelihood. Unlike existing tree search methods,\nthe proposed method can perform backtracking and trade-off exploration with\nexploitation, and yet does not require expensive roll-outs, or sophisticated\nBayesian inference. Through extensive on-model and off-model experiments on\ntimely, large-scale practical applications, we demonstrate that our method\nidentifies paths with high likelihood while requiring fewer costly evaluations.",
    "authors": [
      "Julia Grosse",
      "Ruotian Wu",
      "Ahmad Rashid",
      "Cheng Zhang",
      "Philipp Hennig",
      "Pascal Poupart",
      "Agustinus Kristiadi"
    ],
    "published": "2024-07-04T14:08:50Z",
    "updated": "2025-09-04T12:43:29Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2407.03951v3",
    "arxiv_url": "http://arxiv.org/abs/2407.03951v3",
    "comment": "10 pages",
    "relevance_score": 1.5
  },
  {
    "id": "2509.03317v2",
    "title": "Bayesian Additive Regression Trees for functional ANOVA model",
    "summary": "Bayesian Additive Regression Trees (BART) is a powerful statistical model\nthat leverages the strengths of Bayesian inference and regression trees. It has\nreceived significant attention for capturing complex non-linear relationships\nand interactions among predictors. However, the accuracy of BART often comes at\nthe cost of interpretability. To address this limitation, we propose ANOVA\nBayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART\nbased on the functional ANOVA decomposition, which is used to decompose the\nvariability of a function into different interactions, each representing the\ncontribution of a different set of covariates or factors. Our proposed\nANOVA-BART enhances interpretability, preserves and extends the theoretical\nguarantees of BART, and achieves superior predictive performance. Specifically,\nwe establish that the posterior concentration rate of ANOVA-BART is nearly\nminimax optimal, and further provides the same convergence rates for each\ninteraction that are not available for BART. Moreover, comprehensive\nexperiments confirm that ANOVA-BART surpasses BART in both accuracy and\nuncertainty quantification, while also demonstrating its effectiveness in\ncomponent selection. These results suggest that ANOVA-BART offers a compelling\nalternative to BART by balancing predictive accuracy, interpretability, and\ntheoretical consistency.",
    "authors": [
      "Seokhun Park",
      "Insung Kong",
      "Yongdai Kim"
    ],
    "published": "2025-09-03T13:50:45Z",
    "updated": "2025-09-04T12:40:40Z",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.03317v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03317v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04133v1",
    "title": "Shuffling Heuristic in Variational Inequalities: Establishing New   Convergence Guarantees",
    "summary": "Variational inequalities have gained significant attention in machine\nlearning and optimization research. While stochastic methods for solving these\nproblems typically assume independent data sampling, we investigate an\nalternative approach -- the shuffling heuristic. This strategy involves\npermuting the dataset before sequential processing, ensuring equal\nconsideration of all data points. Despite its practical utility, theoretical\nguarantees for shuffling in variational inequalities remain unexplored. We\naddress this gap by providing the first theoretical convergence estimates for\nshuffling methods in this context. Our analysis establishes rigorous bounds and\nconvergence rates, extending the theoretical framework for this important class\nof algorithms. We validate our findings through extensive experiments on\ndiverse benchmark variational inequality problems, demonstrating faster\nconvergence of shuffling methods compared to independent sampling approaches.",
    "authors": [
      "Daniil Medyakov",
      "Gleb Molodtsov",
      "Grigoriy Evseev",
      "Egor Petrov",
      "Aleksandr Beznosikov"
    ],
    "published": "2025-09-04T12:00:18Z",
    "updated": "2025-09-04T12:00:18Z",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04133v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04133v1",
    "comment": "25 pages, 5 figures, 2 tables",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04107v1",
    "title": "FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data   Heterogeneity",
    "summary": "Federated Learning (FL) provides decentralised model training, which\neffectively tackles problems such as distributed data and privacy preservation.\nHowever, the generalisation of global models frequently faces challenges from\ndata heterogeneity among clients. This challenge becomes even more pronounced\nwhen datasets are limited in size and class imbalance. To address data\nheterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly\noptimises smaller intra-class variance and larger inter-class variance across\nclients, thereby decreasing the negative impact of model aggregation on the\nglobal model over client representations. Our approach minimises the distance\nbetween similar pairs while maximising the distance between negative pairs,\neffectively disentangling client data in the shared feature space. We evaluate\nour method on the CIFAR-10 and CIFAR-100 datasets under various data\ndistributions and with many clients, demonstrating superior performance\ncompared to existing approaches. Furthermore, we provide a detailed analysis of\nmetric learning-based strategies within both supervised and federated learning\nparadigms, highlighting their efficacy in addressing representational learning\nchallenges in federated settings.",
    "authors": [
      "Ozgu Goksu",
      "Nicolas Pugeault"
    ],
    "published": "2025-09-04T11:11:10Z",
    "updated": "2025-09-04T11:11:10Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04107v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04107v1",
    "comment": "The 3rd IEEE International Conference on Federated Learning\n  Technologies and Applications (FLTA25)",
    "relevance_score": 1.5
  },
  {
    "id": "2504.04421v4",
    "title": "Deliberate Planning of 3D Bin Packing on Packing Configuration Trees",
    "summary": "Online 3D Bin Packing Problem (3D-BPP) has widespread applications in\nindustrial automation. Existing methods usually solve the problem with limited\nresolution of spatial discretization, and/or cannot deal with complex practical\nconstraints well. We propose to enhance the practical applicability of online\n3D-BPP via learning on a novel hierarchical representation, packing\nconfiguration tree (PCT). PCT is a full-fledged description of the state and\naction space of bin packing which can support packing policy learning based on\ndeep reinforcement learning (DRL). The size of the packing action space is\nproportional to the number of leaf nodes, making the DRL model easy to train\nand well-performing even with continuous solution space. We further discover\nthe potential of PCT as tree-based planners in deliberately solving packing\nproblems of industrial significance, including large-scale packing and\ndifferent variations of BPP setting. A recursive packing method is proposed to\ndecompose large-scale packing into smaller sub-trees while a spatial ensemble\nmechanism integrates local solutions into global. For different BPP variations\nwith additional decision variables, such as lookahead, buffering, and offline\npacking, we propose a unified planning framework enabling out-of-the-box\nproblem solving. Extensive evaluations demonstrate that our method outperforms\nexisting online BPP baselines and is versatile in incorporating various\npractical constraints. The planning process excels across large-scale problems\nand diverse problem variations. We develop a real-world packing robot for\nindustrial warehousing, with careful designs accounting for constrained\nplacement and transportation stability. Our packing robot operates reliably and\nefficiently on unprotected pallets at 10 seconds per box. It achieves averagely\n19 boxes per pallet with 57.4% space utilization for relatively large-size\nboxes.",
    "authors": [
      "Hang Zhao",
      "Juzhan Xu",
      "Kexiong Yu",
      "Ruizhen Hu",
      "Chenyang Zhu",
      "Bo Du",
      "Kai Xu"
    ],
    "published": "2025-04-06T09:07:10Z",
    "updated": "2025-09-04T10:00:39Z",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2504.04421v4",
    "arxiv_url": "http://arxiv.org/abs/2504.04421v4",
    "comment": "International Journal of Robotics Research",
    "relevance_score": 1.5
  },
  {
    "id": "2507.12070v3",
    "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions",
    "summary": "This paper presents a novel methodology for determining representational\nstructure, which builds upon the existing Spotlight Resonance method. This new\ntool is used to gain insight into how discrete representations can emerge and\norganise in autoencoder models, through a controlled ablation study in which\nonly the activation function is altered. Using this technique, the validity of\nwhether function-driven symmetries can act as implicit inductive biases on\nrepresentations is determined. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. This confirms the\nhypothesis that the symmetries of network primitives can carry unintended\ninductive biases, which produce task-independent artefactual structures in\nrepresentations. The discrete symmetry of contemporary forms is shown to be a\nstrong predictor for the production of discrete representations emerging from\notherwise continuous distributions -- a quantisation effect. This motivates\nfurther reassessment of functional forms in common usage due to such unintended\nconsequences. Moreover, this supports a general causal model for one mode in\nwhich discrete representations may form, and could constitute a prerequisite\nfor downstream interpretability phenomena, including grandmother neurons,\ndiscrete coding schemes, general linear features and possibly Superposition.\nHence, this tool and proposed mechanism for the influence of functional form on\nrepresentations may provide insights into interpretability research. Finally,\npreliminary results indicate that quantisation of representations appears to\ncorrelate with a measurable increase in reconstruction error, reinforcing\nprevious conjectures that this collapse can be detrimental.",
    "authors": [
      "George Bird"
    ],
    "published": "2025-07-16T09:27:54Z",
    "updated": "2025-09-04T09:48:24Z",
    "categories": [
      "cs.LG",
      "I.5.1; F.1.1; I.2.6"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.12070v3",
    "arxiv_url": "http://arxiv.org/abs/2507.12070v3",
    "comment": "41 pages, 37 figures, edited some introductory phrasing and\n  appendices on hyperoctahedral LeakyReLU",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04053v1",
    "title": "On Aligning Prediction Models with Clinical Experiential Learning: A   Prostate Cancer Case Study",
    "summary": "Over the past decade, the use of machine learning (ML) models in healthcare\napplications has rapidly increased. Despite high performance, modern ML models\ndo not always capture patterns the end user requires. For example, a model may\npredict a non-monotonically decreasing relationship between cancer stage and\nsurvival, keeping all other features fixed. In this paper, we present a\nreproducible framework for investigating this misalignment between model\nbehavior and clinical experiential learning, focusing on the effects of\nunderspecification of modern ML pipelines. In a prostate cancer outcome\nprediction case study, we first identify and address these inconsistencies by\nincorporating clinical knowledge, collected by a survey, via constraints into\nthe ML model, and subsequently analyze the impact on model performance and\nbehavior across degrees of underspecification. The approach shows that aligning\nthe ML model with clinical experiential learning is possible without\ncompromising performance. Motivated by recent literature in generative AI, we\nfurther examine the feasibility of a feedback-driven alignment approach in\nnon-generative AI clinical risk prediction models through a randomized\nexperiment with clinicians. Our findings illustrate that, by eliciting\nclinicians' model preferences using our proposed methodology, the larger the\ndifference in how the constrained and unconstrained models make predictions for\na patient, the more apparent the difference is in clinical interpretation.",
    "authors": [
      "Jacqueline J. Vallon",
      "William Overman",
      "Wanqiao Xu",
      "Neil Panjwani",
      "Xi Ling",
      "Sushmita Vij",
      "Hilary P. Bagshaw",
      "John T. Leppert",
      "Sumit Shah",
      "Geoffrey Sonn",
      "Sandy Srinivas",
      "Erqi Pollom",
      "Mark K. Buyyounouski",
      "Mohsen Bayati"
    ],
    "published": "2025-09-04T09:32:19Z",
    "updated": "2025-09-04T09:32:19Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04053v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04053v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04047v1",
    "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface   Scattering for Perlin Distributed Heterogeneous Media",
    "summary": "Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.",
    "authors": [
      "Ashish Tiwari",
      "Satyam Bhardwaj",
      "Yash Bachwana",
      "Parag Sarvoday Sahu",
      "T. M. Feroz Ali",
      "Bhargava Chintalapati",
      "Shanmuganathan Raman"
    ],
    "published": "2025-09-04T09:28:20Z",
    "updated": "2025-09-04T09:28:20Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04047v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04047v1",
    "comment": "To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:\n  https://yashbachwana.github.io/TensoIS/",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04032v1",
    "title": "What if I ask in \\textit{alia lingua}? Measuring Functional Similarity   Across Languages",
    "summary": "How similar are model outputs across languages? In this work, we study this\nquestion using a recently proposed model similarity metric $\\kappa_p$ applied\nto 20 languages and 47 subjects in GlobalMMLU. Our analysis reveals that a\nmodel's responses become increasingly consistent across languages as its size\nand capability grow. Interestingly, models exhibit greater cross-lingual\nconsistency within themselves than agreement with other models prompted in the\nsame language. These results highlight not only the value of $\\kappa_p$ as a\npractical tool for evaluating multilingual reliability, but also its potential\nto guide the development of more consistent multilingual systems.",
    "authors": [
      "Debangan Mishra",
      "Arihant Rastogi",
      "Agyeya Negi",
      "Shashwat Goel",
      "Ponnurangam Kumaraguru"
    ],
    "published": "2025-09-04T09:08:39Z",
    "updated": "2025-09-04T09:08:39Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04032v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04032v1",
    "comment": "Preprint, 11 Pages",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04402v1",
    "title": "Learning neural representations for X-ray ptychography reconstruction   with unknown probes",
    "summary": "X-ray ptychography provides exceptional nanoscale resolution and is widely\napplied in materials science, biology, and nanotechnology. However, its full\npotential is constrained by the critical challenge of accurately reconstructing\nimages when the illuminating probe is unknown. Conventional iterative methods\nand deep learning approaches are often suboptimal, particularly under the\nlow-signal conditions inherent to low-dose and high-speed experiments. These\nlimitations compromise reconstruction fidelity and restrict the broader\nadoption of the technique. In this work, we introduce the Ptychographic\nImplicit Neural Representation (PtyINR), a self-supervised framework that\nsimultaneously addresses the object and probe recovery problem. By\nparameterizing both as continuous neural representations, PtyINR performs\nend-to-end reconstruction directly from raw diffraction patterns without\nrequiring any pre-characterization of the probe. Extensive evaluations\ndemonstrate that PtyINR achieves superior reconstruction quality on both\nsimulated and experimental data, with remarkable robustness under challenging\nlow-signal conditions. Furthermore, PtyINR offers a generalizable,\nphysics-informed framework for addressing probe-dependent inverse problems,\nmaking it applicable to a wide range of computational microscopy problems.",
    "authors": [
      "Tingyou Li",
      "Zixin Xu",
      "Zirui Gao",
      "Hanfei Yan",
      "Xiaojing Huang",
      "Jizhou Li"
    ],
    "published": "2025-09-04T17:13:19Z",
    "updated": "2025-09-04T17:13:19Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04402v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04402v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2507.06949v2",
    "title": "Ecological Legacies of Pre-Columbian Settlements Evident in Palm   Clusters of Neotropical Mountain Forests",
    "summary": "Ancient populations markedly transformed Neotropical forests, yet the spatial\nextent of their ecological influence remains underexplored at high resolution.\nHere we present a deep learning and remote sensing based approach to estimate\nareas of pre-Columbian forest modification based on modern vegetation. We apply\nthis method to high-resolution satellite imagery from the Sierra Nevada de\nSanta Marta, Colombia, as a demonstration of a scalable approach, to evaluate\npalm tree distributions in relation to archaeological infrastructure. Palms\nwere significantly more abundant near archaeological sites with large\ninfrastructure investment. The extent of the largest palm cluster indicates\nthat ancient human-managed areas linked to major infrastructure sites may be up\nto two orders of magnitude bigger than indicated by current archaeological\nevidence alone. Our findings suggest that pre-Columbian populations influenced\nvegetation, fostering conditions conducive to palm proliferation, leaving a\nlasting ecological footprint. This may have lowered the logistical costs of\nestablishing infrastructure-heavy settlements in less accessible locations.",
    "authors": [
      "Sebastian Fajardo",
      "Sina Mohammadi",
      "Jonas Gregorio de Souza",
      "César Ardila",
      "Alan Tapscott Baltar",
      "Shaddai Heidgen",
      "Maria Isabel Mayorga Hernández",
      "Sylvia Mota de Oliveira",
      "Fernando Montejo",
      "Marco Moderato",
      "Vinicius Peripato",
      "Katy Puche",
      "Carlos Reina",
      "Juan Carlos Vargas",
      "Frank W. Takes",
      "Marco Madella"
    ],
    "published": "2025-07-09T15:31:44Z",
    "updated": "2025-09-04T16:39:57Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.06949v2",
    "arxiv_url": "http://arxiv.org/abs/2507.06949v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04370v1",
    "title": "Stitching the Story: Creating Panoramic Incident Summaries from   Body-Worn Footage",
    "summary": "First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.",
    "authors": [
      "Dor Cohen",
      "Inga Efrosman",
      "Yehudit Aperstein",
      "Alexander Apartsin"
    ],
    "published": "2025-09-04T16:27:53Z",
    "updated": "2025-09-04T16:27:53Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04370v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04370v1",
    "comment": "5 pages, 3 figures",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04344v1",
    "title": "MICACL: Multi-Instance Category-Aware Contrastive Learning for   Long-Tailed Dynamic Facial Expression Recognition",
    "summary": "Dynamic facial expression recognition (DFER) faces significant challenges due\nto long-tailed category distributions and complexity of spatio-temporal feature\nmodeling. While existing deep learning-based methods have improved DFER\nperformance, they often fail to address these issues, resulting in severe model\ninduction bias. To overcome these limitations, we propose a novel\nmulti-instance learning framework called MICACL, which integrates\nspatio-temporal dependency modeling and long-tailed contrastive learning\noptimization. Specifically, we design the Graph-Enhanced Instance Interaction\nModule (GEIIM) to capture intricate spatio-temporal between adjacent instances\nrelationships through adaptive adjacency matrices and multiscale convolutions.\nTo enhance instance-level feature aggregation, we develop the Weighted Instance\nAggregation Network (WIAN), which dynamically assigns weights based on instance\nimportance. Furthermore, we introduce a Multiscale Category-aware Contrastive\nLearning (MCCL) strategy to balance training between major and minor\ncategories. Extensive experiments on in-the-wild datasets (i.e., DFEW and\nFERV39k) demonstrate that MICACL achieves state-of-the-art performance with\nsuperior robustness and generalization.",
    "authors": [
      "Feng-Qi Cui",
      "Zhen Lin",
      "Xinlong Rao",
      "Anyang Tong",
      "Shiyao Li",
      "Fei Wang",
      "Changlin Chen",
      "Bin Liu"
    ],
    "published": "2025-09-04T16:03:14Z",
    "updated": "2025-09-04T16:03:14Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04344v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04344v1",
    "comment": "Accepted by IEEE ISPA2025",
    "relevance_score": 1.5
  },
  {
    "id": "2508.15387v4",
    "title": "DIO: Refining Mutual Information and Causal Chain to Enhance Machine   Abstract Reasoning Ability",
    "summary": "Despite the outstanding performance of current deep learning models across\nvarious domains, their fundamental bottleneck in abstract reasoning remains\nunresolved. To address this challenge, the academic community has introduced\nRaven's Progressive Matrices (RPM) problems as an authoritative benchmark for\nevaluating the abstract reasoning capabilities of deep learning algorithms,\nwith a focus on core intelligence dimensions such as abstract reasoning,\npattern recognition, and complex problem-solving. Therefore, this paper centers\non solving RPM problems, aiming to contribute to enhancing the abstract\nreasoning abilities of machine intelligence. Firstly, this paper adopts a\n``causal chain modeling'' perspective to systematically analyze the complete\ncausal chain in RPM tasks: image $\\rightarrow$ abstract attributes\n$\\rightarrow$ progressive attribute patterns $\\rightarrow$ pattern consistency\n$\\rightarrow$ correct answer. Based on this analysis, the network architecture\nof the baseline model DIO is designed. However, experiments reveal that the\noptimization objective formulated for DIO, namely maximizing the variational\nlower bound of mutual information between the context and the correct option,\nfails to enable the model to genuinely acquire the predefined human reasoning\nlogic. This is attributed to two main reasons: the tightness of the lower bound\nsignificantly impacts the effectiveness of mutual information maximization, and\nmutual information, as a statistical measure, does not capture the causal\nrelationship between subjects and objects. To overcome these limitations, this\npaper progressively proposes three improvement methods:",
    "authors": [
      "Ruizhuo Song",
      "Beiming Yuan"
    ],
    "published": "2025-08-21T09:23:51Z",
    "updated": "2025-09-04T15:15:09Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.15387v4",
    "arxiv_url": "http://arxiv.org/abs/2508.15387v4",
    "comment": "15 pages, 9 figures, 8 tables",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04086v1",
    "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale   Category-Aware Temporal Graph",
    "summary": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.",
    "authors": [
      "Yaru Chen",
      "Faegheh Sardari",
      "Peiliang Zhang",
      "Ruohao Guo",
      "Yang Xiang",
      "Zhenbo Li",
      "Wenwu Wang"
    ],
    "published": "2025-09-04T10:32:40Z",
    "updated": "2025-09-04T10:32:40Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04086v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04086v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04050v1",
    "title": "A Re-ranking Method using K-nearest Weighted Fusion for Person   Re-identification",
    "summary": "In person re-identification, re-ranking is a crucial step to enhance the\noverall accuracy by refining the initial ranking of retrieved results. Previous\nstudies have mainly focused on features from single-view images, which can\ncause view bias and issues like pose variation, viewpoint changes, and\nocclusions. Using multi-view features to present a person can help reduce view\nbias. In this work, we present an efficient re-ranking method that generates\nmulti-view features by aggregating neighbors' features using K-nearest Weighted\nFusion (KWF) method. Specifically, we hypothesize that features extracted from\nre-identification models are highly similar when representing the same\nidentity. Thus, we select K neighboring features in an unsupervised manner to\ngenerate multi-view features. Additionally, this study explores the weight\nselection strategies during feature aggregation, allowing us to identify an\neffective strategy. Our re-ranking approach does not require model fine-tuning\nor extra annotations, making it applicable to large-scale datasets. We evaluate\nour method on the person re-identification datasets Market1501, MSMT17, and\nOccluded-DukeMTMC. The results show that our method significantly improves\nRank@1 and mAP when re-ranking the top M candidates from the initial ranking\nresults. Specifically, compared to the initial results, our re-ranking method\nachieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:\nMSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach\ndemonstrates substantial enhancements in computational efficiency compared to\nother re-ranking methods.",
    "authors": [
      "Quang-Huy Che",
      "Le-Chuong Nguyen",
      "Gia-Nghia Tran",
      "Dinh-Duy Phan",
      "Vinh-Tiep Nguyen"
    ],
    "published": "2025-09-04T09:29:25Z",
    "updated": "2025-09-04T09:29:25Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04050v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04050v1",
    "comment": "Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313",
    "relevance_score": 1.5
  },
  {
    "id": "2509.00451v2",
    "title": "Encoder-Only Image Registration",
    "summary": "Learning-based techniques have significantly improved the accuracy and speed\nof deformable image registration. However, challenges such as reducing\ncomputational complexity and handling large deformations persist. To address\nthese challenges, we analyze how convolutional neural networks (ConvNets)\ninfluence registration performance using the Horn-Schunck optical flow\nequation. Supported by prior studies and our empirical experiments, we observe\nthat ConvNets play two key roles in registration: linearizing local intensities\nand harmonizing global contrast variations. Based on these insights, we propose\nthe Encoder-Only Image Registration (EOIR) framework, designed to achieve a\nbetter accuracy-efficiency trade-off. EOIR separates feature learning from flow\nestimation, employing only a 3-layer ConvNet for feature extraction and a set\nof 3-layer flow estimators to construct a Laplacian feature pyramid,\nprogressively composing diffeomorphic deformations under a large-deformation\nmodel. Results on five datasets across different modalities and anatomical\nregions demonstrate EOIR's effectiveness, achieving superior\naccuracy-efficiency and accuracy-smoothness trade-offs. With comparable\naccuracy, EOIR provides better efficiency and smoothness, and vice versa. The\nsource code of EOIR is publicly available on\nhttps://github.com/XiangChen1994/EOIR.",
    "authors": [
      "Xiang Chen",
      "Renjiu Hu",
      "Jinwei Zhang",
      "Yuxi Zhang",
      "Xinyao Yue",
      "Min Liu",
      "Yaonan Wang",
      "Hang Zhang"
    ],
    "published": "2025-08-30T10:45:39Z",
    "updated": "2025-09-04T08:19:24Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.00451v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00451v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2505.23525v2",
    "title": "Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference   Optimization and Temporal Motion Modulation",
    "summary": "Generating highly dynamic and photorealistic portrait animations driven by\naudio and skeletal motion remains challenging due to the need for precise lip\nsynchronization, natural facial expressions, and high-fidelity body motion\ndynamics. We propose a human-preference-aligned diffusion framework that\naddresses these challenges through two key innovations. First, we introduce\ndirect preference optimization tailored for human-centric animation, leveraging\na curated dataset of human preferences to align generated outputs with\nperceptual metrics for portrait motion-video alignment and naturalness of\nexpression. Second, the proposed temporal motion modulation resolves\nspatiotemporal resolution mismatches by reshaping motion conditions into\ndimensionally aligned latent features through temporal channel redistribution\nand proportional feature expansion, preserving the fidelity of high-frequency\nmotion details in diffusion-based synthesis. The proposed mechanism is\ncomplementary to existing UNet and DiT-based portrait diffusion approaches, and\nexperiments demonstrate obvious improvements in lip-audio synchronization,\nexpression vividness, body motion coherence over baseline methods, alongside\nnotable gains in human preference metrics. Our model and source code can be\nfound at: https://github.com/xyz123xyz456/hallo4.",
    "authors": [
      "Jiahao Cui",
      "Yan Chen",
      "Mingwang Xu",
      "Hanlin Shang",
      "Yuxuan Chen",
      "Yun Zhan",
      "Zilong Dong",
      "Yao Yao",
      "Jingdong Wang",
      "Siyu Zhu"
    ],
    "published": "2025-05-29T15:04:00Z",
    "updated": "2025-09-04T07:13:25Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2505.23525v2",
    "arxiv_url": "http://arxiv.org/abs/2505.23525v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2412.12722v2",
    "title": "Defending LVLMs Against Vision Attacks through Partial-Perception   Supervision",
    "summary": "Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.",
    "authors": [
      "Qi Zhou",
      "Tianlin Li",
      "Qing Guo",
      "Dongxia Wang",
      "Yun Lin",
      "Yang Liu",
      "Jin Song Dong"
    ],
    "published": "2024-12-17T09:38:58Z",
    "updated": "2025-09-04T06:43:22Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2412.12722v2",
    "arxiv_url": "http://arxiv.org/abs/2412.12722v2",
    "comment": "Accepted to ICML 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2509.03922v1",
    "title": "LMVC: An End-to-End Learned Multiview Video Coding Framework",
    "summary": "Multiview video is a key data source for volumetric video, enabling immersive\n3D scene reconstruction but posing significant challenges in storage and\ntransmission due to its massive data volume. Recently, deep learning-based\nend-to-end video coding has achieved great success, yet most focus on\nsingle-view or stereo videos, leaving general multiview scenarios\nunderexplored. This paper proposes an end-to-end learned multiview video coding\n(LMVC) framework that ensures random access and backward compatibility while\nenhancing compression efficiency. Our key innovation lies in effectively\nleveraging independent-view motion and content information to enhance\ndependent-view compression. Specifically, to exploit the inter-view motion\ncorrelation, we propose a feature-based inter-view motion vector prediction\nmethod that conditions dependent-view motion encoding on decoded\nindependent-view motion features, along with an inter-view motion entropy model\nthat learns inter-view motion priors. To exploit the inter-view content\ncorrelation, we propose a disparity-free inter-view context prediction module\nthat predicts inter-view contexts from decoded independent-view content\nfeatures, combined with an inter-view contextual entropy model that captures\ninter-view context priors. Experimental results show that our proposed LMVC\nframework outperforms the reference software of the traditional MV-HEVC\nstandard by a large margin, establishing a strong baseline for future research\nin this field.",
    "authors": [
      "Xihua Sheng",
      "Yingwen Zhang",
      "Long Xu",
      "Shiqi Wang"
    ],
    "published": "2025-09-04T06:15:41Z",
    "updated": "2025-09-04T06:15:41Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03922v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03922v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.03891v1",
    "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation",
    "summary": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv",
    "authors": [
      "Gowen Loo",
      "Chang Liu",
      "Qinghong Yin",
      "Xiang Chen",
      "Jiawei Chen",
      "Jingyuan Zhang",
      "Yu Tian"
    ],
    "published": "2025-09-04T05:22:42Z",
    "updated": "2025-09-04T05:22:42Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03891v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03891v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04393v1",
    "title": "Contextualized Token Discrimination for Speech Search Query Correction",
    "summary": "Query spelling correction is an important function of modern search engines\nsince it effectively helps users express their intentions clearly. With the\ngrowing popularity of speech search driven by Automated Speech Recognition\n(ASR) systems, this paper introduces a novel method named Contextualized Token\nDiscrimination (CTD) to conduct effective speech query correction. In CTD, we\nfirst employ BERT to generate token-level contextualized representations and\nthen construct a composition layer to enhance semantic information. Finally, we\nproduce the correct query according to the aggregated token representation,\ncorrecting the incorrect tokens by comparing the original token representations\nand the contextualized representations. Extensive experiments demonstrate the\nsuperior performance of our proposed method across all metrics, and we further\npresent a new benchmark dataset with erroneous ASR transcriptions to offer\ncomprehensive evaluations for audio query correction.",
    "authors": [
      "Junyu Lu",
      "Di Jiang",
      "Mengze Hong",
      "Victor Junqiu Wei",
      "Qintian Guo",
      "Zhiyang Su"
    ],
    "published": "2025-09-04T17:04:44Z",
    "updated": "2025-09-04T17:04:44Z",
    "categories": [
      "cs.SD",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04393v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04393v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2411.01747v3",
    "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
    "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.",
    "authors": [
      "Dang Nguyen",
      "Viet Dac Lai",
      "Seunghyun Yoon",
      "Ryan A. Rossi",
      "Handong Zhao",
      "Ruiyi Zhang",
      "Puneet Mathur",
      "Nedim Lipka",
      "Yu Wang",
      "Trung Bui",
      "Franck Dernoncourt",
      "Tianyi Zhou"
    ],
    "published": "2024-11-04T02:08:59Z",
    "updated": "2025-09-04T16:22:32Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2411.01747v3",
    "arxiv_url": "http://arxiv.org/abs/2411.01747v3",
    "comment": "Published as a conference paper at COLM 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2508.20324v2",
    "title": "Can Compact Language Models Search Like Agents? Distillation-Guided   Policy Optimization for Preserving Agentic RAG Capabilities",
    "summary": "Reinforcement Learning has emerged as a post-training approach to elicit\nagentic RAG behaviors such as search and planning from language models.\nHowever, compact language models (e.g., 0.5B parameters) struggle due to poor\nreasoning ability, resulting in sparse rewards and unstable training. To\novercome these difficulties, we propose Distillation-Guided Policy Optimization\n(DGPO), which addresses the challenges through cold-start initialization from\nteacher demonstrations and continuous teacher guidance during policy\noptimization. To systematically evaluate our approach, we introduce Agentic RAG\nCapabilities (ARC), a fine-grained metric analyzing reasoning, search\ncoordination, and response synthesis. Comprehensive experiments demonstrate\nthat DGPO enables compact models to achieve sophisticated agentic search\nbehaviors, even outperforming the larger teacher model in some cases. DGPO\nmakes agentic RAG feasible in computing resource-constrained environments.",
    "authors": [
      "Rikuto Kotoge",
      "Mai Nishimura",
      "Jiaxin Ma"
    ],
    "published": "2025-08-27T23:57:29Z",
    "updated": "2025-09-04T15:36:07Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2508.20324v2",
    "arxiv_url": "http://arxiv.org/abs/2508.20324v2",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04202v1",
    "title": "Explicit and Implicit Data Augmentation for Social Event Detection",
    "summary": "Social event detection involves identifying and categorizing important events\nfrom social media, which relies on labeled data, but annotation is costly and\nlabor-intensive. To address this problem, we propose Augmentation framework for\nSocial Event Detection (SED-Aug), a plug-and-play dual augmentation framework,\nwhich combines explicit text-based and implicit feature-space augmentation to\nenhance data diversity and model robustness. The explicit augmentation utilizes\nlarge language models to enhance textual information through five diverse\ngeneration strategies. For implicit augmentation, we design five novel\nperturbation techniques that operate in the feature space on structural fused\nembeddings. These perturbations are crafted to keep the semantic and relational\nproperties of the embeddings and make them more diverse. Specifically, SED-Aug\noutperforms the best baseline model by approximately 17.67% on the Twitter2012\ndataset and by about 15.57% on the Twitter2018 dataset in terms of the average\nF1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.",
    "authors": [
      "Congbo Ma",
      "Yuxia Wang",
      "Jia Wu",
      "Jian Yang",
      "Jing Du",
      "Zitai Qiu",
      "Qing Li",
      "Hu Wang",
      "Preslav Nakov"
    ],
    "published": "2025-09-04T13:26:24Z",
    "updated": "2025-09-04T13:26:24Z",
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04202v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04202v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.04182v1",
    "title": "Joint Modeling of Entities and Discourse Relations for Coherence   Assessment",
    "summary": "In linguistics, coherence can be achieved by different means, such as by\nmaintaining reference to the same set of entities across sentences and by\nestablishing discourse relations between them. However, most existing work on\ncoherence modeling focuses exclusively on either entity features or discourse\nrelation features, with little attention given to combining the two. In this\nstudy, we explore two methods for jointly modeling entities and discourse\nrelations for coherence assessment. Experiments on three benchmark datasets\nshow that integrating both types of features significantly enhances the\nperformance of coherence models, highlighting the benefits of modeling both\nsimultaneously for coherence evaluation.",
    "authors": [
      "Wei Liu",
      "Michael Strube"
    ],
    "published": "2025-09-04T12:58:37Z",
    "updated": "2025-09-04T12:58:37Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04182v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04182v1",
    "comment": "EMNLP 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2508.15478v2",
    "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on   Environmental Impacts--Extended Version",
    "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, yet a systematic evaluation of their performance and\nenvironmental impact remains lacking. We introduce SLM-Bench, the first\nbenchmark specifically designed to assess SLMs across multiple dimensions,\nincluding accuracy, computational efficiency, and sustainability metrics.\nSLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14\ndomains. The evaluation is conducted on 4 hardware configurations, providing a\nrigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench\nquantifies 11 metrics across correctness, computation, and consumption,\nenabling a holistic assessment of efficiency trade-offs. Our evaluation\nconsiders controlled hardware conditions, ensuring fair comparisons across\nmodels. We develop an open-source benchmarking pipeline with standardized\nevaluation protocols to facilitate reproducibility and further research. Our\nfindings highlight the diverse trade-offs among SLMs, where some models excel\nin accuracy while others achieve superior energy efficiency. SLM-Bench sets a\nnew standard for SLM evaluation, bridging the gap between resource efficiency\nand real-world applicability.",
    "authors": [
      "Nghiem Thanh Pham",
      "Tung Kieu",
      "Duc-Manh Nguyen",
      "Son Ha Xuan",
      "Nghia Duong-Trung",
      "Danh Le-Phuoc"
    ],
    "published": "2025-08-21T11:56:05Z",
    "updated": "2025-09-04T11:23:22Z",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2508.15478v2",
    "arxiv_url": "http://arxiv.org/abs/2508.15478v2",
    "comment": "24 pages. An extended version of \"SLM-Bench: A Comprehensive\n  Benchmark of Small Language Models on Environmental Impacts\" accepted at\n  EMNLP 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2509.04111v1",
    "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
    "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.",
    "authors": [
      "Dan Saattrup Smart"
    ],
    "published": "2025-09-04T11:20:53Z",
    "updated": "2025-09-04T11:20:53Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04111v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04111v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2306.03774v4",
    "title": "Exploring Linguistic Features for Turkish Text Readability",
    "summary": "This paper presents the first comprehensive study on automatic readability\nassessment of Turkish texts. We combine state-of-the-art neural network models\nwith linguistic features at lexical, morphological, syntactic and discourse\nlevels to develop an advanced readability tool. We evaluate the effectiveness\nof traditional readability formulas compared to modern automated methods and\nidentify key linguistic features that determine the readability of Turkish\ntexts.",
    "authors": [
      "Ahmet Yavuz Uluslu",
      "Gerold Schneider"
    ],
    "published": "2023-06-06T15:32:22Z",
    "updated": "2025-09-04T11:06:14Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2306.03774v4",
    "arxiv_url": "http://arxiv.org/abs/2306.03774v4",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2509.03871v1",
    "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large   Language Models",
    "summary": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",
    "authors": [
      "Yanbo Wang",
      "Yongcan Yu",
      "Jian Liang",
      "Ran He"
    ],
    "published": "2025-09-04T04:12:31Z",
    "updated": "2025-09-04T04:12:31Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03871v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03871v1",
    "comment": "38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress",
    "relevance_score": 1.5
  },
  {
    "id": "2509.03805v1",
    "title": "Measuring How (Not Just Whether) VLMs Build Common Ground",
    "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.",
    "authors": [
      "Saki Imai",
      "Mert İnan",
      "Anthony Sicilia",
      "Malihe Alikhani"
    ],
    "published": "2025-09-04T01:43:49Z",
    "updated": "2025-09-04T01:43:49Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03805v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03805v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2502.11948v3",
    "title": "HalluEntity: Benchmarking and Understanding Entity-Level Hallucination   Detection",
    "summary": "To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research.\n  HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity",
    "authors": [
      "Min-Hsuan Yeh",
      "Max Kamachee",
      "Seongheon Park",
      "Yixuan Li"
    ],
    "published": "2025-02-17T16:01:41Z",
    "updated": "2025-09-04T01:22:12Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2502.11948v3",
    "arxiv_url": "http://arxiv.org/abs/2502.11948v3",
    "comment": "TMLR 2025",
    "relevance_score": 1.5
  },
  {
    "id": "2509.03791v1",
    "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation   Evaluation",
    "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.",
    "authors": [
      "Saki Imai",
      "Mert İnan",
      "Anthony Sicilia",
      "Malihe Alikhani"
    ],
    "published": "2025-09-04T00:58:43Z",
    "updated": "2025-09-04T00:58:43Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.03791v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03791v1",
    "comment": null,
    "relevance_score": 1.5
  },
  {
    "id": "2507.11236v2",
    "title": "Improved sampling algorithms and Poincaré inequalities for   non-log-concave distributions",
    "summary": "We study the problem of sampling from a distribution $\\mu$ with density\n$\\propto e^{-V}$ for some potential function $V:\\mathbb R^d\\to \\mathbb R$ with\nquery access to $V$ and $\\nabla V$. We start with the following standard\nassumptions:\n  (1) The potential function $V$ is $L$-smooth.\n  (2) The second moment $\\mathbf{E}_{X\\sim \\mu}[\\|X\\|^2]\\leq M$.\n  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling\nfrom such distributions is at least\n$\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ where $\\epsilon$ is the desired\naccuracy in total variation distance, and the Poincar\\'e constant can be\narbitrarily large.\n  Meanwhile, another common assumption in the study of diffusion based samplers\n(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))\nstrengthens the smoothness condition (1) to the following:\n  (1*) The potential function of *every* distribution along the\nOrnstein-Uhlenbeck process starting from $\\mu$ is $L$-smooth.\n  We show that under the assumptions (1*) and (2), the query complexity of\nsampling from $\\mu$ can be $\\mathrm{poly}(L,d)\\cdot\n\\left(\\frac{Ld+M}{\\epsilon^2}\\right)^{\\mathcal{O}(L+1)}$, which is polynomial\nin $d$ and $\\frac{1}{\\epsilon}$ when $L=\\mathcal{O}(1)$ and\n$M=\\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query\ncomplexity developed by Huang et al. (COLT'24). Our results imply that the\nseemly moderate strengthening of the smoothness condition (1) to (1*) can lead\nto an exponential gap in the query complexity of sampling algorithms.\n  Moreover, we show that together with the assumption (1*) and the stronger\nmoment assumption that $\\|X\\|$ is $\\lambda$-sub-Gaussian for $X\\sim\\mu$, the\nPoincar\\'e constant of $\\mu$ is at most $\\mathcal{O}(\\lambda)^{2(L+1)}$. As an\napplication of our technique, we obtain improved estimate of the Poincar\\'e\nconstant for mixture of Gaussians with the same covariance.",
    "authors": [
      "Yuchen He",
      "Zhehan Lei",
      "Jianan Shao",
      "Chihao Zhang"
    ],
    "published": "2025-07-15T12:06:11Z",
    "updated": "2025-09-04T08:53:47Z",
    "categories": [
      "cs.DS",
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2507.11236v2",
    "arxiv_url": "http://arxiv.org/abs/2507.11236v2",
    "comment": null,
    "relevance_score": 1.0
  },
  {
    "id": "2509.04008v1",
    "title": "Towards understanding Accelerated Stein Variational Gradient Flow --   Analysis of Generalized Bilinear Kernels for Gaussian target distributions",
    "summary": "Stein variational gradient descent (SVGD) is a kernel-based and\nnon-parametric particle method for sampling from a target distribution, such as\nin Bayesian inference and other machine learning tasks. Different from other\nparticle methods, SVGD does not require estimating the score, which is the\ngradient of the log-density. However, in practice, SVGD can be slow compared to\nscore-estimation-based sampling algorithms. To design a fast and efficient\nhigh-dimensional sampling algorithm with the advantages of SVGD, we introduce\naccelerated SVGD (ASVGD), based on an accelerated gradient flow in a metric\nspace of probability densities following Nesterov's method. We then derive a\nmomentum-based discrete-time sampling algorithm, which evolves a set of\nparticles deterministically. To stabilize the particles' position update, we\nalso include a Wasserstein metric regularization. This paper extends the\nconference version \\cite{SL2025}. For the bilinear kernel and Gaussian target\ndistributions, we study the kernel parameter and damping parameters with an\noptimal convergence rate of the proposed dynamics. This is achieved by\nanalyzing the linearized accelerated gradient flows at the equilibrium.\nInterestingly, the optimal parameter is a constant, which does not depend on\nthe covariance of the target distribution. For the generalized kernel\nfunctions, such as the Gaussian kernel, numerical examples with varied target\ndistributions demonstrate the effectiveness of ASVGD compared to SVGD and other\npopular sampling methods. Furthermore, we show that in the setting of Bayesian\nneural networks, ASVGD outperforms SVGD significantly in terms of\nlog-likelihood and total iteration times.",
    "authors": [
      "Viktor Stein",
      "Wuchen Li"
    ],
    "published": "2025-09-04T08:39:47Z",
    "updated": "2025-09-04T08:39:47Z",
    "categories": [
      "math.OC",
      "stat.ML",
      "46N10 (Primary) 46E22 94A15 37Lxx 37A50 (Secondary)"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.04008v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04008v1",
    "comment": "46 pages, 4 figures, 4 algorithms, 4 tables, comments welcome!",
    "relevance_score": 1.0
  },
  {
    "id": "2509.03853v1",
    "title": "Simulation-based Inference via Langevin Dynamics with Score Matching",
    "summary": "Simulation-based inference (SBI) enables Bayesian analysis when the\nlikelihood is intractable but model simulations are available. Recent advances\nin statistics and machine learning, including Approximate Bayesian Computation\nand deep generative models, have expanded the applicability of SBI, yet these\nmethods often face challenges in moderate to high-dimensional parameter spaces.\nMotivated by the success of gradient-based Monte Carlo methods in Bayesian\nsampling, we propose a novel SBI method that integrates score matching with\nLangevin dynamics to explore complex posterior landscapes more efficiently in\nsuch settings. Our approach introduces tailored score-matching procedures for\nSBI, including a localization scheme that reduces simulation costs and an\narchitectural regularization that embeds the statistical structure of\nlog-likelihood scores to improve score-matching accuracy. We provide\ntheoretical analysis of the method and illustrate its practical benefits on\nbenchmark tasks and on more challenging problems in moderate to high\ndimensions, where it performs favorably compared to existing approaches.",
    "authors": [
      "Haoyu Jiang",
      "Yuexi Wang",
      "Yun Yang"
    ],
    "published": "2025-09-04T03:28:50Z",
    "updated": "2025-09-04T03:28:50Z",
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.03853v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03853v1",
    "comment": null,
    "relevance_score": 1.0
  },
  {
    "id": "2210.07456v3",
    "title": "Estimation of High-Dimensional Markov-Switching VAR Models with an   Approximate EM Algorithm",
    "summary": "Regime shifts in high-dimensional time series arise naturally in many\napplications, from neuroimaging to finance. This problem has received\nconsiderable attention in low-dimensional settings, with both Bayesian and\nfrequentist methods used extensively for parameter estimation. The EM algorithm\nis a particularly popular strategy for parameter estimation in low-dimensional\nsettings, although the statistical properties of the resulting estimates have\nnot been well understood. Furthermore, its extension to high-dimensional time\nseries has proved challenging. To overcome these challenges, in this paper we\npropose an approximate EM algorithm for Markov-switching VAR models that leads\nto efficient computation and also facilitates the investigation of asymptotic\nproperties of the resulting parameter estimates. We establish the consistency\nof the proposed EM algorithm in high dimensions and investigate its performance\nvia simulation studies. We also demonstrate the algorithm by analyzing a brain\nelectroencephalography (EEG) dataset recorded on a patient experiencing\nepileptic seizure.",
    "authors": [
      "Xiudi Li",
      "Abolfazl Safikhani",
      "Ali Shojaie"
    ],
    "published": "2022-10-14T01:55:02Z",
    "updated": "2025-09-04T00:21:04Z",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2210.07456v3",
    "arxiv_url": "http://arxiv.org/abs/2210.07456v3",
    "comment": null,
    "relevance_score": 1.0
  },
  {
    "id": "2508.21043v2",
    "title": "HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and   Learning",
    "summary": "Humanoid robots have recently achieved impressive progress in locomotion and\nwhole-body control, yet they remain constrained in tasks that demand rapid\ninteraction with dynamic environments through manipulation. Table tennis\nexemplifies such a challenge: with ball speeds exceeding 5 m/s, players must\nperceive, predict, and act within sub-second reaction times, requiring both\nagility and precision. To address this, we present a hierarchical framework for\nhumanoid table tennis that integrates a model-based planner for ball trajectory\nprediction and racket target planning with a reinforcement learning-based\nwhole-body controller. The planner determines striking position, velocity and\ntiming, while the controller generates coordinated arm and leg motions that\nmimic human strikes and maintain stability and agility across consecutive\nrallies. Moreover, to encourage natural movements, human motion references are\nincorporated during training. We validate our system on a general-purpose\nhumanoid robot, achieving up to 106 consecutive shots with a human opponent and\nsustained exchanges against another humanoid. These results demonstrate\nreal-world humanoid table tennis with sub-second reactive control, marking a\nstep toward agile and interactive humanoid behaviors.",
    "authors": [
      "Zhi Su",
      "Bike Zhang",
      "Nima Rahmanian",
      "Yuman Gao",
      "Qiayuan Liao",
      "Caitlin Regan",
      "Koushil Sreenath",
      "S. Shankar Sastry"
    ],
    "published": "2025-08-28T17:49:12Z",
    "updated": "2025-09-04T13:32:15Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2508.21043v2",
    "arxiv_url": "http://arxiv.org/abs/2508.21043v2",
    "comment": "add more references",
    "relevance_score": 1.0
  },
  {
    "id": "2509.02478v2",
    "title": "Classification of Vision-Based Tactile Sensors: A Review",
    "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.",
    "authors": [
      "Haoran Li",
      "Yijiong Lin",
      "Chenghua Lu",
      "Max Yang",
      "Efi Psomopoulou",
      "Nathan F Lepora"
    ],
    "published": "2025-09-02T16:29:06Z",
    "updated": "2025-09-04T12:09:21Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.02478v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02478v2",
    "comment": "15 pages",
    "relevance_score": 1.0
  },
  {
    "id": "2509.04018v1",
    "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for   Failure Prediction and Correction",
    "summary": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.",
    "authors": [
      "Yifan Yang",
      "Zhixiang Duan",
      "Tianshi Xie",
      "Fuyu Cao",
      "Pinxi Shen",
      "Peili Song",
      "Piaopiao Jin",
      "Guokang Sun",
      "Shaoqing Xu",
      "Yangwei You",
      "Jingtai Liu"
    ],
    "published": "2025-09-04T08:47:26Z",
    "updated": "2025-09-04T08:47:26Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04018v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04018v1",
    "comment": null,
    "relevance_score": 1.0
  },
  {
    "id": "2509.04016v1",
    "title": "Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall   Climbing Robot",
    "summary": "This paper presents the design of a pose estimator for a four wheel\nindependent steer four wheel independent drive (4WIS4WID) wall climbing mobile\nrobot, based on the fusion of multimodal measurements, including wheel\nodometry, visual odometry, and an inertial measurement unit (IMU) data using\nExtended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose\nestimator is a critical component of wall climbing mobile robots, as their\noperational environment involves carrying precise measurement equipment and\nmaintenance tools in construction, requiring information about pose on the\nbuilding at the time of measurement. Due to the complex geometry and material\nproperties of building facades, the use of traditional localization sensors\nsuch as laser, ultrasonic, or radar is often infeasible for wall-climbing\nrobots. Moreover, GPS-based localization is generally unreliable in these\nenvironments because of signal degradation caused by reinforced concrete and\nelectromagnetic interference. Consequently, robot odometry remains the primary\nsource of velocity and position information, despite being susceptible to drift\ncaused by both systematic and non-systematic errors. The calibrations of the\nrobot's systematic parameters were conducted using nonlinear optimization and\nLevenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting\nmethods, while Genetic algorithm and Particle swarm were used as\nstochastic-based methods for kinematic parameter calibration. Performance and\nresults of the calibration methods and pose estimators were validated in detail\nwith experiments on the experimental mobile wall climbing robot.",
    "authors": [
      "Branimir Ćaran",
      "Vladimir Milić",
      "Marko Švaco",
      "Bojan Jerbić"
    ],
    "published": "2025-09-04T08:44:36Z",
    "updated": "2025-09-04T08:44:36Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04016v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04016v1",
    "comment": "ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT\n  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025",
    "relevance_score": 1.0
  },
  {
    "id": "2501.19241v5",
    "title": "Emancipatory Information Retrieval",
    "summary": "Our world today is facing a confluence of several mutually reinforcing crises\neach of which intersects with concerns of social justice and emancipation. This\npaper is a provocation for the role of computer-mediated information access in\nour emancipatory struggles. We define emancipatory information retrieval as the\nstudy and development of information access methods that challenge various\nforms of human oppression, and situates its activities within broader\ncollective emancipatory praxis. The term \"emancipatory\" here signifies the\nmoral concerns of universal humanization of all peoples and the elimination of\noppression to create the conditions under which we can collectively flourish.\nTo develop an emancipatory research agenda for information retrieval (IR), in\nthis paper we speculate about the practices that the community can adopt,\nenumerate some of the projects that the field should undertake, and discuss\nprovocations to spark new ideas and directions for research. We challenge the\nfield of IR research to embrace humanistic values and commit to universal\nemancipation and social justice. We also invite scholars from fields such as\nhuman-computer interaction, information sciences, media studies, design,\nscience and technology studies, social and political sciences, philosophy, law,\nenvironmental sciences, public health, educational sciences, as well as legal\nand policy experts, civil rights advocates, social justice activists and\nmovement organizers, and artists to join us in realizing this transformation.\nIn this process, we must both imagine post-oppressive worlds, and reimagine the\nrole of IR in that world and in the journey that leads us there.",
    "authors": [
      "Bhaskar Mitra"
    ],
    "published": "2025-01-31T15:55:14Z",
    "updated": "2025-09-04T14:50:03Z",
    "categories": [
      "cs.IR",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2501.19241v5",
    "arxiv_url": "http://arxiv.org/abs/2501.19241v5",
    "comment": null,
    "relevance_score": 1.0
  },
  {
    "id": "2509.04241v1",
    "title": "Would I regret being different? The influence of social norms on   attitudes toward AI usage",
    "summary": "Prior research shows that social norms can reduce algorithm aversion, but\nlittle is known about how such norms become established. Most accounts\nemphasize technological and individual determinants, yet AI adoption unfolds\nwithin organizational social contexts shaped by peers and supervisors. We ask\nwhether the source of the norm-peers or supervisors-shapes AI usage behavior.\nThis question is practically relevant for organizations seeking to promote\neffective AI adoption. We conducted an online vignette experiment, complemented\nby qualitative data on participants' feelings and justifications after\n(counter-)normative behavior. In line with the theory, counter-normative\nchoices elicited higher regret than norm-adherent choices. On average, choosing\nAI increased regret compared to choosing an human. This aversion was weaker\nwhen AI use was presented as the prevailing norm, indicating a statistically\nsignificant interaction between AI use and an AI-favoring norm. Participants\nalso attributed less blame to technology than to humans, which increased regret\nwhen AI was chosen over human expertise. Both peer and supervisor influence\nemerged as relevant factors, though contrary to expectations they did not\nsignificantly affect regret. Our findings suggest that regret aversion,\nembedded in social norms, is a central mechanism driving imitation in\nAI-related decision-making.",
    "authors": [
      "Jaroslaw Kornowicz",
      "Maurice Pape",
      "Kirsten Thommes"
    ],
    "published": "2025-09-04T14:15:44Z",
    "updated": "2025-09-04T14:15:44Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.04241v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04241v1",
    "comment": "30 pages, 5 figures",
    "relevance_score": 1.0
  },
  {
    "id": "2509.04056v1",
    "title": "The MolecularWeb Universe: Web-Based, Immersive, Multiuser Molecular   Graphics And Modeling, for Education and Work in Chemistry, Structural   Biology, and Materials Sciences",
    "summary": "Molecular visualization software has long supported research and education in\nchemical and structural sciences, but consumer devices constrained to 2D inputs\nand outputs pose two major challenges: they poorly convey 3D nature, and 3D\nmanipulation is very difficult. eXtended Reality (XR, including AR and VR)\noffers new ways to see and interact with molecules in three dimensions. This\nchapter presents the \"MolecularWeb\" ecosystem (https://molecularweb.org), a set\nof web-based tools for immersive visualization, modeling, and simulations,\nalready widely used in education and science communication and now expanding\ntoward research applications. We cover moleculARweb, which provides AR\neducational activities via phones, tablets, and computers; MolecularWebXR, a\nmultiuser WebXR platform accessible from both headsets and simpler devices,\nsupporting immersive education, outreach, and scientific discussion; and\nPDB2AR, which enables users to generate custom content for MolecularWebXR and\nstandalone AR/VR. Finally, we introduce a prototype and an upcoming version of\nHandMol, our latest WebXR software which allows concurrent multiuser immersive\nvisualization and modeling of molecules with bare hands supported by real-time\nmolecular mechanics, natural language input via a language model, and access\nthrough both high-end headsets or consumer devices like smartphones and\nlaptops. Together, these tools demonstrate the present and near-future of\naccessible, interactive molecular science on the web.",
    "authors": [
      "Luciano A. Abriata"
    ],
    "published": "2025-09-04T09:37:24Z",
    "updated": "2025-09-04T09:37:24Z",
    "categories": [
      "physics.chem-ph",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.04056v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04056v1",
    "comment": "37 pages, 7 figures",
    "relevance_score": 1.0
  },
  {
    "id": "2306.17477v3",
    "title": "Beyond-Voice: Towards Continuous 3D Hand Pose Tracking on Commercial   Home Assistant Devices",
    "summary": "The surging popularity of home assistants and their voice user interface\n(VUI) have made them an ideal central control hub for smart home devices.\nHowever, current form factors heavily rely on VUI, which poses accessibility\nand usability issues; some latest ones are equipped with additional cameras and\ndisplays, which are costly and raise privacy concerns. These concerns jointly\nmotivate Beyond-Voice, a novel high-fidelity acoustic sensing system that\nallows commodity home assistant devices to track and reconstruct hand poses\ncontinuously. It transforms the home assistant into an active sonar system\nusing its existing onboard microphones and speakers. We feed a high-resolution\nrange profile to the deep learning model that can analyze the motions of\nmultiple body parts and predict the 3D positions of 21 finger joints, bringing\nthe granularity for acoustic hand tracking to the next level. It operates\nacross different environments and users without the need for personalized\ntraining data. A user study with 11 participants in 3 different environments\nshows that Beyond-Voice can track joints with an average mean absolute error of\n16.47mm without any training data provided by the testing subject.",
    "authors": [
      "Yin Li",
      "Rohan Reddy",
      "Cheng Zhang",
      "Rajalakshmi Nandakumar"
    ],
    "published": "2023-06-30T08:49:41Z",
    "updated": "2025-09-04T06:13:55Z",
    "categories": [
      "cs.SD",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2306.17477v3",
    "arxiv_url": "http://arxiv.org/abs/2306.17477v3",
    "comment": "Accepted by IPSN 2024",
    "relevance_score": 1.0
  },
  {
    "id": "2509.03848v1",
    "title": "Towards an Understanding of Developer Experience-Driven Transparency in   Software Ecosystems",
    "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.",
    "authors": [
      "Rodrigo Oliveira Zacarias",
      "Rodrigo Pereira dos Santos",
      "Patricia Lago"
    ],
    "published": "2025-09-04T03:17:22Z",
    "updated": "2025-09-04T03:17:22Z",
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.03848v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03848v1",
    "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025",
    "relevance_score": 1.0
  },
  {
    "id": "2509.03812v1",
    "title": "Exploring the Integration of Extended Reality and Artificial   Intelligence (AI) for Remote STEM Education and Assessment",
    "summary": "This paper presents a dynamic gamification architecture for an Extended\nReality Artificial Intelligence virtual training environment designed to\nenhance STEM education through immersive adaptive, and kinesthetic learning.\nThe proposed system can be introduced in four phases: Introduction Phase,\nComponent Development Phase, Fault Introduction and Correction Phase and\nGenerative AI XR scenarios Phase. Security and privacy are discussed via a\ndefense-in-depth approach spanning client, middleware, and backend layers,\nincorporating AES 256 encryption, multi-factor authentication, role-based\naccess control and GDPR or FERPA compliance. Risks such as sensor exploitation,\nperceptual manipulation, and virtual physical harm are identified, with\nmitigation strategies embedded at the design stage. Potential barriers to large\nscale adoption-including technical complexity, cost of deployment, and need for\ncybersecurity expertise are discussed.",
    "authors": [
      "Shadeeb Hossain",
      "Natalie Sommer",
      "Neda Adib"
    ],
    "published": "2025-09-04T01:57:45Z",
    "updated": "2025-09-04T01:57:45Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.03812v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03812v1",
    "comment": "9 pages, 5 figures, 1 table",
    "relevance_score": 1.0
  },
  {
    "id": "2509.04441v1",
    "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
    "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
    "authors": [
      "Hao-Shu Fang",
      "Branden Romero",
      "Yichen Xie",
      "Arthur Hu",
      "Bo-Ruei Huang",
      "Juan Alvarez",
      "Matthew Kim",
      "Gabriel Margolis",
      "Kavya Anbarasu",
      "Masayoshi Tomizuka",
      "Edward Adelson",
      "Pulkit Agrawal"
    ],
    "published": "2025-09-04T17:57:13Z",
    "updated": "2025-09-04T17:57:13Z",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04441v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04441v1",
    "comment": "project page: https://dex-op.github.io",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04357v1",
    "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity   Disambiguation",
    "summary": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.",
    "authors": [
      "Jiajun He",
      "Naoki Sawada",
      "Koichi Miyazaki",
      "Tomoki Toda"
    ],
    "published": "2025-09-04T16:18:34Z",
    "updated": "2025-09-04T16:18:34Z",
    "categories": [
      "cs.CL",
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04357v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04357v1",
    "comment": "Accepted by ASRU 2025",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04337v1",
    "title": "Decoupled Entity Representation Learning for Pinterest Ads Ranking",
    "summary": "In this paper, we introduce a novel framework following an\nupstream-downstream paradigm to construct user and item (Pin) embeddings from\ndiverse data sources, which are essential for Pinterest to deliver personalized\nPins and ads effectively. Our upstream models are trained on extensive data\nsources featuring varied signals, utilizing complex architectures to capture\nintricate relationships between users and Pins on Pinterest. To ensure\nscalability of the upstream models, entity embeddings are learned, and\nregularly refreshed, rather than real-time computation, allowing for\nasynchronous interaction between the upstream and downstream models. These\nembeddings are then integrated as input features in numerous downstream tasks,\nincluding ad retrieval and ranking models for CTR and CVR predictions. We\ndemonstrate that our framework achieves notable performance improvements in\nboth offline and online settings across various downstream tasks. This\nframework has been deployed in Pinterest's production ad ranking systems,\nresulting in significant gains in online metrics.",
    "authors": [
      "Jie Liu",
      "Yinrui Li",
      "Jiankai Sun",
      "Kungang Li",
      "Han Sun",
      "Sihan Wang",
      "Huasen Wu",
      "Siyuan Gao",
      "Paulo Soares",
      "Nan Li",
      "Zhifang Liu",
      "Haoyang Li",
      "Siping Ji",
      "Ling Leng",
      "Prathibha Deshikachar"
    ],
    "published": "2025-09-04T15:56:40Z",
    "updated": "2025-09-04T15:56:40Z",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04337v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04337v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2211.12143v3",
    "title": "Autonomation, Not Automation: Activities and Needs of European   Fact-checkers as a Basis for Designing Human-Centered AI Systems",
    "summary": "To mitigate the negative effects of false information more effectively, the\ndevelopment of Artificial Intelligence (AI) systems to assist fact-checkers is\nneeded. Nevertheless, the lack of focus on the needs of these stakeholders\nresults in their limited acceptance and skepticism toward automating the whole\nfact-checking process. In this study, we conducted semi-structured in-depth\ninterviews with Central European fact-checkers. Their activities and problems\nwere analyzed using iterative content analysis. The most significant problems\nwere validated with a survey of European fact-checkers, in which we collected\n24 responses from 20 countries, i.e., 62% of active European signatories of the\nInternational Fact-Checking Network (IFCN). Our contributions include an\nin-depth examination of the variability of fact-checking work in\nnon-English-speaking regions, which still remained largely uncovered. By\naligning them with the knowledge from prior studies, we created conceptual\nmodels that help to understand the fact-checking processes. In addition, we\nmapped our findings on the fact-checkers' activities and needs to the relevant\ntasks for AI research, while providing a discussion on three AI tasks that were\nnot covered by previous similar studies. The new opportunities identified for\nAI researchers and developers have implications for the focus of AI research in\nthis domain.",
    "authors": [
      "Andrea Hrckova",
      "Robert Moro",
      "Ivan Srba",
      "Jakub Simko",
      "Maria Bielikova"
    ],
    "published": "2022-11-22T10:18:09Z",
    "updated": "2025-09-04T15:52:13Z",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2211.12143v3",
    "arxiv_url": "http://arxiv.org/abs/2211.12143v3",
    "comment": "44 pages, 13 figures, 2 annexes. Accepted to ACM Journal on\n  Responsible Computing",
    "relevance_score": 0.5
  },
  {
    "id": "2508.02548v2",
    "title": "The KG-ER Conceptual Schema Language",
    "summary": "We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.",
    "authors": [
      "Enrico Franconi",
      "Benoît Groz",
      "Jan Hidders",
      "Nina Pardal",
      "Sławek Staworko",
      "Jan Van den Bussche",
      "Piotr Wieczorek"
    ],
    "published": "2025-08-04T16:01:28Z",
    "updated": "2025-09-04T15:06:45Z",
    "categories": [
      "cs.DB",
      "cs.AI",
      "68P15"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2508.02548v2",
    "arxiv_url": "http://arxiv.org/abs/2508.02548v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2502.10118v2",
    "title": "Image Embedding Sampling Method for Diverse Captioning",
    "summary": "Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, comparably smaller\nVLMs prioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.",
    "authors": [
      "Sania Waheed",
      "Na Min An"
    ],
    "published": "2025-02-14T12:33:19Z",
    "updated": "2025-09-04T15:00:25Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2502.10118v2",
    "arxiv_url": "http://arxiv.org/abs/2502.10118v2",
    "comment": "17 pages, 5 figures, 9 tables",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04243v1",
    "title": "Learning Active Perception via Self-Evolving Preference Optimization for   GUI Grounding",
    "summary": "Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.",
    "authors": [
      "Wanfu Wang",
      "Qipeng Huang",
      "Guangquan Xue",
      "Xiaobo Liang",
      "Juntao Li"
    ],
    "published": "2025-09-04T14:17:01Z",
    "updated": "2025-09-04T14:17:01Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04243v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04243v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2410.02807v2",
    "title": "AutoPETIII: The Tracer Frontier. What Frontier?",
    "summary": "For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.",
    "authors": [
      "Zacharia Mesbah",
      "Léo Mottay",
      "Romain Modzelewski",
      "Pierre Decazes",
      "Sébastien Hapdey",
      "Su Ruan",
      "Sébastien Thureau"
    ],
    "published": "2024-09-19T13:26:31Z",
    "updated": "2025-09-04T14:13:49Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2410.02807v2",
    "arxiv_url": "http://arxiv.org/abs/2410.02807v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04192v1",
    "title": "Domain size asymptotics for Markov logic networks",
    "summary": "A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.",
    "authors": [
      "Vera Koponen"
    ],
    "published": "2025-09-04T13:15:02Z",
    "updated": "2025-09-04T13:15:02Z",
    "categories": [
      "cs.AI",
      "cs.LO",
      "math.LO",
      "68T27, 68T30, 68T37, 03C13",
      "I.2; F.4; G.3"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04192v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04192v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04159v1",
    "title": "Towards an Action-Centric Ontology for Cooking Procedures Using Temporal   Graphs",
    "summary": "Formalizing cooking procedures remains a challenging task due to their\ninherent complexity and ambiguity. We introduce an extensible domain-specific\nlanguage for representing recipes as directed action graphs, capturing\nprocesses, transfers, environments, concurrency, and compositional structure.\nOur approach enables precise, modular modeling of complex culinary workflows.\nInitial manual evaluation on a full English breakfast recipe demonstrates the\nDSL's expressiveness and suitability for future automated recipe analysis and\nexecution. This work represents initial steps towards an action-centric\nontology for cooking, using temporal graphs to enable structured machine\nunderstanding, precise interpretation, and scalable automation of culinary\nprocesses - both in home kitchens and professional culinary settings.",
    "authors": [
      "Aarush Kumbhakern",
      "Saransh Kumar Gupta",
      "Lipika Dey",
      "Partha Pratim Das"
    ],
    "published": "2025-09-04T12:34:56Z",
    "updated": "2025-09-04T12:34:56Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04159v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04159v1",
    "comment": "6 pages, 3 figures, 1 table, 11 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04130v1",
    "title": "The human biological advantage over AI",
    "summary": "Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.",
    "authors": [
      "William Stewart"
    ],
    "published": "2025-09-04T11:54:27Z",
    "updated": "2025-09-04T11:54:27Z",
    "categories": [
      "cs.AI",
      "cs.CY",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04130v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04130v1",
    "comment": "12 pages",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04129v1",
    "title": "Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on   Controllers in Reactive Synthesis",
    "summary": "In the game-theoretic approach to controller synthesis, we model the\ninteraction between a system to be controlled and its environment as a game\nbetween these entities, and we seek an appropriate (e.g., winning or optimal)\nstrategy for the system. This strategy then serves as a formal blueprint for a\nreal-world controller. A common belief is that simple (e.g., using limited\nmemory) strategies are better: corresponding controllers are easier to conceive\nand understand, and cheaper to produce and maintain.\n  This invited contribution focuses on the complexity of strategies in a\nvariety of synthesis contexts. We discuss recent results concerning memory and\nrandomness, and take a brief look at what lies beyond our traditional notions\nof complexity for strategies.",
    "authors": [
      "Mickael Randour"
    ],
    "published": "2025-09-04T11:54:19Z",
    "updated": "2025-09-04T11:54:19Z",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.FL",
      "math.PR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04129v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04129v1",
    "comment": "Invited paper at RP 2025",
    "relevance_score": 0.5
  },
  {
    "id": "2506.08570v3",
    "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling   Paradigms for Text-to-Music Generation",
    "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly in many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and identify which design choices influence\nperformance the most. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: auto-regressive decoding and conditional\nflow-matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
    "authors": [
      "Or Tal",
      "Felix Kreuk",
      "Yossi Adi"
    ],
    "published": "2025-06-10T08:37:45Z",
    "updated": "2025-09-04T11:54:13Z",
    "categories": [
      "cs.SD",
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2506.08570v3",
    "arxiv_url": "http://arxiv.org/abs/2506.08570v3",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04051v1",
    "title": "Neural Video Compression with In-Loop Contextual Filtering and   Out-of-Loop Reconstruction Enhancement",
    "summary": "This paper explores the application of enhancement filtering techniques in\nneural video compression. Specifically, we categorize these techniques into\nin-loop contextual filtering and out-of-loop reconstruction enhancement based\non whether the enhanced representation affects the subsequent coding loop.\nIn-loop contextual filtering refines the temporal context by mitigating error\npropagation during frame-by-frame encoding. However, its influence on both the\ncurrent and subsequent frames poses challenges in adaptively applying filtering\nthroughout the sequence. To address this, we introduce an adaptive coding\ndecision strategy that dynamically determines filtering application during\nencoding. Additionally, out-of-loop reconstruction enhancement is employed to\nrefine the quality of reconstructed frames, providing a simple yet effective\nimprovement in coding efficiency. To the best of our knowledge, this work\npresents the first systematic study of enhancement filtering in the context of\nconditional-based neural video compression. Extensive experiments demonstrate a\n7.71% reduction in bit rate compared to state-of-the-art neural video codecs,\nvalidating the effectiveness of the proposed approach.",
    "authors": [
      "Yaojun Wu",
      "Chaoyi Lin",
      "Yiming Wang",
      "Semih Esenlik",
      "Zhaobin Zhang",
      "Kai Zhang",
      "Li Zhang"
    ],
    "published": "2025-09-04T09:29:30Z",
    "updated": "2025-09-04T09:29:30Z",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04051v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04051v1",
    "comment": "9 pages, 8 figures, Accepted to ACMMM 2025",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04041v1",
    "title": "Oruga: An Avatar of Representational Systems Theory",
    "summary": "Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.",
    "authors": [
      "Daniel Raggi",
      "Gem Stapleton",
      "Mateja Jamnik",
      "Aaron Stockdill",
      "Grecia Garcia Garcia",
      "Peter C-H. Cheng"
    ],
    "published": "2025-09-04T09:21:57Z",
    "updated": "2025-09-04T09:21:57Z",
    "categories": [
      "cs.AI",
      "cs.LO",
      "68T30, 68T27, 03B35",
      "I.2.4; I.2.3; F.4.1; F.4.3"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.04041v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04041v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2506.03315v2",
    "title": "Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum   as Fallback",
    "summary": "We study how linear orders can be employed to realise choice functions for\nwhich the set of potential choices is restricted, i.e., the possible choice is\nnot possible among the full powerset of all alternatives. In such restricted\nsettings, constructing a choice function via a relation on the alternatives is\nnot always possible. However, we show that one can always construct a choice\nfunction via a linear order on sets of alternatives, even when a fallback value\nis encoded as the minimal element in the linear order. The axiomatics of such\nchoice functions are presented for the general case and the case of\nunion-closed input restrictions. Restricted choice structures have applications\nin knowledge representation and reasoning, and here we discuss their\napplications for theory change and abstract argumentation.",
    "authors": [
      "Kai Sauerwald",
      "Kenneth Skiba",
      "Eduardo Fermé",
      "Thomas Meyer"
    ],
    "published": "2025-06-03T19:03:12Z",
    "updated": "2025-09-04T09:04:10Z",
    "categories": [
      "cs.AI",
      "cs.LO",
      "03E99, 91B14",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2506.03315v2",
    "arxiv_url": "http://arxiv.org/abs/2506.03315v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.03249v2",
    "title": "Structure Transfer: an Inference-Based Calculus for the Transformation   of Representations",
    "summary": "Representation choice is of fundamental importance to our ability to\ncommunicate and reason effectively. A major unsolved problem, addressed in this\npaper, is how to devise representational-system (RS) agnostic techniques that\ndrive representation transformation and choice. We present a novel calculus,\ncalled structure transfer, that enables representation transformation across\ndiverse RSs. Specifically, given a source representation drawn from a source\nRS, the rules of structure transfer allow us to generate a target\nrepresentation for a target RS. The generality of structure transfer comes in\npart from its ability to ensure that the source representation and the\ngenerated target representation satisfy any specified relation (such as\nsemantic equivalence). This is done by exploiting schemas, which encode\nknowledge about RSs. Specifically, schemas can express preservation of\ninformation across relations between any pair of RSs, and this knowledge is\nused by structure transfer to derive a structure for the target representation\nwhich ensures that the desired relation holds. We formalise this using\nRepresentational Systems Theory, building on the key concept of a construction\nspace. The abstract nature of construction spaces grants them the generality to\nmodel RSs of diverse kinds, including formal languages, geometric figures and\ndiagrams, as well as informal notations. Consequently, structure transfer is a\nsystem-agnostic calculus that can be used to identify alternative\nrepresentations in a wide range of practical settings.",
    "authors": [
      "Daniel Raggi",
      "Gem Stapleton",
      "Mateja Jamnik",
      "Aaron Stockdill",
      "Grecia Garcia Garcia",
      "Peter C-H. Cheng"
    ],
    "published": "2025-09-03T12:07:23Z",
    "updated": "2025-09-04T08:55:32Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "68T30, 68T27, 03B35",
      "I.2.4; I.2.3; F.4.1; F.4.3"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2509.03249v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03249v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04445v1",
    "title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI   Alignment",
    "summary": "Recent AI work trends towards incorporating human-centric objectives, with\nthe explicit goal of aligning AI models to personal preferences and societal\nvalues. Using standard preference elicitation methods, researchers and\npractitioners build models of human decisions and judgments, which are then\nused to align AI behavior with that of humans. However, models commonly used in\nsuch elicitation processes often do not capture the true cognitive processes of\nhuman decision making, such as when people use heuristics to simplify\ninformation associated with a decision problem. As a result, models learned\nfrom people's decisions often do not align with their cognitive processes, and\ncan not be used to validate the learning framework for generalization to other\ndecision-making tasks. To address this limitation, we take an axiomatic\napproach to learning cognitively faithful decision processes from pairwise\ncomparisons. Building on the vast literature characterizing the cognitive\nprocesses that contribute to human decision-making, and recent work\ncharacterizing such processes in pairwise comparison tasks, we define a class\nof models in which individual features are first processed and compared across\nalternatives, and then the processed features are then aggregated via a fixed\nrule, such as the Bradley-Terry rule. This structured processing of information\nensures such models are realistic and feasible candidates to represent\nunderlying human decision-making processes. We demonstrate the efficacy of this\nmodeling approach in learning interpretable models of human decision making in\na kidney allocation task, and show that our proposed models match or surpass\nthe accuracy of prior models of human pairwise decision-making.",
    "authors": [
      "Cyrus Cousins",
      "Vijay Keswani",
      "Vincent Conitzer",
      "Hoda Heidari",
      "Jana Schaich Borg",
      "Walter Sinnott-Armstrong"
    ],
    "published": "2025-09-04T17:59:29Z",
    "updated": "2025-09-04T17:59:29Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04445v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04445v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04422v1",
    "title": "Echo State Networks as State-Space Models: A Systems Perspective",
    "summary": "Echo State Networks (ESNs) are typically presented as efficient,\nreadout-trained recurrent models, yet their dynamics and design are often\nguided by heuristics rather than first principles. We recast ESNs explicitly as\nstate-space models (SSMs), providing a unified systems-theoretic account that\nlinks reservoir computing with classical identification and modern kernelized\nSSMs. First, we show that the echo-state property is an instance of\ninput-to-state stability for a contractive nonlinear SSM and derive verifiable\nconditions in terms of leak, spectral scaling, and activation Lipschitz\nconstants. Second, we develop two complementary mappings: (i) small-signal\nlinearizations that yield locally valid LTI SSMs with interpretable poles and\nmemory horizons; and (ii) lifted/Koopman random-feature expansions that render\nthe ESN a linear SSM in an augmented state, enabling transfer-function and\nconvolutional-kernel analyses. This perspective yields frequency-domain\ncharacterizations of memory spectra and clarifies when ESNs emulate structured\nSSM kernels. Third, we cast teacher forcing as state estimation and propose\nKalman/EKF-assisted readout learning, together with EM for hyperparameters\n(leak, spectral radius, process/measurement noise) and a hybrid subspace\nprocedure for spectral shaping under contraction constraints.",
    "authors": [
      "Pradeep Singh",
      "Balasubramanian Raman"
    ],
    "published": "2025-09-04T17:42:03Z",
    "updated": "2025-09-04T17:42:03Z",
    "categories": [
      "cs.LG",
      "93C10, 68T07, 93C05, 93E11, 93B30, 93B05, 93B07, 62M10",
      "I.2.6; I.5.1; I.6.5; I.6.4; G.3"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04422v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04422v1",
    "comment": "27 pages, 1 figure",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04415v1",
    "title": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure   Learning in Mixed Observational Data",
    "summary": "Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.",
    "authors": [
      "Wenrui Li",
      "Qinghao Zhang",
      "Xiaowo Wang"
    ],
    "published": "2025-09-04T17:37:35Z",
    "updated": "2025-09-04T17:37:35Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04415v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04415v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04413v1",
    "title": "SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety   Certificates",
    "summary": "This paper proposes a fully data-driven motion-planning framework for\nhomogeneous linear multi-agent systems that operate in shared, obstacle-filled\nworkspaces without access to explicit system models. Each agent independently\nlearns its closed-loop behavior from experimental data by solving convex\nsemidefinite programs that generate locally invariant ellipsoids and\ncorresponding state-feedback gains. These ellipsoids, centered along grid-based\nwaypoints, certify the dynamic feasibility of short-range transitions and\ndefine safe regions of operation. A sampling-based planner constructs a tree of\nsuch waypoints, where transitions are allowed only when adjacent ellipsoids\noverlap, ensuring invariant-to-invariant transitions and continuous safety. All\nagents expand their trees simultaneously and are coordinated through a\nspace-time reservation table that guarantees inter-agent safety by preventing\nsimultaneous occupancy and head-on collisions. Each successful edge in the tree\nis equipped with its own local controller, enabling execution without\nre-solving optimization problems at runtime. The resulting trajectories are not\nonly dynamically feasible but also provably safe with respect to both\nenvironmental constraints and inter-agent collisions. Simulation results\ndemonstrate the effectiveness of the approach in synthesizing synchronized,\nsafe trajectories for multiple agents under shared dynamics and constraints,\nusing only data and convex optimization tools.",
    "authors": [
      "Babak Esmaeili",
      "Hamidreza Modares"
    ],
    "published": "2025-09-04T17:34:59Z",
    "updated": "2025-09-04T17:34:59Z",
    "categories": [
      "cs.MA",
      "cs.SY",
      "math.OC",
      "eess.SY",
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04413v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04413v1",
    "comment": "Submitted to IEEE Transactions on Automation Science and Engineering",
    "relevance_score": 0.5
  },
  {
    "id": "2504.04873v2",
    "title": "Closed-Loop Neural Operator-Based Observer of Traffic Density",
    "summary": "We consider the problem of traffic density estimation with sparse\nmeasurements from stationary roadside sensors. Our approach uses Fourier neural\noperators to learn macroscopic traffic flow dynamics from high-fidelity data.\nDuring inference, the operator functions as an open-loop predictor of traffic\nevolution. To close the loop, we couple the open-loop operator with a\ncorrection operator that combines the predicted density with sparse\nmeasurements from the sensors. Simulations with the SUMO software indicate\nthat, compared to open-loop observers, the proposed closed-loop observer\nexhibits classical closed-loop properties such as robustness to noise and\nultimate boundedness of the error. This shows the advantages of combining\nlearned physics with real-time corrections, and opens avenues for accurate,\nefficient, and interpretable data-driven observers.",
    "authors": [
      "Alice Harting",
      "Karl Henrik Johansson",
      "Matthieu Barreau"
    ],
    "published": "2025-04-07T09:28:50Z",
    "updated": "2025-09-04T17:30:11Z",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2504.04873v2",
    "arxiv_url": "http://arxiv.org/abs/2504.04873v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04363v1",
    "title": "When three experiments are better than two: Avoiding intractable   correlated aleatoric uncertainty by leveraging a novel bias--variance   tradeoff",
    "summary": "Real-world experimental scenarios are characterized by the presence of\nheteroskedastic aleatoric uncertainty, and this uncertainty can be correlated\nin batched settings. The bias--variance tradeoff can be used to write the\nexpected mean squared error between a model distribution and a ground-truth\nrandom variable as the sum of an epistemic uncertainty term, the bias squared,\nand an aleatoric uncertainty term. We leverage this relationship to propose\nnovel active learning strategies that directly reduce the bias between\nexperimental rounds, considering model systems both with and without noise.\nFinally, we investigate methods to leverage historical data in a quadratic\nmanner through the use of a novel cobias--covariance relationship, which\nnaturally proposes a mechanism for batching through an eigendecomposition\nstrategy. When our difference-based method leveraging the cobias--covariance\nrelationship is utilized in a batched setting (with a quadratic estimator), we\noutperform a number of canonical methods including BALD and Least Confidence.",
    "authors": [
      "Paul Scherer",
      "Andreas Kirsch",
      "Jake P. Taylor-King"
    ],
    "published": "2025-09-04T16:23:54Z",
    "updated": "2025-09-04T16:23:54Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04363v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04363v1",
    "comment": "16 pages, 5 figures",
    "relevance_score": 0.5
  },
  {
    "id": "2505.23445v2",
    "title": "The Strong, Weak and Benign Goodhart's law. An independence-free and   paradigm-agnostic formalisation",
    "summary": "Goodhart's law is a famous adage in policy-making that states that ``When a\nmeasure becomes a target, it ceases to be a good measure''. As machine learning\nmodels and the optimisation capacity to train them grow, growing empirical\nevidence reinforced the belief in the validity of this law without however\nbeing formalised. Recently, a few attempts were made to formalise Goodhart's\nlaw, either by categorising variants of it, or by looking at how optimising a\nproxy metric affects the optimisation of an intended goal. In this work, we\nalleviate the simplifying independence assumption, made in previous works, and\nthe assumption on the learning paradigm made in most of them, to study the\neffect of the coupling between the proxy metric and the intended goal on\nGoodhart's law. Our results show that in the case of light tailed goal and\nlight tailed discrepancy, dependence does not change the nature of Goodhart's\neffect. However, in the light tailed goal and heavy tailed discrepancy case, we\nexhibit an example where over-optimisation occurs at a rate inversely\nproportional to the heavy tailedness of the discrepancy between the goal and\nthe metric. %",
    "authors": [
      "Adrien Majka",
      "El-Mahdi El-Mhamdi"
    ],
    "published": "2025-05-29T13:42:05Z",
    "updated": "2025-09-04T15:25:45Z",
    "categories": [
      "math.ST",
      "stat.ML",
      "cs.LG",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2505.23445v2",
    "arxiv_url": "http://arxiv.org/abs/2505.23445v2",
    "comment": "32 pages, 1 figure",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04296v1",
    "title": "Using causal abstractions to accelerate decision-making in complex   bandit problems",
    "summary": "Although real-world decision-making problems can often be encoded as causal\nmulti-armed bandits (CMABs) at different levels of abstraction, a general\nmethodology exploiting the information and computational advantages of each\nabstraction level is missing. In this paper, we propose AT-UCB, an algorithm\nwhich efficiently exploits shared information between CMAB problem instances\ndefined at different levels of abstraction. More specifically, AT-UCB leverages\ncausal abstraction (CA) theory to explore within a cheap-to-simulate and\ncoarse-grained CMAB instance, before employing the traditional upper confidence\nbound (UCB) algorithm on a restricted set of potentially optimal actions in the\nCMAB of interest, leading to significant reductions in cumulative regret when\ncompared to the classical UCB algorithm. We illustrate the advantages of AT-UCB\ntheoretically, through a novel upper bound on the cumulative regret, and\nempirically, by applying AT-UCB to epidemiological simulators with varying\nresolution and computational cost.",
    "authors": [
      "Joel Dyer",
      "Nicholas Bishop",
      "Anisoara Calinescu",
      "Michael Wooldridge",
      "Fabio Massimo Zennaro"
    ],
    "published": "2025-09-04T15:11:04Z",
    "updated": "2025-09-04T15:11:04Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04296v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04296v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04295v1",
    "title": "A Primer on Causal and Statistical Dataset Biases for Fair and Robust   Image Analysis",
    "summary": "Machine learning methods often fail when deployed in the real world. Worse\nstill, they fail in high-stakes situations and across socially sensitive lines.\nThese issues have a chilling effect on the adoption of machine learning methods\nin settings such as medical diagnosis, where they are arguably best-placed to\nprovide benefits if safely deployed. In this primer, we introduce the causal\nand statistical structures which induce failure in machine learning methods for\nimage analysis. We highlight two previously overlooked problems, which we call\nthe \\textit{no fair lunch} problem and the \\textit{subgroup separability}\nproblem. We elucidate why today's fair representation learning methods fail to\nadequately solve them and propose potential paths forward for the field.",
    "authors": [
      "Charles Jones",
      "Ben Glocker"
    ],
    "published": "2025-09-04T15:06:58Z",
    "updated": "2025-09-04T15:06:58Z",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04295v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04295v1",
    "comment": "Excerpt from C. Jones' PhD thesis. Winner of the G-Research PhD prize\n  2025",
    "relevance_score": 0.5
  },
  {
    "id": "2507.22832v3",
    "title": "Pulling Back the Curtain on ReLU Networks",
    "summary": "Since any ReLU network is piecewise affine, its hidden units can be\ncharacterized by their pullbacks through the active subnetwork, i.e., by their\ngradients (up to bias terms). However, gradients of deeper neurons are\nnotoriously misaligned, which obscures the network's internal representations.\nWe posit that models do align gradients with data, yet this is concealed by the\nintrinsic noise of the ReLU hard gating. We validate this intuition by applying\nsoft gating in the backward pass only, reducing the local impact of weakly\nexcited neurons. The resulting modified gradients, which we call \"excitation\npullbacks\", exhibit striking perceptual alignment on a number of\nImageNet-pretrained architectures, while the rudimentary pixel-space gradient\nascent quickly produces easily interpretable input- and target-specific\nfeatures. Inspired by these findings, we formulate the \"path stability\"\nhypothesis, claiming that the binary activation patterns largely stabilize\nduring training and get encoded in the pre-activation distribution of the final\nmodel. When true, excitation pullbacks become aligned with the gradients of a\nkernel machine that mainly determines the network's decision. This provides a\ntheoretical justification for the apparent faithfulness of the feature\nattributions based on these pullbacks, potentially even leading to mechanistic\ninterpretability of deeper models. Incidentally, we give a possible explanation\nfor the effectiveness of Batch Normalization and Deep Features, together with a\nnovel perspective on the network's internal memory and generalization\nproperties. We release the code and an interactive app for easier exploration\nof the excitation pullbacks.",
    "authors": [
      "Maciej Satkiewicz"
    ],
    "published": "2025-07-30T16:47:42Z",
    "updated": "2025-09-04T15:05:54Z",
    "categories": [
      "I.2.6; I.4.10",
      "cs.NE",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.22832v3",
    "arxiv_url": "http://arxiv.org/abs/2507.22832v3",
    "comment": "12 pages, 3-page appendix, 4 figures, preprint; v3 changes: changed\n  title, improved abstract, expanded introduction, added section on\n  implications of the path stability",
    "relevance_score": 0.5
  },
  {
    "id": "2507.06469v3",
    "title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph   Representation Learning",
    "summary": "Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection (MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.",
    "authors": [
      "Yudan Song",
      "Yuecen Wei",
      "Yuhang Lu",
      "Qingyun Sun",
      "Minglai Shao",
      "Li-e Wang",
      "Chunming Hu",
      "Xianxian Li",
      "Xingcheng Fu"
    ],
    "published": "2025-07-09T01:00:55Z",
    "updated": "2025-09-04T15:01:28Z",
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.06469v3",
    "arxiv_url": "http://arxiv.org/abs/2507.06469v3",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2405.06464v5",
    "title": "Single-seed generation of Brownian paths and integrals for adaptive and   high order SDE solvers",
    "summary": "Despite the success of adaptive time-stepping in ODE simulation, it has so\nfar seen few applications for Stochastic Differential Equations (SDEs). To\nsimulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have\nbeen developed, which can generate Brownian motion (BM) non-chronologically.\nHowever, in most applications, knowing only the values of Brownian motion is\nnot enough to achieve a high order of convergence; for that, we must compute\ntime-integrals of BM such as $\\int_s^t W_r \\, dr$. With the aim of using high\norder SDE solvers adaptively, we extend the VBT to generate these integrals of\nBM in addition to the Brownian increments. A JAX-based implementation of our\nconstruction is included in the popular Diffrax library\n(https://github.com/patrick-kidger/diffrax).\n  Since the entire Brownian path produced by VBT is uniquely determined by a\nsingle PRNG seed, previously generated samples need not be stored, which\nresults in a constant memory footprint and enables experiment repeatability and\nstrong error estimation. Based on binary search, the VBT's time complexity is\nlogarithmic in the tolerance parameter $\\varepsilon$. Unlike the original VBT\nalgorithm, which was only precise at some dyadic times, we prove that our\nconstruction exactly matches the joint distribution of the Brownian motion and\nits time integrals at any query times, provided they are at least $\\varepsilon$\napart.\n  We present two applications of adaptive high order solvers enabled by our new\nVBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve\nmore than twice the convergence order of constant stepping. We apply an\nadaptive third order underdamped or kinetic Langevin solver to an MCMC problem,\nwhere our approach outperforms the No U-Turn Sampler, while using only a tenth\nof its function evaluations.",
    "authors": [
      "Andraž Jelinčič",
      "James Foster",
      "Patrick Kidger"
    ],
    "published": "2024-05-10T13:16:23Z",
    "updated": "2025-09-04T14:32:54Z",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "math.PR",
      "stat.CO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2405.06464v5",
    "arxiv_url": "http://arxiv.org/abs/2405.06464v5",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2212.14641v2",
    "title": "Reservoir kernels and Volterra series",
    "summary": "A universal kernel is constructed whose sections approximate any causal and\ntime-invariant filter in the fading memory category with inputs and outputs in\na finite-dimensional Euclidean space. This kernel is built using the reservoir\nfunctional associated with a state-space representation of the Volterra series\nexpansion available for any analytic fading memory filter, and it is hence\ncalled the Volterra reservoir kernel. Even though the state-space\nrepresentation and the corresponding reservoir feature map are defined on an\ninfinite-dimensional tensor algebra space, the kernel map is characterized by\nexplicit recursions that are readily computable for specific data sets when\nemployed in estimation problems using the representer theorem. The empirical\nperformance of the Volterra reservoir kernel is showcased and compared to other\nstandard static and sequential kernels in a multidimensional and highly\nnonlinear learning task for the conditional covariances of financial asset\nreturns.",
    "authors": [
      "Lukas Gonon",
      "Lyudmila Grigoryeva",
      "Juan-Pablo Ortega"
    ],
    "published": "2022-12-30T11:33:20Z",
    "updated": "2025-09-04T14:25:28Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2212.14641v2",
    "arxiv_url": "http://arxiv.org/abs/2212.14641v2",
    "comment": "11 pages, 2 tables",
    "relevance_score": 0.5
  },
  {
    "id": "2504.16943v2",
    "title": "Revealing the empirical flexibility of gas units through deep clustering",
    "summary": "The flexibility of a power generation unit determines how quickly and often\nit can ramp up or down. In energy models, it depends on assumptions on the\ntechnical characteristics of the unit, such as its installed capacity or\nturbine technology. In this paper, we learn the empirical flexibility of gas\nunits from their electricity generation, revealing how real-world limitations\ncan lead to substantial differences between units with similar technical\ncharacteristics. Using a novel deep clustering approach, we transform 5 years\n(2019-2023) of unit-level hourly generation data for 49 German units from 100\nMWp of installed capacity into low-dimensional embeddings. Our unsupervised\napproach identifies two clusters of peaker units (high flexibility) and two\nclusters of non-peaker units (low flexibility). The estimated ramp rates of\nnon-peakers, which constitute half of the sample, display a low empirical\nflexibility, comparable to coal units. Non-peakers, predominantly owned by\nindustry and municipal utilities, show limited response to low residual load\nand negative prices, generating on average 1.3 GWh during those hours. As the\ntransition to renewables increases market variability, regulatory changes will\nbe needed to unlock this flexibility potential.",
    "authors": [
      "Chiara Fusar Bassini",
      "Alice Lixuan Xu",
      "Jorge Sánchez Canales",
      "Lion Hirth",
      "Lynn H. Kaack"
    ],
    "published": "2025-04-14T15:04:01Z",
    "updated": "2025-09-04T14:06:07Z",
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2504.16943v2",
    "arxiv_url": "http://arxiv.org/abs/2504.16943v2",
    "comment": "19 pages, 4 figures, 3 tables",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04222v1",
    "title": "Why Can't I See My Clusters? A Precision-Recall Approach to   Dimensionality Reduction Validation",
    "summary": "Dimensionality Reduction (DR) is widely used for visualizing high-dimensional\ndata, often with the goal of revealing expected cluster structure. However,\nsuch a structure may not always appear in the projections. Existing DR quality\nmetrics assess projection reliability (to some extent) or cluster structure\nquality, but do not explain why expected structures are missing. Visual\nAnalytics solutions can help, but are often time-consuming due to the large\nhyperparameter space. This paper addresses this problem by leveraging a recent\nframework that divides the DR process into two phases: a relationship phase,\nwhere similarity relationships are modeled, and a mapping phase, where the data\nis projected accordingly. We introduce two supervised metrics, precision and\nrecall, to evaluate the relationship phase. These metrics quantify how well the\nmodeled relationships align with an expected cluster structure based on some\nset of labels representing this structure. We illustrate their application\nusing t-SNE and UMAP, and validate the approach through various usage\nscenarios. Our approach can guide hyperparameter tuning, uncover projection\nartifacts, and determine if the expected structure is captured in the\nrelationships, making the DR process faster and more reliable.",
    "authors": [
      "Diede P. M. van der Hoorn",
      "Alessio Arleo",
      "Fernando V. Paulovich"
    ],
    "published": "2025-09-04T13:53:16Z",
    "updated": "2025-09-04T13:53:16Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04222v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04222v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04194v1",
    "title": "Batched Stochastic Matching Bandits",
    "summary": "In this study, we introduce a novel bandit framework for stochastic matching\nbased on the Multi-nomial Logit (MNL) choice model. In our setting, $N$ agents\non one side are assigned to $K$ arms on the other side, where each arm\nstochastically selects an agent from its assigned pool according to an unknown\npreference and yields a corresponding reward. The objective is to minimize\nregret by maximizing the cumulative revenue from successful matches across all\nagents. This task requires solving a combinatorial optimization problem based\non estimated preferences, which is NP-hard and leads a naive approach to incur\na computational cost of $O(K^N)$ per round. To address this challenge, we\npropose batched algorithms that limit the frequency of matching updates,\nthereby reducing the amortized computational cost (i.e., the average cost per\nround) to $O(1)$ while still achieving a regret bound of $\\tilde{O}(\\sqrt{T})$.",
    "authors": [
      "Jung-hun Kim",
      "Min-hwan Oh"
    ],
    "published": "2025-09-04T13:16:32Z",
    "updated": "2025-09-04T13:16:32Z",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04194v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04194v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04174v1",
    "title": "Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric   Similarity Learning of Motion Patterns",
    "summary": "This paper introduces an unobtrusive in-situ measurement method to detect\nuser behavior changes during arbitrary exposures in XR systems. Here, such\nbehavior changes are typically associated with the Proteus effect or bodily\naffordances elicited by different avatars that the users embody in XR. We\npresent a biometric user model based on deep metric similarity learning, which\nuses high-dimensional embeddings as reference vectors to identify behavior\nchanges of individual users. We evaluate our model against two alternative\napproaches: a (non-learned) motion analysis based on central tendencies of\nmovement patterns and subjective post-exposure embodiment questionnaires\nfrequently used in various XR exposures. In a within-subject study,\nparticipants performed a fruit collection task while embodying avatars of\ndifferent body heights (short, actual-height, and tall). Subjective assessments\nconfirmed the effective manipulation of perceived body schema, while the\n(non-learned) objective analyses of head and hand movements revealed\nsignificant differences across conditions. Our similarity learning model\ntrained on the motion data successfully identified the elicited behavior change\nfor various query and reference data pairings of the avatar conditions. The\napproach has several advantages in comparison to existing methods: 1) In-situ\nmeasurement without additional user input, 2) generalizable and scalable motion\nanalysis for various use cases, 3) user-specific analysis on the individual\nlevel, and 4) with a trained model, users can be added and evaluated in real\ntime to study how avatar changes affect behavior.",
    "authors": [
      "Christian Merz",
      "Lukas Schach",
      "Marie Luisa Fiedler",
      "Jean-Luc Lugrin",
      "Carolin Wienrich",
      "Marc Erich Latoschik"
    ],
    "published": "2025-09-04T12:46:18Z",
    "updated": "2025-09-04T12:46:18Z",
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04174v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04174v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2411.14013v3",
    "title": "Exposing Synthetic Speech: Model Attribution and Detection of   AI-generated Speech via Audio Fingerprints",
    "summary": "As speech generation technologies continue to advance in quality and\naccessibility, the risk of malicious use cases, including impersonation,\nmisinformation, and spoofing, increases rapidly. This work addresses this\nthreat by introducing a simple, training-free, yet effective approach for\ndetecting AI-generated speech and attributing it to its source model.\nSpecifically, we tackle three key tasks: (1) single-model attribution in an\nopen-world setting, where the goal is to determine whether a given audio sample\nwas generated by a specific target neural speech synthesis system (with access\nonly to data from that system); (2) multi-model attribution in a closed-world\nsetting, where the objective is to identify the generating system from a known\npool of candidates; and last but not least (3) detection of synthetic versus\nreal speech. Our approach leverages standardized average residuals-the\ndifference between an input audio signal and its filtered version using either\na low-pass filter or the EnCodec audio autoencoder. We demonstrate that these\nresiduals consistently capture artifacts introduced by diverse speech synthesis\nsystems, serving as distinctive, model-agnostic fingerprints for attribution.\nAcross extensive experiments, our approach achieves AUROC scores exceeding 99%\nin most scenarios, evaluated on augmented benchmark datasets that pair real\nspeech with synthetic audio generated by multiple synthesis systems. In\naddition, our robustness analysis underscores the method's ability to maintain\nhigh performance even in the presence of moderate additive noise. Due to its\nsimplicity, efficiency, and strong generalization across speech synthesis\nsystems and languages, this technique offers a practical tool for digital\nforensics and security applications.",
    "authors": [
      "Matías Pizarro",
      "Mike Laszkiewicz",
      "Shawkat Hesso",
      "Dorothea Kolossa",
      "Asja Fischer"
    ],
    "published": "2024-11-21T10:55:49Z",
    "updated": "2025-09-04T12:43:52Z",
    "categories": [
      "eess.AS",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2411.14013v3",
    "arxiv_url": "http://arxiv.org/abs/2411.14013v3",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04128v1",
    "title": "Who Pays for Fairness? Rethinking Recourse under Social Burden",
    "summary": "Machine learning based predictions are increasingly used in sensitive\ndecision-making applications that directly affect our lives. This has led to\nextensive research into ensuring the fairness of classifiers. Beyond just fair\nclassification, emerging legislation now mandates that when a classifier\ndelivers a negative decision, it must also offer actionable steps an individual\ncan take to reverse that outcome. This concept is known as algorithmic\nrecourse. Nevertheless, many researchers have expressed concerns about the\nfairness guarantees within the recourse process itself. In this work, we\nprovide a holistic theoretical characterization of unfairness in algorithmic\nrecourse, formally linking fairness guarantees in recourse and classification,\nand highlighting limitations of the standard equal cost paradigm. We then\nintroduce a novel fairness framework based on social burden, along with a\npractical algorithm (MISOB), broadly applicable under real-world conditions.\nEmpirical results on real-world datasets show that MISOB reduces the social\nburden across all groups without compromising overall classifier accuracy.",
    "authors": [
      "Ainhize Barrainkua",
      "Giovanni De Toni",
      "Jose Antonio Lozano",
      "Novi Quadrianto"
    ],
    "published": "2025-09-04T11:53:42Z",
    "updated": "2025-09-04T11:53:42Z",
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04128v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04128v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04112v1",
    "title": "Synthetic Counterfactual Labels for Efficient Conformal Counterfactual   Inference",
    "summary": "This work addresses the problem of constructing reliable prediction intervals\nfor individual counterfactual outcomes. Existing conformal counterfactual\ninference (CCI) methods provide marginal coverage guarantees but often produce\noverly conservative intervals, particularly under treatment imbalance when\ncounterfactual samples are scarce. We introduce synthetic data-powered CCI\n(SP-CCI), a new framework that augments the calibration set with synthetic\ncounterfactual labels generated by a pre-trained counterfactual model. To\nensure validity, SP-CCI incorporates synthetic samples into a conformal\ncalibration procedure based on risk-controlling prediction sets (RCPS) with a\ndebiasing step informed by prediction-powered inference (PPI). We prove that\nSP-CCI achieves tighter prediction intervals while preserving marginal\ncoverage, with theoretical guarantees under both exact and approximate\nimportance weighting. Empirical results on different datasets confirm that\nSP-CCI consistently reduces interval width compared to standard CCI across all\nsettings.",
    "authors": [
      "Amirmohammad Farzaneh",
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ],
    "published": "2025-09-04T11:22:08Z",
    "updated": "2025-09-04T11:22:08Z",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04112v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04112v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04089v1",
    "title": "Gromov-Wasserstein and optimal transport: from assignment problems to   probabilistic numeric",
    "summary": "The assignment problem, a cornerstone of operations research, seeks an\noptimal one-to-one mapping between agents and tasks to minimize total cost.\nThis work traces its evolution from classical formulations and algorithms to\nmodern optimal transport (OT) theory, positioning the Quadratic Assignment\nProblem (QAP) and related structural matching tasks within this framework. We\nconnect the linear assignment problem to Monge's transport problem,\nKantorovich's relaxation, and Wasserstein distances, then extend to cases where\nsource and target lie in different metric-measure spaces requiring\nGromov-Wasserstein (GW) distances. GW formulations, including the fused GW\nvariant that integrates structural and feature information, naturally address\nQAP-like problems by optimizing alignment based on both intra-domain distances\nand cross-domain attributes. Applications include graph matching, keypoint\ncorrespondence, and feature-based assignments. We present exact solvers,\nGenetic Algorithms (GA), and multiple GW variants, including a proposed\nmulti-initialization strategy (GW-MultiInit) that mitigates the risk of getting\nstuck in local optima alongside entropic Sinkhorn-based approximations and\nfused GW. Computational experiments on capacitated QAP instances show that\nGW-MultiInit consistently achieves near-optimal solutions and scales\nefficiently to large problems where exact methods become impractical, while\nparameterized EGW and FGW variants provide flexible trade-offs between accuracy\nand runtime. Our findings provide theoretical foundations, computational\ninsights, and practical guidelines for applying OT and GW methods to QAP and\nother real-world matching problems, such as those in machine learning and\nlogistics.",
    "authors": [
      "Iman Seyedi",
      "Antonio Candelieri",
      "Enza Messina",
      "Francesco Archetti"
    ],
    "published": "2025-09-04T10:44:30Z",
    "updated": "2025-09-04T10:44:30Z",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2509.04089v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04089v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2508.04180v3",
    "title": "One Small Step with Fingerprints, One Giant Leap for De Novo Molecule   Generation from Mass Spectra",
    "summary": "A common approach to the de novo molecular generation problem from mass\nspectra involves a two-stage pipeline: (1) encoding mass spectra into molecular\nfingerprints, followed by (2) decoding these fingerprints into molecular\nstructures. In our work, we adopt MIST as the encoder and MolForge as the\ndecoder, leveraging additional training data to enhance performance. We also\nthreshold the probabilities of each fingerprint bit to focus on the presence of\nsubstructures. This results in a tenfold improvement over previous\nstate-of-the-art methods, generating top-1 28% / top-10 36% of molecular\nstructures correctly from mass spectra in MassSpecGym. We position this as a\nstrong baseline for future research in de novo molecule elucidation from mass\nspectra.",
    "authors": [
      "Neng Kai Nigel Neo",
      "Lim Jing",
      "Ngoui Yong Zhau Preston",
      "Koh Xue Ting Serene",
      "Bingquan Shen"
    ],
    "published": "2025-08-06T08:05:01Z",
    "updated": "2025-09-04T09:10:11Z",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2508.04180v3",
    "arxiv_url": "http://arxiv.org/abs/2508.04180v3",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04438v1",
    "title": "The Telephone Game: Evaluating Semantic Drift in Unified Models",
    "summary": "Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened\na new direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T, as consistency between understanding\nand generation is critical for downstream use. Existing evaluations consider\nthese capabilities in isolation: FID and GenEval for T2I, and benchmarks such\nas MME, MMBench for I2T. These single-pass metrics do not reveal whether a\nmodel that understands a concept can also render it, nor whether meaning is\npreserved when cycling between image and text modalities. To address this, we\nintroduce the Unified Consistency Framework for Unified Models (UCF-UM), a\ncyclic evaluation protocol that alternates I2T and T2I over multiple\ngenerations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean\nCumulative Drift (MCD), an embedding-based measure of overall semantic loss;\n(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)\nMulti-Generation GenEval (MGG), an object-level compliance score extending\nGenEval. To assess generalization beyond COCO, which is widely used in\ntraining; we create a new benchmark ND400, sampled from NoCaps and DOCCI and\nevaluate on seven recent models. UCF-UM reveals substantial variation in\ncross-modal stability: some models like BAGEL maintain semantics over many\nalternations, whereas others like Vila-u drift quickly despite strong\nsingle-pass scores. Our results highlight cyclic consistency as a necessary\ncomplement to standard I2T and T2I evaluations, and provide practical metrics\nto consistently assess unified model's cross-modal stability and strength of\ntheir shared representations. Code:\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models",
    "authors": [
      "Sabbir Mollah",
      "Rohit Gupta",
      "Sirnam Swetha",
      "Qingyang Liu",
      "Ahnaf Munir",
      "Mubarak Shah"
    ],
    "published": "2025-09-04T17:53:52Z",
    "updated": "2025-09-04T17:53:52Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04438v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04438v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04437v1",
    "title": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray   Collimators via Hough Transform",
    "summary": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.",
    "authors": [
      "Benjamin El-Zein",
      "Dominik Eckert",
      "Andreas Fieselmann",
      "Christopher Syben",
      "Ludwig Ritschl",
      "Steffen Kappler",
      "Sebastian Stober"
    ],
    "published": "2025-09-04T17:53:45Z",
    "updated": "2025-09-04T17:53:45Z",
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04437v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04437v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2502.08352v2",
    "title": "Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images   with Depth and Normal Supervision",
    "summary": "With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.",
    "authors": [
      "Tianle Liu",
      "Shuangming Zhao",
      "Wanshou Jiang",
      "Bingxuan Guo"
    ],
    "published": "2025-02-12T12:27:32Z",
    "updated": "2025-09-04T17:14:29Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2502.08352v2",
    "arxiv_url": "http://arxiv.org/abs/2502.08352v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2408.05750v2",
    "title": "FADE: A Dataset for Detecting Falling Objects around Buildings in Video",
    "summary": "Falling objects from buildings can cause severe injuries to pedestrians due\nto the great impact force they exert. Although surveillance cameras are\ninstalled around some buildings, it is challenging for humans to capture such\nevents in surveillance videos due to the small size and fast motion of falling\nobjects, as well as the complex background. Therefore, it is necessary to\ndevelop methods to automatically detect falling objects around buildings in\nsurveillance videos. To facilitate the investigation of falling object\ndetection, we propose a large, diverse video dataset called FADE (FAlling\nObject DEtection around Buildings) for the first time. FADE contains 1,881\nvideos from 18 scenes, featuring 8 falling object categories, 4 weather\nconditions, and 4 video resolutions. Additionally, we develop a new object\ndetection method called FADE-Net, which effectively leverages motion\ninformation and produces small-sized but high-quality proposals for detecting\nfalling objects around buildings. Importantly, our method is extensively\nevaluated and analyzed by comparing it with the previous approaches used for\ngeneric object detection, video object detection, and moving object detection\non the FADE dataset. Experimental results show that the proposed FADE-Net\nsignificantly outperforms other methods, providing an effective baseline for\nfuture research. The dataset and code are publicly available at\nhttps://fadedataset.github.io/FADE.github.io/.",
    "authors": [
      "Zhigang Tu",
      "Zitao Gao",
      "Zhengbo Zhang",
      "Chunluan Zhou",
      "Junsong Yuan",
      "Bo Du"
    ],
    "published": "2024-08-11T11:43:56Z",
    "updated": "2025-09-04T16:38:49Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2408.05750v2",
    "arxiv_url": "http://arxiv.org/abs/2408.05750v2",
    "comment": "Accepted by IEEE Transactions on Information Forensics and Security\n  (TIFS), 2025",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04351v1",
    "title": "Global-to-Local or Local-to-Global? Enhancing Image Retrieval with   Efficient Local Search and Effective Global Re-ranking",
    "summary": "The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.",
    "authors": [
      "Dror Aiger",
      "Bingyi Cao",
      "Kaifeng Chen",
      "Andre Araujo"
    ],
    "published": "2025-09-04T16:12:14Z",
    "updated": "2025-09-04T16:12:14Z",
    "categories": [
      "cs.IR",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04351v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04351v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04334v1",
    "title": "GeoArena: An Open Platform for Benchmarking Large Vision-language Models   on WorldWide Image Geolocalization",
    "summary": "Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.",
    "authors": [
      "Pengyue Jia",
      "Yingyi Zhang",
      "Xiangyu Zhao",
      "Yixuan Li"
    ],
    "published": "2025-09-04T15:52:04Z",
    "updated": "2025-09-04T15:52:04Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04334v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04334v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04298v1",
    "title": "Noisy Label Refinement with Semantically Reliable Synthetic Images",
    "summary": "Semantic noise in image classification datasets, where visually similar\ncategories are frequently mislabeled, poses a significant challenge to\nconventional supervised learning approaches. In this paper, we explore the\npotential of using synthetic images generated by advanced text-to-image models\nto address this issue. Although these high-quality synthetic images come with\nreliable labels, their direct application in training is limited by domain gaps\nand diversity constraints. Unlike conventional approaches, we propose a novel\nmethod that leverages synthetic images as reliable reference points to identify\nand correct mislabeled samples in noisy datasets. Extensive experiments across\nmultiple benchmark datasets show that our approach significantly improves\nclassification accuracy under various noise conditions, especially in\nchallenging scenarios with semantic label noise. Additionally, since our method\nis orthogonal to existing noise-robust learning techniques, when combined with\nstate-of-the-art noise-robust training methods, it achieves superior\nperformance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100\nunder 70% semantic noise, and by 24% on ImageNet-100 under real-world noise\nconditions.",
    "authors": [
      "Yingxuan Li",
      "Jiafeng Mao",
      "Yusuke Matsui"
    ],
    "published": "2025-09-04T15:13:29Z",
    "updated": "2025-09-04T15:13:29Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04298v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04298v1",
    "comment": "Accepted to ICIP2025",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04276v1",
    "title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images",
    "summary": "We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.",
    "authors": [
      "Jianning Deng",
      "Kartic Subr",
      "Hakan Bilen"
    ],
    "published": "2025-09-04T14:51:03Z",
    "updated": "2025-09-04T14:51:03Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04276v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04276v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04273v1",
    "title": "Dual-Scale Volume Priors with Wasserstein-Based Consistency for   Semi-Supervised Medical Image Segmentation",
    "summary": "Despite signi cant progress in semi-supervised medical image segmentation,\nmost existing segmentation networks overlook e ective methodological guidance\nfor feature extraction and important prior information from\n  datasets. In this paper, we develop a semi-supervised medical image\nsegmentation framework that e ectively integrates spatial regularization\nmethods and volume priors. Speci cally, our approach integrates a strong\nexplicit volume prior at the image scale and Threshold Dynamics spatial\nregularization, both derived from variational models, into the backbone\nsegmentation network. The target region volumes for each unlabeled image are\nestimated by a regression network, which e ectively regularizes the backbone\nsegmentation network through an image-scale Wasserstein distance constraint,\nensuring that the class ratios in the segmentation results for each unlabeled\nimage match those predicted by the regression network. Additionally, we design\na dataset-scale Wasserstein distance loss function based on a weak implicit\nvolume prior, which enforces that the volume distribution predicted for the\nunlabeled dataset is similar to that of labeled dataset. Experimental results\non the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset\nshow the superiority of the proposed method.",
    "authors": [
      "Junying Meng",
      "Gangxuan Zhou",
      "Jun Liu",
      "Weihong Guo"
    ],
    "published": "2025-09-04T14:47:25Z",
    "updated": "2025-09-04T14:47:25Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04273v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04273v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04150v1",
    "title": "Revisiting Simple Baselines for In-The-Wild Deepfake Detection",
    "summary": "The widespread adoption of synthetic media demands accessible deepfake\ndetectors and realistic benchmarks. While most existing research evaluates\ndeepfake detectors on highly controlled datasets, we focus on the recently\nreleased \"in-the-wild\" benchmark, Deepfake-Eval-2024. Initial reporting on\nDeepfake-Eval-2024 showed that three finetuned open-source models achieve\naccuracies between 61% and 69%, significantly lagging behind the leading\ncommercial deepfake detector with 82% accuracy. Our work revisits one of these\nbaseline approaches, originally introduced by Ojha et al., which adapts\nstandard pretrained vision backbones to produce generalizable deepfake\ndetectors. We demonstrate that with better-tuned hyperparameters, this simple\napproach actually yields much higher performance -- 81% accuracy on\nDeepfake-Eval-2024 -- surpassing the previously reported accuracy of this\nbaseline approach by 18% and competing with commercial deepfake detectors. We\ndiscuss tradeoffs in accuracy, computational costs, and interpretability,\nfocusing on how practical these deepfake detectors might be when deployed in\nreal-world settings. Our code can be found at\nhttps://github.com/Deepfake-Detection-KKO/deepfake-detection.",
    "authors": [
      "Orlando Castaneda",
      "Kevin So-Tang",
      "Kshitij Gurung"
    ],
    "published": "2025-09-04T12:23:59Z",
    "updated": "2025-09-04T12:23:59Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04150v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04150v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2405.06911v2",
    "title": "Replication Study and Benchmarking of Real-Time Object Detection Models",
    "summary": "This work examines the reproducibility and benchmarking of state-of-the-art\nreal-time object detection models. As object detection models are often used in\nreal-world contexts, such as robotics, where inference time is paramount,\nsimply measuring models' accuracy is not enough to compare them. We thus\ncompare a large variety of object detection models' accuracy and inference\nspeed on multiple graphics cards. In addition to this large benchmarking\nattempt, we also reproduce the following models from scratch using PyTorch on\nthe MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we\npropose a unified training and evaluation pipeline, based on MMDetection's\nfeatures, to better compare models. Our implementation of DETR and ViTDet could\nnot achieve accuracy or speed performances comparable to what is declared in\nthe original papers. On the other hand, reproduced RTMDet and YOLOv7 could\nmatch such performances. Studied papers are also found to be generally lacking\nfor reproducibility purposes. As for MMDetection pretrained models, speed\nperformances are severely reduced with limited computing resources (larger,\nmore accurate models even more so). Moreover, results exhibit a strong\ntrade-off between accuracy and speed, prevailed by anchor-free models - notably\nRTMDet or YOLOx models. The code used is this paper and all the experiments is\navailable in the repository at https://github.com/willGuimont/segdet_mlcr2024.",
    "authors": [
      "Pierre-Luc Asselin",
      "Vincent Coulombe",
      "William Guimont-Martin",
      "William Larrivée-Hardy"
    ],
    "published": "2024-05-11T04:47:50Z",
    "updated": "2025-09-04T12:21:13Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2405.06911v2",
    "arxiv_url": "http://arxiv.org/abs/2405.06911v2",
    "comment": "Authors are presented in alphabetical order, each having equal\n  contribution to the work",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04092v1",
    "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception",
    "summary": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.",
    "authors": [
      "Quang-Huy Che",
      "Duc-Khai Lam"
    ],
    "published": "2025-09-04T10:48:25Z",
    "updated": "2025-09-04T10:48:25Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04092v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04092v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2405.20188v2",
    "title": "SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid   Registration",
    "summary": "Existing optimization-based methods for non-rigid registration typically\nminimize an alignment error metric based on the point-to-point or\npoint-to-plane distance between corresponding point pairs on the source surface\nand target surface. However, these metrics can result in slow convergence or a\nloss of detail. In this paper, we propose SPARE, a novel formulation that\nutilizes a symmetrized point-to-plane distance for robust non-rigid\nregistration. The symmetrized point-to-plane distance relies on both the\npositions and normals of the corresponding points, resulting in a more accurate\napproximation of the underlying geometry and can achieve higher accuracy than\nexisting methods. To solve this optimization problem efficiently, we introduce\nan as-rigid-as-possible regulation term to estimate the deformed normals and\npropose an alternating minimization solver using a majorization-minimization\nstrategy. Moreover, for effective initialization of the solver, we incorporate\na deformation graph-based coarse alignment that improves registration quality\nand efficiency. Extensive experiments show that the proposed method greatly\nimproves the accuracy of non-rigid registration problems and maintains\nrelatively high solution efficiency. The code is publicly available at\nhttps://github.com/yaoyx689/spare.",
    "authors": [
      "Yuxin Yao",
      "Bailin Deng",
      "Junhui Hou",
      "Juyong Zhang"
    ],
    "published": "2024-05-30T15:55:04Z",
    "updated": "2025-09-04T10:02:22Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2405.20188v2",
    "arxiv_url": "http://arxiv.org/abs/2405.20188v2",
    "comment": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04043v1",
    "title": "Millisecond-Response Tracking and Gazing System for UAVs: A Domestic   Solution Based on \"Phytium + Cambricon\"",
    "summary": "In the frontier research and application of current video surveillance\ntechnology, traditional camera systems exhibit significant limitations of\nresponse delay exceeding 200 ms in dynamic scenarios due to the insufficient\ndeep feature extraction capability of automatic recognition algorithms and the\nefficiency bottleneck of computing architectures, failing to meet the real-time\nrequirements in complex scenes. To address this issue, this study proposes a\nheterogeneous computing architecture based on Phytium processors and Cambricon\naccelerator cards, constructing a UAV tracking and gazing system with\nmillisecond-level response capability. At the hardware level, the system adopts\na collaborative computing architecture of Phytium FT-2000/4 processors and\nMLU220 accelerator cards, enhancing computing power through multi-card\nparallelism. At the software level, it innovatively integrates a lightweight\nYOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming\na closed-loop control chain of \"detection-tracking-feedback\". Experimental\nresults demonstrate that the system achieves a stable single-frame\ncomprehensive processing delay of 50-100 ms in 1920*1080 resolution video\nstream processing, with a multi-scale target recognition accuracy of over\n98.5%, featuring both low latency and high precision. This study provides an\ninnovative solution for UAV monitoring and the application of domestic chips.",
    "authors": [
      "Yuchen Zhu",
      "Longxiang Yin",
      "Kai Zhao"
    ],
    "published": "2025-09-04T09:26:00Z",
    "updated": "2025-09-04T09:26:00Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04043v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04043v1",
    "comment": "16 pages,17 figures",
    "relevance_score": 0.5
  },
  {
    "id": "2508.17832v2",
    "title": "HLG: Comprehensive 3D Room Construction via Hierarchical Layout   Generation",
    "summary": "Realistic 3D indoor scene generation is crucial for virtual reality, interior\ndesign, embodied intelligence, and scene understanding. While existing methods\nhave made progress in coarse-scale furniture arrangement, they struggle to\ncapture fine-grained object placements, limiting the realism and utility of\ngenerated environments. This gap hinders immersive virtual experiences and\ndetailed scene comprehension for embodied AI applications. To address these\nissues, we propose Hierarchical Layout Generation (HLG), a novel method for\nfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine\nhierarchical approach, refining scene layouts from large-scale furniture\nplacement to intricate object arrangements. Specifically, our fine-grained\nlayout alignment module constructs a hierarchical layout through vertical and\nhorizontal decoupling, effectively decomposing complex 3D indoor scenes into\nmultiple levels of granularity. Additionally, our trainable layout optimization\nnetwork addresses placement issues, such as incorrect positioning, orientation\nerrors, and object intersections, ensuring structurally coherent and physically\nplausible scene generation. We demonstrate the effectiveness of our approach\nthrough extensive experiments, showing superior performance in generating\nrealistic indoor scenes compared to existing methods. This work advances the\nfield of scene generation and opens new possibilities for applications\nrequiring detailed 3D environments. We will release our code upon publication\nto encourage future research.",
    "authors": [
      "Xiping Wang",
      "Yuxi Wang",
      "Mengqi Zhou",
      "Junsong Fan",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-08-25T09:32:57Z",
    "updated": "2025-09-04T09:06:17Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2508.17832v2",
    "arxiv_url": "http://arxiv.org/abs/2508.17832v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.04023v1",
    "title": "Learning from Majority Label: A Novel Problem in Multi-class   Multiple-Instance Learning",
    "summary": "The paper proposes a novel multi-class Multiple-Instance Learning (MIL)\nproblem called Learning from Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag-level label. The goal of LML is to\ntrain a classification model that estimates the class of each instance using\nthe majority label. This problem is valuable in a variety of applications,\nincluding pathology image segmentation, political voting prediction, customer\nsentiment analysis, and environmental monitoring. To solve LML, we propose a\nCounting Network trained to produce bag-level majority labels, estimated by\ncounting the number of instances in each class. Furthermore, analysis\nexperiments on the characteristics of LML revealed that bags with a high\nproportion of the majority class facilitate learning. Based on this result, we\ndeveloped a Majority Proportion Enhancement Module (MPEM) that increases the\nproportion of the majority class by removing minority class instances within\nthe bags. Experiments demonstrate the superiority of the proposed method on\nfour datasets compared to conventional MIL methods. Moreover, ablation studies\nconfirmed the effectiveness of each module. The code is available at\n\\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.",
    "authors": [
      "Shiku Kaito",
      "Shinnosuke Matsuo",
      "Daiki Suehiro",
      "Ryoma Bise"
    ],
    "published": "2025-09-04T08:50:03Z",
    "updated": "2025-09-04T08:50:03Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.04023v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04023v1",
    "comment": "35 pages, 9 figures, Accepted in Pattern recognition",
    "relevance_score": 0.5
  },
  {
    "id": "2405.20321v2",
    "title": "Vision-based Manipulation from Single Human Video with Open-World Object   Graphs",
    "summary": "This work presents an object-centric approach to learning vision-based\nmanipulation skills from human videos. We investigate the problem of robot\nmanipulation via imitation in the open-world setting, where a robot learns to\nmanipulate novel objects from a single video demonstration. We introduce ORION,\nan algorithm that tackles the problem by extracting an object-centric\nmanipulation plan from a single RGB or RGB-D video and deriving a policy that\nconditions on the extracted plan. Our method enables the robot to learn from\nvideos captured by daily mobile devices and to generalize the policies to\ndeployment environments with varying visual backgrounds, camera angles, spatial\nlayouts, and novel object instances. We systematically evaluate our method on\nboth short-horizon and long-horizon tasks, using RGB-D and RGB-only\ndemonstration videos. Across varied tasks and demonstration types (RGB-D /\nRGB), we observe an average success rate of 74.4%, demonstrating the efficacy\nof ORION in learning from a single human video in the open world. Additional\nmaterials can be found on our project website:\nhttps://ut-austin-rpl.github.io/ORION-release.",
    "authors": [
      "Yifeng Zhu",
      "Arisrei Lim",
      "Peter Stone",
      "Yuke Zhu"
    ],
    "published": "2024-05-30T17:56:54Z",
    "updated": "2025-09-04T08:23:37Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2405.20321v2",
    "arxiv_url": "http://arxiv.org/abs/2405.20321v2",
    "comment": "Extended version of paper adding results with RGB-only demonstration\n  videos uploaded on 09/04/2025",
    "relevance_score": 0.5
  },
  {
    "id": "2509.03975v1",
    "title": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary   Data Available Only During Model Training",
    "summary": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.",
    "authors": [
      "Daniel Sobotka",
      "Alexander Herold",
      "Matthias Perkonigg",
      "Lucian Beer",
      "Nina Bastati",
      "Alina Sablatnig",
      "Ahmed Ba-Ssalamah",
      "Georg Langs"
    ],
    "published": "2025-09-04T08:01:27Z",
    "updated": "2025-09-04T08:01:27Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03975v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03975v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.03950v1",
    "title": "Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer   Learning in a U-Net Architecture",
    "summary": "Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.",
    "authors": [
      "Alvaro Aranibar Roque",
      "Helga Sebastian"
    ],
    "published": "2025-09-04T07:21:37Z",
    "updated": "2025-09-04T07:21:37Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03950v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03950v1",
    "comment": "10 page, 5 figures",
    "relevance_score": 0.5
  },
  {
    "id": "2509.03938v1",
    "title": "TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained   Tubular Shapes",
    "summary": "Medical tubular anatomical structures are inherently three-dimensional\nconduits with lumens, enclosing walls, and complex branching topologies.\nAccurate reconstruction of their geometry and topology is crucial for\napplications such as bronchoscopic navigation and cerebral arterial\nconnectivity assessment. Existing methods often rely on voxel-wise overlap\nmeasures, which fail to capture topological correctness and completeness.\nAlthough topology-aware losses and persistent homology constraints have shown\npromise, they are usually applied patch-wise and cannot guarantee global\npreservation or correct geometric errors at inference. To address these\nlimitations, we propose a novel TopoSculpt, a framework for topological\nrefinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a\nholistic whole-region modeling strategy to capture full spatial context, (ii)\nfirst introduces a Topological Integrity Betti (TIB) constraint that jointly\nenforces Betti number priors and global integrity, and (iii) employs a\ncurriculum refinement scheme with persistent homology to progressively correct\nerrors from coarse to fine scales. Extensive experiments on challenging\npulmonary airway and Circle of Willis datasets demonstrate substantial\nimprovements in both geometry and topology. For instance, $\\beta_{0}$ errors\nare reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on\nthe CoW dataset, with Tree length detected and branch detected rates improving\nby nearly 10\\%. These results highlight the effectiveness of TopoSculpt in\ncorrecting critical topological errors and advancing the high-fidelity modeling\nof complex 3D tubular anatomy. The project homepage is available at:\nhttps://github.com/Puzzled-Hui/TopoSculpt.",
    "authors": [
      "Minghui Zhang",
      "Yaoyu Liu",
      "Junyang Wu",
      "Xin You",
      "Hanxiao Zhang",
      "Junjun He",
      "Yun Gu"
    ],
    "published": "2025-09-04T06:56:06Z",
    "updated": "2025-09-04T06:56:06Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03938v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03938v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2502.07107v2",
    "title": "A Framework for Supervised and Unsupervised Segmentation and   Classification of Materials Microstructure Images",
    "summary": "Microstructure of materials is often characterized through image analysis to\nunderstand processing-structure-properties linkages. We propose a largely\nautomated framework that integrates unsupervised and supervised learning\nmethods to classify micrographs according to microstructure phase/class and,\nfor multiphase microstructures, segments them into different homogeneous\nregions. With the advance of manufacturing and imaging techniques, the\nultra-high resolution of imaging that reveals the complexity of microstructures\nand the rapidly increasing quantity of images (i.e., micrographs) enables and\nnecessitates a more powerful and automated framework to extract materials\ncharacteristics and knowledge. The framework we propose can be used to\ngradually build a database of microstructure classes relevant to a particular\nprocess or group of materials, which can help in analyzing and\ndiscovering/identifying new materials. The framework has three steps: (1)\nsegmentation of multiphase micrographs through a recently developed score-based\nmethod so that different microstructure homogeneous regions can be identified\nin an unsupervised manner; (2) {identification and classification of}\nhomogeneous regions of micrographs through an uncertainty-aware supervised\nclassification network trained using the segmented micrographs from Step $1$\nwith their identified labels verified via the built-in uncertainty\nquantification and minimal human inspection; (3) supervised segmentation (more\npowerful than the segmentation in Step $1$) of multiphase microstructures\nthrough a segmentation network trained with micrographs and the results from\nSteps $1$-$2$ using a form of data augmentation. This framework can iteratively\ncharacterize/segment new homogeneous or multiphase materials while expanding\nthe database to enhance performance. The framework is demonstrated on various\nsets of materials and texture images.",
    "authors": [
      "Kungang Zhang",
      "Wei Chen",
      "Wing K. Liu",
      "L. Catherine Brinson",
      "Daniel W. Apley"
    ],
    "published": "2025-02-10T23:05:35Z",
    "updated": "2025-09-04T06:28:55Z",
    "categories": [
      "stat.AP",
      "stat.ML",
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2502.07107v2",
    "arxiv_url": "http://arxiv.org/abs/2502.07107v2",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2509.03893v1",
    "title": "Weakly-Supervised Learning of Dense Functional Correspondences",
    "summary": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.",
    "authors": [
      "Stefan Stojanov",
      "Linan Zhao",
      "Yunzhi Zhang",
      "Daniel L. K. Yamins",
      "Jiajun Wu"
    ],
    "published": "2025-09-04T05:39:16Z",
    "updated": "2025-09-04T05:39:16Z",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2509.03893v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03893v1",
    "comment": "Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04432v1",
    "title": "Can Language Models Handle a Non-Gregorian Calendar?",
    "summary": "Temporal reasoning and knowledge are essential capabilities for language\nmodels (LMs). While much prior work has analyzed and improved temporal\nreasoning in LMs, most studies have focused solely on the Gregorian calendar.\nHowever, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew\ncalendars, are in active use and reflect culturally grounded conceptions of\ntime. If and how well current LMs can accurately handle such non-Gregorian\ncalendars has not been evaluated so far. Here, we present a systematic\nevaluation of how well open-source LMs handle one such non-Gregorian system:\nthe Japanese calendar. For our evaluation, we create datasets for four tasks\nthat require both temporal knowledge and temporal reasoning. Evaluating a range\nof English-centric and Japanese-centric LMs, we find that some models can\nperform calendar conversions, but even Japanese-centric models struggle with\nJapanese-calendar arithmetic and with maintaining consistency across calendars.\nOur results highlight the importance of developing LMs that are better equipped\nfor culture-specific calendar understanding.",
    "authors": [
      "Mutsumi Sasaki",
      "Go Kamoda",
      "Ryosuke Takahashi",
      "Kosuke Sato",
      "Kentaro Inui",
      "Keisuke Sakaguchi",
      "Benjamin Heinzerling"
    ],
    "published": "2025-09-04T17:52:00Z",
    "updated": "2025-09-04T17:52:00Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2509.04432v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04432v1",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2503.05720v3",
    "title": "That is Unacceptable: the Moral Foundations of Canceling",
    "summary": "Canceling is a morally-driven phenomenon that hinders the development of safe\nsocial media platforms and contributes to ideological polarization. To address\nthis issue we present the Canceling Attitudes Detection (CADE) dataset, an\nannotated corpus of canceling incidents aimed at exploring the factors of\ndisagreements in evaluating people canceling attitudes on social media.\nSpecifically, we study the impact of annotators' morality in their perception\nof canceling, showing that morality is an independent axis for the explanation\nof disagreement on this phenomenon. Annotator's judgments heavily depend on the\ntype of controversial events and involved celebrities. This shows the need to\ndevelop more event-centric datasets to better understand how harms are\nperpetrated in social media and to develop more aware technologies for their\ndetection.",
    "authors": [
      "Soda Marem Lo",
      "Oscar Araque",
      "Rajesh Sharma",
      "Marco Antonio Stranisci"
    ],
    "published": "2025-02-17T13:01:06Z",
    "updated": "2025-09-04T12:08:46Z",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2503.05720v3",
    "arxiv_url": "http://arxiv.org/abs/2503.05720v3",
    "comment": null,
    "relevance_score": 0.5
  },
  {
    "id": "2109.02325v5",
    "title": "MyProfessors: Mining Turkish Student Reviews",
    "summary": "We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.",
    "authors": [
      "Ibrahim Faruk Ceylan",
      "Necmettin Bera Calik"
    ],
    "published": "2021-09-06T09:55:58Z",
    "updated": "2025-09-04T11:02:32Z",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2109.02325v5",
    "arxiv_url": "http://arxiv.org/abs/2109.02325v5",
    "comment": "The paper is withdrawn due to the scraping errors in the dataset\n  collection process and affected results",
    "relevance_score": 0.5
  },
  {
    "id": "2509.04315v1",
    "title": "We Have It Covered: A Resampling-based Method for Uplift Model   Comparison",
    "summary": "Uplift models play a critical role in modern marketing applications to help\nunderstand the incremental benefits of interventions and identify optimal\ntargeting strategies. A variety of techniques exist for building uplift models,\nand it is essential to understand the model differences in the context of\nintended applications. The uplift curve is a widely adopted tool for assessing\nuplift model performance on the selection universe when observations are\navailable for the entire population. However, when it is uneconomical or\ninfeasible to select the entire population, it becomes difficult or even\nimpossible to estimate the uplift curve without appropriate sampling design. To\nthe best of our knowledge, no prior work has addressed uncertainty\nquantification of uplift curve estimates, which is essential for model\ncomparisons. We propose a two-step sampling procedure and a resampling-based\napproach to compare uplift models with uncertainty quantification, examine the\nproposed method via simulations and real data applications, and conclude with a\ndiscussion.",
    "authors": [
      "Yang Liu",
      "Chaoyu Yuan"
    ],
    "published": "2025-09-04T15:33:25Z",
    "updated": "2025-09-04T15:33:25Z",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.04315v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04315v1",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.04225v1",
    "title": "Sharp Convergence Rates of Empirical Unbalanced Optimal Transport for   Spatio-Temporal Point Processes",
    "summary": "We statistically analyze empirical plug-in estimators for unbalanced optimal\ntransport (UOT) formalisms, focusing on the Kantorovich-Rubinstein distance,\nbetween general intensity measures based on observations from spatio-temporal\npoint processes. Specifically, we model the observations by two weakly\ntime-stationary point processes with spatial intensity measures $\\mu$ and $\\nu$\nover the expanding window $(0,t]$ as $t$ increases to infinity, and establish\nsharp convergence rates of the empirical UOT in terms of the intrinsic\ndimensions of the measures. We assume a sub-quadratic temporal growth condition\nof the variance of the process, which allows for a wide range of temporal\ndependencies. As the growth approaches quadratic, the convergence rate becomes\nslower. This variance assumption is related to the time-reduced factorial\ncovariance measure, and we exemplify its validity for various point processes,\nincluding the Poisson cluster, Hawkes, Neyman-Scott, and log-Gaussian Cox\nprocesses. Complementary to our upper bounds, we also derive matching lower\nbounds for various spatio-temporal point processes of interest and establish\nnear minimax rate optimality of the empirical Kantorovich-Rubinstein distance.",
    "authors": [
      "Marina Struleva",
      "Shayan Hundrieser",
      "Dominic Schuhmacher",
      "Axel Munk"
    ],
    "published": "2025-09-04T13:55:01Z",
    "updated": "2025-09-04T13:55:01Z",
    "categories": [
      "math.ST",
      "stat.ML",
      "stat.TH",
      "primary 62G05, 62G07, 62R20, secondary: 60D05, 60G60"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.04225v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04225v1",
    "comment": "The first two authors contributed equally, 76 pages, 7 figures",
    "relevance_score": 0.0
  },
  {
    "id": "2312.05993v2",
    "title": "FastPart: Over-Parameterized Stochastic Gradient Descent for Sparse   optimisation on Measures",
    "summary": "This paper presents a novel algorithm that leverages Stochastic Gradient\nDescent strategies in conjunction with Random Features to augment the\nscalability of Conic Particle Gradient Descent (CPGD) specifically tailored for\nsolving sparse optimization problems on measures. By formulating the CPGD steps\nwithin a variational framework, we provide rigorous mathematical proofs\ndemonstrating the following key findings: $\\mathrm{(i)}$ The total variation\nnorms of the solution measures along the descent trajectory remain bounded,\nensuring stability and preventing undesirable divergence; $\\mathrm{(ii)}$ We\nestablish a global convergence guarantee with a convergence rate of\n${O}(\\log(K)/\\sqrt{K})$ over $K$ iterations, showcasing the efficiency and\neffectiveness of our algorithm, $\\mathrm{(iii)}$ Additionally, we analyse and\nestablish local control over the first-order condition discrepancy,\ncontributing to a deeper understanding of the algorithm's behaviour and\nreliability in practical applications.",
    "authors": [
      "Yohann De Castro",
      "Sébastien Gadat",
      "Clément Marteau"
    ],
    "published": "2023-12-10T20:41:43Z",
    "updated": "2025-09-04T08:42:55Z",
    "categories": [
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2312.05993v2",
    "arxiv_url": "http://arxiv.org/abs/2312.05993v2",
    "comment": "45 pages, 4 figures",
    "relevance_score": 0.0
  },
  {
    "id": "2509.03945v1",
    "title": "Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for   Differential Equations",
    "summary": "We introduce Prob-GParareal, a probabilistic extension of the GParareal\nalgorithm designed to provide uncertainty quantification for the\nParallel-in-Time (PinT) solution of (ordinary and partial) differential\nequations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model\nthe Parareal correction function, as GParareal does, further enabling the\npropagation of numerical uncertainty across time and yielding probabilistic\nforecasts of system's evolution. Furthermore, Prob-GParareal accommodates\nprobabilistic initial conditions and maintains compatibility with classical\nnumerical solvers, ensuring its straightforward integration into existing\nParareal frameworks. Here, we first conduct a theoretical analysis of the\ncomputational complexity and derive error bounds of Prob-GParareal. Then, we\nnumerically demonstrate the accuracy and robustness of the proposed algorithm\non five benchmark ODE systems, including chaotic, stiff, and bifurcation\nproblems. To showcase the flexibility and potential scalability of the proposed\nalgorithm, we also consider Prob-nnGParareal, a variant obtained by replacing\nthe GPs in Parareal with the nearest-neighbors GPs, illustrating its increased\nperformance on an additional PDE example. This work bridges a critical gap in\nthe development of probabilistic counterparts to established PinT methods.",
    "authors": [
      "Guglielmo Gattiglio",
      "Lyudmila Grigoryeva",
      "Massimiliano Tamborrino"
    ],
    "published": "2025-09-04T07:09:59Z",
    "updated": "2025-09-04T07:09:59Z",
    "categories": [
      "stat.CO",
      "cs.DC",
      "cs.NA",
      "math.NA",
      "stat.ML",
      "65M55, 65M22, 65L05, 50G15, 65Y05"
    ],
    "primary_category": "stat.ML",
    "pdf_url": "http://arxiv.org/pdf/2509.03945v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03945v1",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.04443v1",
    "title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data",
    "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.",
    "authors": [
      "Lawrence Y. Zhu",
      "Pranav Kuppili",
      "Ryan Punamiya",
      "Patcharapong Aphiwetsa",
      "Dhruv Patel",
      "Simar Kareer",
      "Sehoon Ha",
      "Danfei Xu"
    ],
    "published": "2025-09-04T17:59:10Z",
    "updated": "2025-09-04T17:59:10Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04443v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04443v1",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2504.03157v2",
    "title": "Taming High-Dimensional Dynamics: Learning Optimal Projections onto   Spectral Submanifolds",
    "summary": "High-dimensional nonlinear systems pose considerable challenges for modeling\nand control across many domains, from fluid mechanics to advanced robotics.\nSuch systems are typically approximated with reduced-order models, which often\nrely on orthogonal projections, a simplification that may lead to large\nprediction errors. In this work, we derive optimality of fiber-aligned\nprojections onto spectral submanifolds, preserving the nonlinear geometric\nstructure and minimizing long-term prediction error. We propose a data-driven\nprocedure to learn these projections from trajectories and demonstrate its\neffectiveness through a 180-dimensional robotic system. Our reduced-order\nmodels achieve up to fivefold improvement in trajectory tracking accuracy under\nmodel predictive control compared to the state of the art.",
    "authors": [
      "Hugo Buurmeijer",
      "Luis A. Pabon",
      "John Irvin Alora",
      "Roshan S. Kaundinya",
      "George Haller",
      "Marco Pavone"
    ],
    "published": "2025-04-04T04:30:55Z",
    "updated": "2025-09-04T17:21:58Z",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2504.03157v2",
    "arxiv_url": "http://arxiv.org/abs/2504.03157v2",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.04399v1",
    "title": "Leveraging Equivariances and Symmetries in the Control Barrier Function   Synthesis",
    "summary": "The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis.",
    "authors": [
      "Adrian Wiltz",
      "Dimos V. Dimarogonas"
    ],
    "published": "2025-09-04T17:10:15Z",
    "updated": "2025-09-04T17:10:15Z",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04399v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04399v1",
    "comment": "15 pages",
    "relevance_score": 0.0
  },
  {
    "id": "2508.17449v2",
    "title": "Robotic Manipulation via Imitation Learning: Taxonomy, Evolution,   Benchmark, and Challenges",
    "summary": "Robotic Manipulation (RM) is central to the advancement of autonomous robots,\nenabling them to interact with and manipulate objects in real-world\nenvironments. This survey focuses on RM methodologies that leverage imitation\nlearning, a powerful technique that allows robots to learn complex manipulation\nskills by mimicking human demonstrations. We identify and analyze the most\ninfluential studies in this domain, selected based on community impact and\nintrinsic quality. For each paper, we provide a structured summary, covering\nthe research purpose, technical implementation, hierarchical classification,\ninput formats, key priors, strengths and limitations, and citation metrics.\nAdditionally, we trace the chronological development of imitation learning\ntechniques within RM policy (RMP), offering a timeline of key technological\nadvancements. Where available, we report benchmark results and perform\nquantitative evaluations to compare existing methods. By synthesizing these\ninsights, this review provides a comprehensive resource for researchers and\npractitioners, highlighting both the state of the art and the challenges that\nlie ahead in the field of robotic manipulation through imitation learning.",
    "authors": [
      "Zezeng Li",
      "Alexandre Chapin",
      "Enda Xiang",
      "Rui Yang",
      "Bruno Machado",
      "Na Lei",
      "Emmanuel Dellandrea",
      "Di Huang",
      "Liming Chen"
    ],
    "published": "2025-08-24T17:01:15Z",
    "updated": "2025-09-04T16:46:34Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2508.17449v2",
    "arxiv_url": "http://arxiv.org/abs/2508.17449v2",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.04358v1",
    "title": "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the   Roles of Information Transparency, User Control, and Proactivity",
    "summary": "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.",
    "authors": [
      "Atikkhan Faridkhan Nilgar",
      "Manuel Dietrich",
      "Kristof Van Laerhoven"
    ],
    "published": "2025-09-04T16:19:24Z",
    "updated": "2025-09-04T16:19:24Z",
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04358v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04358v1",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.04220v1",
    "title": "Compatibility of Multiple Control Barrier Functions for Constrained   Nonlinear Systems",
    "summary": "Control barrier functions (CBFs) are a powerful tool for the constrained\ncontrol of nonlinear systems; however, the majority of results in the\nliterature focus on systems subject to a single CBF constraint, making it\nchallenging to synthesize provably safe controllers that handle multiple state\nconstraints. This paper presents a framework for constrained control of\nnonlinear systems subject to box constraints on the systems' vector-valued\noutputs using multiple CBFs. Our results illustrate that when the output has a\nvector relative degree, the CBF constraints encoding these box constraints are\ncompatible, and the resulting optimization-based controller is locally\nLipschitz continuous and admits a closed-form expression. Additional results\nare presented to characterize the degradation of nominal tracking objectives in\nthe presence of safety constraints. Simulations of a planar quadrotor are\npresented to demonstrate the efficacy of the proposed framework.",
    "authors": [
      "Max H. Cohen",
      "Eugene Lavretsky",
      "Aaron D. Ames"
    ],
    "published": "2025-09-04T13:52:11Z",
    "updated": "2025-09-04T13:52:11Z",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04220v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04220v1",
    "comment": "To appear at IEEE CDC 2025",
    "relevance_score": 0.0
  },
  {
    "id": "2509.04119v1",
    "title": "Lightweight Kinematic and Static Modeling of Cable-Driven Continuum   Robots via Actuation-Space Energy Formulation",
    "summary": "Continuum robots, inspired by octopus arms and elephant trunks, combine\ndexterity with intrinsic compliance, making them well suited for unstructured\nand confined environments. Yet their continuously deformable morphology poses\nchallenges for motion planning and control, calling for accurate but\nlightweight models. We propose the Lightweight Actuation Space Energy Modeling\n(LASEM) framework for cable driven continuum robots, which formulates actuation\npotential energy directly in actuation space. LASEM yields an analytical\nforward model derived from geometrically nonlinear beam and rod theories via\nHamilton's principle, while avoiding explicit modeling of cable backbone\ncontact. It accepts both force and displacement inputs, thereby unifying\nkinematic and static formulations. Assuming the friction is neglected, the\nframework generalizes to nonuniform geometries, arbitrary cable routings,\ndistributed loading and axial extensibility, while remaining computationally\nefficient for real-time use. Numerical simulations validate its accuracy, and a\nsemi-analytical iterative scheme is developed for inverse kinematics. To\naddress discretization in practical robots, LASEM further reformulates the\nfunctional minimization as a numerical optimization, which also naturally\nincorporates cable potential energy without explicit contact modeling.",
    "authors": [
      "Ke Wu",
      "Yuhao Wang",
      "Kevin Henry",
      "Cesare Stefanini",
      "Gang Zheng"
    ],
    "published": "2025-09-04T11:33:53Z",
    "updated": "2025-09-04T11:33:53Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04119v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04119v1",
    "comment": "Journal",
    "relevance_score": 0.0
  },
  {
    "id": "2509.04095v1",
    "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to   Proof-of-Concept Implementation",
    "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.",
    "authors": [
      "Achilleas Santi Seisa",
      "Viswa Narayanan Sankaranarayanan",
      "Gerasimos Damigos",
      "Sumeet Gajanan Satpute",
      "George Nikolakopoulos"
    ],
    "published": "2025-09-04T10:53:27Z",
    "updated": "2025-09-04T10:53:27Z",
    "categories": [
      "cs.RO",
      "cs.DC"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04095v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04095v1",
    "comment": "6 pages, 7 figures, CCGridW 2025",
    "relevance_score": 0.0
  },
  {
    "id": "2509.04094v1",
    "title": "Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators",
    "summary": "Object reconstruction and inspection tasks play a crucial role in various\nrobotics applications. Identifying paths that reveal the most unknown areas of\nthe object becomes paramount in this context, as it directly affects\nefficiency, and this problem is known as the view path planning problem.\nCurrent methods often use sampling-based path planning techniques, evaluating\npotential views along the path to enhance reconstruction performance. However,\nthese methods are computationally expensive as they require evaluating several\ncandidate views on the path. To this end, we propose a computationally\nefficient solution that relies on calculating a focus point in the most\ninformative (unknown) region and having the robot maintain this point in the\ncamera field of view along the path. We incorporated this strategy into the\nwhole-body control of a mobile manipulator employing a visibility constraint\nwithout the need for an additional path planner. We conducted comprehensive and\nrealistic simulations using a large dataset of 114 diverse objects of varying\nsizes from 57 categories to compare our method with a sampling-based planning\nstrategy using Bayesian data analysis. Furthermore, we performed real-world\nexperiments with an 8-DoF mobile manipulator to demonstrate the proposed\nmethod's performance in practice. Our results suggest that there is no\nsignificant difference in object coverage and entropy. In contrast, our method\nis approximately nine times faster than the baseline sampling-based method in\nterms of the average time the robot spends between views.",
    "authors": [
      "Fatih Dursun",
      "Bruno Vilhena Adorno",
      "Simon Watson",
      "Wei Pan"
    ],
    "published": "2025-09-04T10:52:27Z",
    "updated": "2025-09-04T10:52:27Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04094v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04094v1",
    "comment": "14 pages, 13 figures, 3 tables. Under Review for the IEEE\n  Transactions on Robotics (T-RO)",
    "relevance_score": 0.0
  },
  {
    "id": "2504.05041v2",
    "title": "Segmented Trajectory Optimization for Autonomous Parking in Unstructured   Environments",
    "summary": "This paper presents a Segmented Trajectory Optimization (STO) method for\nautonomous parking, which refines an initial trajectory into a dynamically\nfeasible and collision-free one using an iterative SQP-based approach. STO\nmaintains the maneuver strategy of the high-level global planner while allowing\ncurvature discontinuities at switching points to improve maneuver efficiency.\nTo ensure safety, a convex corridor is constructed via GJK-accelerated ellipse\nshrinking and expansion, serving as safety constraints in each iteration.\nNumerical simulations in perpendicular and reverse-angled parking scenarios\ndemonstrate that STO enhances maneuver efficiency while ensuring safety.\nMoreover, computational performance confirms its practicality for real-world\napplications.",
    "authors": [
      "Hang Yu",
      "Renjie Li"
    ],
    "published": "2025-04-07T13:07:17Z",
    "updated": "2025-09-04T09:47:45Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2504.05041v2",
    "arxiv_url": "http://arxiv.org/abs/2504.05041v2",
    "comment": "8 pages, 6 figures",
    "relevance_score": 0.0
  },
  {
    "id": "2509.04061v1",
    "title": "Integrated Wheel Sensor Communication using ESP32 -- A Contribution   towards a Digital Twin of the Road System",
    "summary": "While current onboard state estimation methods are adequate for most driving\nand safety-related applications, they do not provide insights into the\ninteraction between tires and road surfaces. This paper explores a novel\ncommunication concept for efficiently transmitting integrated wheel sensor data\nfrom an ESP32 microcontroller. Our proposed approach utilizes a\npublish-subscribe system, surpassing comparable solutions in the literature\nregarding data transmission volume. We tested this approach on a drum tire test\nrig with our prototype sensors system utilizing a diverse selection of sample\nfrequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our\ncommunication concept. The implemented prototype sensor showcases minimal data\nloss, approximately 0.1 % of the sampled data, validating the reliability of\nour developed communication system. This work contributes to advancing\nreal-time data acquisition, providing insights into optimizing integrated wheel\nsensor communication.",
    "authors": [
      "Ventseslav Yordanov",
      "Simon Schäfer",
      "Alexander Mann",
      "Stefan Kowalewski",
      "Bassam Alrifaee",
      "Lutz Eckstein"
    ],
    "published": "2025-09-04T09:47:10Z",
    "updated": "2025-09-04T09:47:10Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.04061v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04061v1",
    "comment": "6 pages, 2 figures, this work was submitted to and accepted by IEEE\n  International Conference on Intelligent Transportation Systems (ITSC) 2025",
    "relevance_score": 0.0
  },
  {
    "id": "2509.02760v2",
    "title": "A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual   Reality",
    "summary": "Studying tissue samples obtained during autopsies is the gold standard when\ndiagnosing the cause of death and for understanding disease pathophysiology.\nRecently, the interest in post mortem minimally invasive biopsies has grown\nwhich is a less destructive approach in comparison to an open autopsy and\nreduces the risk of infection. While manual biopsies under ultrasound guidance\nare more widely performed, robotic post mortem biopsies have been recently\nproposed. This approach can further reduce the risk of infection for\nphysicians. However, planning of the procedure and control of the robot need to\nbe efficient and usable. We explore a virtual reality setup with a digital twin\nto realize fully remote planning and control of robotic post mortem biopsies.\nThe setup is evaluated with forensic pathologists in a usability study for\nthree interaction methods. Furthermore, we evaluate clinical feasibility and\nevaluate the system with three human cadavers. Overall, 132 needle insertions\nwere performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue\nsamples were successfully biopsied and histopathologically verified. Users\nreported a very intuitive needle placement approach, indicating that the system\nis a promising, precise, and low-risk alternative to conventional approaches.",
    "authors": [
      "Maximilian Neidhardt",
      "Ludwig Bosse",
      "Vidas Raudonis",
      "Kristina Allgoewer",
      "Axel Heinemann",
      "Benjamin Ondruschka",
      "Alexander Schlaefer"
    ],
    "published": "2025-09-02T19:06:25Z",
    "updated": "2025-09-04T07:45:49Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.02760v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02760v2",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.03889v1",
    "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense   Correspondence and Visuotactile Affordance",
    "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.",
    "authors": [
      "Neha Sunil",
      "Megha Tippur",
      "Arnau Saumell",
      "Edward Adelson",
      "Alberto Rodriguez"
    ],
    "published": "2025-09-04T05:16:56Z",
    "updated": "2025-09-04T05:16:56Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.03889v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03889v1",
    "comment": "Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/",
    "relevance_score": 0.0
  },
  {
    "id": "2509.03859v1",
    "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator",
    "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.",
    "authors": [
      "Haichao Zhang",
      "Haonan Yu",
      "Le Zhao",
      "Andrew Choi",
      "Qinxun Bai",
      "Yiqing Yang",
      "Wei Xu"
    ],
    "published": "2025-09-04T03:36:07Z",
    "updated": "2025-09-04T03:36:07Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.03859v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03859v1",
    "comment": "Project: https://horizonrobotics.github.io/gail/SLIM",
    "relevance_score": 0.0
  },
  {
    "id": "2506.14770v2",
    "title": "GMT: General Motion Tracking for Humanoid Whole-Body Control",
    "summary": "The ability to track general whole-body motions in the real world is a useful\nway to build general-purpose humanoid robots. However, achieving this can be\nchallenging due to the temporal and kinematic diversity of the motions, the\npolicy's capability, and the difficulty of coordination of the upper and lower\nbodies. To address these issues, we propose GMT, a general and scalable\nmotion-tracking framework that trains a single unified policy to enable\nhumanoid robots to track diverse motions in the real world. GMT is built upon\ntwo core components: an Adaptive Sampling strategy and a Motion\nMixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically\nbalances easy and difficult motions during training. The MoE ensures better\nspecialization of different regions of the motion manifold. We show through\nextensive experiments in both simulation and the real world the effectiveness\nof GMT, achieving state-of-the-art performance across a broad spectrum of\nmotions using a unified general policy. Videos and additional information can\nbe found at https://gmt-humanoid.github.io.",
    "authors": [
      "Zixuan Chen",
      "Mazeyu Ji",
      "Xuxin Cheng",
      "Xuanbin Peng",
      "Xue Bin Peng",
      "Xiaolong Wang"
    ],
    "published": "2025-06-17T17:59:33Z",
    "updated": "2025-09-04T02:48:07Z",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2506.14770v2",
    "arxiv_url": "http://arxiv.org/abs/2506.14770v2",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2503.22524v2",
    "title": "Robust Offline Imitation Learning Through State-level Trajectory   Stitching",
    "summary": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.",
    "authors": [
      "Shuze Wang",
      "Yunpeng Mei",
      "Hongjie Cao",
      "Yetian Yuan",
      "Gang Wang",
      "Jian Sun",
      "Jie Chen"
    ],
    "published": "2025-03-28T15:28:36Z",
    "updated": "2025-09-04T02:32:37Z",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2503.22524v2",
    "arxiv_url": "http://arxiv.org/abs/2503.22524v2",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2509.03804v1",
    "title": "Real-Time Buoyancy Estimation for AUV Simulations Using Convex   Hull-Based Submerged Volume Calculation",
    "summary": "Accurate real-time buoyancy modeling is essential for high-fidelity\nAutonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a\nnative buoyancy system, requiring external solutions for precise underwater\nphysics. This paper presents a novel convex hull-based approach to dynamically\ncompute the submerged volume of an AUV in real time. By extracting mesh\ngeometry from the simulation environment and calculating the hull portion\nintersecting the water level along the z-axis, our method enhances accuracy\nover traditional geometric approximations. A cross-sectional area extension\nreduces computational overhead, enabling efficient buoyant force updates that\nadapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested\non a custom AUV design for SAUVC 2025, this approach delivers real-time\nperformance and scalability, improving simulation fidelity for underwater\nrobotics research without precomputed hydrodynamic models.",
    "authors": [
      "Ad-Deen Mahbub",
      "Md Ragib Shaharear"
    ],
    "published": "2025-09-04T01:42:11Z",
    "updated": "2025-09-04T01:42:11Z",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2509.03804v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03804v1",
    "comment": "7 pages, 10 figures",
    "relevance_score": 0.0
  },
  {
    "id": "2408.04348v3",
    "title": "Fuzzy to Clear: Elucidating the Threat Hunter Cognitive Process and   Cognitive Support Needs",
    "summary": "With security threats increasing in frequency and severity, it is critical\nthat we consider the important role of threat hunters. These highly-trained\nsecurity professionals learn to see, identify, and intercept security threats.\nMany recent works and existing tools in cybersecurity are focused on automating\nthe threat hunting process, often overlooking the critical human element. Our\nstudy shifts this paradigm by emphasizing a human-centered approach to\nunderstanding the lived experiences of threat hunters. By observing threat\nhunters during hunting sessions and analyzing the rich insights they provide,\nwe seek to advance the understanding of their cognitive processes and the tool\nsupport they need. Through an in-depth observational study of threat hunters,\nwe introduce a model of how they build and refine their mental models during\nthreat hunting sessions. We also present 23 themes that provide a foundation to\nbetter understand threat hunter needs and suggest five actionable design\npropositions to enhance the tools that support them. Through these\ncontributions, our work enriches the theoretical understanding of threat\nhunting and provides practical insights for designing more effective,\nhuman-centered cybersecurity tools.",
    "authors": [
      "Alessandra Maciel Paz Milani",
      "Arty Starr",
      "Samantha Hill",
      "Callum Curtis",
      "Norman Anderson",
      "David Moreno-Lumbreras",
      "Margaret-Anne Storey"
    ],
    "published": "2024-08-08T10:18:52Z",
    "updated": "2025-09-04T17:59:25Z",
    "categories": [
      "cs.CR",
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2408.04348v3",
    "arxiv_url": "http://arxiv.org/abs/2408.04348v3",
    "comment": "22 Pages; 5 Figures; 8 Tables",
    "relevance_score": 0.0
  },
  {
    "id": "2505.05923v2",
    "title": "Human causal perception in a cube-stacking task",
    "summary": "In intuitive physics the process of stacking cubes has become a paradigmatic,\ncanonical task. Even though it gets employed in various shades and\ncomplexities, the very fundamental setting with two cubes has not been\nthoroughly investigated. Furthermore, the majority of settings feature only a\nreduced, one dimensional (1D) decision space. In this paper an experiment is\nconducted in which participants judge the stability of two cubes stacked on top\nof each other. It is performed in the full 3D setting which features a 2D\ndecision surface. The analysis yield a shape of a rotated square for the\nperceived stability area instead of the commonly reported safety margin in 1D.\nThis implies a more complex decision behavior in human than previously assumed.",
    "authors": [
      "Nikolai Bahr",
      "Christoph Zetzsche"
    ],
    "published": "2025-05-09T09:54:34Z",
    "updated": "2025-09-04T10:36:46Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2505.05923v2",
    "arxiv_url": "http://arxiv.org/abs/2505.05923v2",
    "comment": "7 pages, 6 figures",
    "relevance_score": 0.0
  },
  {
    "id": "2509.03931v1",
    "title": "\"Low Frequency Tweeters Have More to Say!\" A New Approach to Identify   Importance of Tweets",
    "summary": "Twitter is one of the most popular social media platforms.With a large number\nof tweets, the activity feed of users becomes noisy, challenging to read, and\nmost importantly tweets often get lost. We present a new approach to\npersonalise the ranking of the tweets toward solving the problem of information\noverload which is achieved by analysing the relationship between the importance\nof tweets to the frequency at which the author tweets. The hypothesis tested is\nthat \"low-frequency tweeters have more to say\", i.e. if a user who tweets\ninfrequently actually goes to the effort of tweeting, then it is more likely to\nbe of more importance or contain more \"meaning\" than a tweet by a user who\ntweets continuously. We propose six new measures to evaluate the importance of\ntweets based on the ability of the tweet to drive interaction among its\nreaders, which is measured through metrics such as retweets, favourites, and\ncomments, and the extent of the author's network interacting with the tweet.\nOur study shows that users who tweeted less than ten tweets per week were more\nlikely to be perceived as important by their followers and have the most\nimportant messages. This identified tweet-frequency band could be used to\nreorder the activity feed of users and such reordering would ensure the\nmessages of low-frequency tweeters do not get lost in the stream of tweets.\nThis could also serve as a scoring index for Twitter users to identify users\nfrequently tweeting important messages.",
    "authors": [
      "Gautam Khannaa",
      "Yeliz Yesilada",
      "Sukru Eraslan",
      "Simon Harper"
    ],
    "published": "2025-09-04T06:40:47Z",
    "updated": "2025-09-04T06:40:47Z",
    "categories": [
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.03931v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03931v1",
    "comment": "12 pages",
    "relevance_score": 0.0
  },
  {
    "id": "2509.03792v1",
    "title": "Map as a By-product: Collective Landmark Mapping from IMU Data and   User-provided Texts in Situated Tasks",
    "summary": "This paper presents Collective Landmark Mapper, a novel map-as-a-by-product\nsystem for generating semantic landmark maps of indoor environments. Consider\nusers engaged in situated tasks that require them to navigate these\nenvironments and regularly take notes on their smartphones. Collective Landmark\nMapper exploits the smartphone's IMU data and the user's free text input during\nthese tasks to identify a set of landmarks encountered by the user. The\nidentified landmarks are then aggregated across multiple users to generate a\nunified map representing the positions and semantic information of all\nlandmarks. In developing the proposed system, we focused specifically on retail\napplications and conducted a formative interview with stakeholders to confirm\ntheir practical needs that motivate the map-as-a-byproduct approach. Our user\nstudy demonstrates the feasibility of the proposed system and its superior\nmapping performance in two different setups: creating a product availability\nmap from restocking checklist tasks at a retail store and constructing a room\nusage map from office inspection tasks, further demonstrating the potential\napplicability to non-retail applications.",
    "authors": [
      "Ryo Yonetani",
      "Kotaro Hara"
    ],
    "published": "2025-09-04T01:00:19Z",
    "updated": "2025-09-04T01:00:19Z",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "pdf_url": "http://arxiv.org/pdf/2509.03792v1",
    "arxiv_url": "http://arxiv.org/abs/2509.03792v1",
    "comment": "(c) 2025 Copyright held by the owner/author(s). Publication rights\n  licensed to ACM. This is the author's version of the work. It is posted here\n  for your personal use. Not for redistribution. The definitive Version of\n  Record was published in Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n  9, 3, Article 146 (September 2025), https://doi.org/10.1145/3749455",
    "relevance_score": 0.0
  },
  {
    "id": "2509.04052v1",
    "title": "Safeguarding Patient Trust in the Age of AI: Tackling Health   Misinformation with Explainable AI",
    "summary": "AI-generated health misinformation poses unprecedented threats to patient\nsafety and healthcare system trust globally. This white paper presents an\nexplainable AI framework developed through the EPSRC INDICATE project to combat\nmedical misinformation while enhancing evidence-based healthcare delivery. Our\nsystematic review of 17 studies reveals the urgent need for transparent AI\nsystems in healthcare. The proposed solution demonstrates 95% recall in\nclinical evidence retrieval and integrates novel trustworthiness classifiers\nachieving 76% F1 score in detecting biomedical misinformation. Results show\nthat explainable AI can transform traditional 6-month expert review processes\ninto real-time, automated evidence synthesis while maintaining clinical rigor.\nThis approach offers a critical intervention to preserve healthcare integrity\nin the AI era.",
    "authors": [
      "Sueun Hong",
      "Shuojie Fu",
      "Ovidiu Serban",
      "Brianna Bao",
      "James Kinross",
      "Francesa Toni",
      "Guy Martin",
      "Uddhav Vaghela"
    ],
    "published": "2025-09-04T09:29:34Z",
    "updated": "2025-09-04T09:29:34Z",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2509.04052v1",
    "arxiv_url": "http://arxiv.org/abs/2509.04052v1",
    "comment": null,
    "relevance_score": 0.0
  },
  {
    "id": "2411.04228v3",
    "title": "dsld: A Socially Relevant Tool for Teaching Statistics",
    "summary": "The growing influence of data science in statistics education requires tools\nthat make key concepts accessible through real-world applications. We introduce\n\"Data Science Looks At Discrimination\" (dsld), an R package that provides a\ncomprehensive set of analytical and graphical methods for examining issues of\ndiscrimination involving attributes such as race, gender, and age. By\npositioning fairness analysis as a teaching tool, the package enables\ninstructors to demonstrate confounder effects, model bias, and related topics\nthrough applied examples. An accompanying 80-page Quarto book guides students\nand legal professionals in understanding these principles and applying them to\nreal data. We describe the implementation of the package functions and\nillustrate their use with examples. Python interfaces are also available.",
    "authors": [
      "Aditya Mittal",
      "Taha Abdullah",
      "Arjun Ashok",
      "Brandon Zarate Estrada",
      "Shubhada Martha",
      "Billy Ouattara",
      "Jonathan Tran",
      "Norman Matloff"
    ],
    "published": "2024-11-06T19:50:00Z",
    "updated": "2025-09-04T04:44:46Z",
    "categories": [
      "stat.ME",
      "cs.IR",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.IR",
    "pdf_url": "http://arxiv.org/pdf/2411.04228v3",
    "arxiv_url": "http://arxiv.org/abs/2411.04228v3",
    "comment": "preprint",
    "relevance_score": 0.0
  }
]