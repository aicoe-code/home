name: Daily AI Papers Pipeline

on:
  schedule:
    # Run twice daily at 9 AM and 6 PM UTC
    - cron: '0 9 * * *'
    - cron: '0 18 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      date:
        description: 'Date to fetch papers (YYYY-MM-DD format)'
        required: false
        type: string
      force:
        description: 'Force refetch even if already processed'
        required: false
        type: boolean
        default: false
      max_papers:
        description: 'Maximum papers to process'
        required: false
        type: string
        default: '50'

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  process-papers:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml markdown requests
          # Note: In production, you would also install:
          # pip install anthropic  # For Claude API
          # pip install PyPDF2     # For PDF processing
      
      - name: Set up environment
        run: |
          # Create necessary directories
          mkdir -p data/raw data/processed data/archive
          mkdir -p docs/papers docs/assets
          
          # Set environment variables
          echo "CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }}" >> $GITHUB_ENV
          echo "PIPELINE_DATE=$(date -d 'yesterday' '+%Y-%m-%d')" >> $GITHUB_ENV
      
      - name: Run pipeline
        run: |
          cd pipeline
          
          # Determine parameters
          if [ -n "${{ github.event.inputs.date }}" ]; then
            DATE_ARG="--date ${{ github.event.inputs.date }}"
          else
            DATE_ARG=""
          fi
          
          if [ "${{ github.event.inputs.force }}" == "true" ]; then
            FORCE_ARG="--force"
          else
            FORCE_ARG=""
          fi
          
          # Update config with max papers if specified
          if [ -n "${{ github.event.inputs.max_papers }}" ]; then
            # Use Python to update the config
            python -c "
import yaml
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)
config['processing']['max_papers_per_run'] = ${{ github.event.inputs.max_papers }}
with open('config.yaml', 'w') as f:
    yaml.dump(config, f)
"
          fi
          
          # Run the pipeline
          python run_pipeline.py $DATE_ARG $FORCE_ARG
      
      - name: Check for changes
        id: check-changes
        run: |
          git add -A
          if git diff --staged --quiet; then
            echo "changes=false" >> $GITHUB_OUTPUT
            echo "No new papers or changes detected"
          else
            echo "changes=true" >> $GITHUB_OUTPUT
            echo "New papers processed, preparing to commit"
          fi
      
      - name: Commit changes
        if: steps.check-changes.outputs.changes == 'true'
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Count changes
          NEW_FILES=$(git diff --staged --name-only | wc -l)
          
          git commit -m "📚 Update AI papers for ${{ env.PIPELINE_DATE }}
          
          Automated pipeline run
          Files updated: ${NEW_FILES}
          
          [skip ci]"
          
          git push origin main
      
      - name: Setup Pages
        if: steps.check-changes.outputs.changes == 'true'
        uses: actions/configure-pages@v4
      
      - name: Upload artifact
        if: steps.check-changes.outputs.changes == 'true'
        uses: actions/upload-pages-artifact@v3
        with:
          path: './docs'
      
      - name: Deploy to GitHub Pages
        if: steps.check-changes.outputs.changes == 'true'
        id: deployment
        uses: actions/deploy-pages@v4
      
      - name: Create summary
        if: always()
        run: |
          echo "## 📊 Pipeline Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.check-changes.outputs.changes }}" == "true" ]; then
            echo "✅ **Status:** Successfully processed and published papers" >> $GITHUB_STEP_SUMMARY
            echo "🌐 **Website:** ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "ℹ️ **Status:** No new papers found or no changes detected" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date processed:** ${{ env.PIPELINE_DATE }}" >> $GITHUB_STEP_SUMMARY
          
          # Show statistics if available
          if [ -f "data/stats_$(date +%Y%m%d).json" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📈 Statistics" >> $GITHUB_STEP_SUMMARY
            python -c "
import json
with open('data/stats_$(date +%Y%m%d).json', 'r') as f:
    stats = json.load(f)
print(f\"- Papers fetched: {stats.get('papers_fetched', 0)}\")
print(f\"- Papers processed: {stats.get('papers_processed', 0)}\")
print(f\"- High relevance: {stats['relevance_distribution'].get('high', 0)}\")
print(f\"- Medium relevance: {stats['relevance_distribution'].get('medium', 0)}\")
print(f\"- Low relevance: {stats['relevance_distribution'].get('low', 0)}\")
" >> $GITHUB_STEP_SUMMARY
          fi
          
          # List generated files
          if [ -d "docs/papers" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📁 Recent Papers" >> $GITHUB_STEP_SUMMARY
            ls -lt docs/papers/*.html 2>/dev/null | head -5 | awk '{print "- " $9}' >> $GITHUB_STEP_SUMMARY || echo "No paper files found" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Send notification (optional)
        if: steps.check-changes.outputs.changes == 'true' && vars.NOTIFICATION_WEBHOOK_URL != ''
        run: |
          # Send notification to Slack/Discord webhook if configured
          curl -X POST ${{ vars.NOTIFICATION_WEBHOOK_URL }} \
            -H 'Content-Type: application/json' \
            -d '{
              "text": "📚 AI Papers Pipeline completed successfully!",
              "blocks": [{
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*AI Papers Pipeline Update*\n• Date: ${{ env.PIPELINE_DATE }}\n• Status: ✅ Success\n• View: ${{ steps.deployment.outputs.page_url }}"
                }
              }]
            }' || echo "Notification failed, but pipeline succeeded"
      
      - name: Handle errors
        if: failure()
        run: |
          echo "## ❌ Pipeline Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The AI paper processing pipeline encountered an error." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Common issues:**" >> $GITHUB_STEP_SUMMARY
          echo "- arXiv API may be temporarily unavailable" >> $GITHUB_STEP_SUMMARY
          echo "- Claude API key not configured or invalid" >> $GITHUB_STEP_SUMMARY
          echo "- Rate limiting from APIs" >> $GITHUB_STEP_SUMMARY
          echo "- Network connectivity issues" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check the workflow logs for detailed error information." >> $GITHUB_STEP_SUMMARY

  cleanup-old-data:
    runs-on: ubuntu-latest
    needs: process-papers
    if: success()
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Clean up old data
        run: |
          # Archive data older than 30 days
          find data/raw -type f -mtime +30 -delete 2>/dev/null || true
          find data/processed -type f -mtime +30 -exec mv {} data/archive/ \; 2>/dev/null || true
          
          echo "Old data cleanup completed"