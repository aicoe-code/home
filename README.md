# AICOE AI Paper Processing Pipeline

An automated pipeline for fetching, processing, and publishing AI research papers from arXiv.

## ğŸš€ Features

- **Automated Fetching**: Daily collection of AI papers from arXiv
- **Smart Filtering**: Relevance scoring based on keywords and categories
- **Deduplication**: Avoids processing the same papers multiple times
- **AI Processing**: Claude AI integration for generating summaries (optional)
- **Beautiful Output**: Clean, professional HTML website
- **GitHub Actions**: Fully automated with twice-daily runs
- **GitHub Pages**: Automatic deployment to your website

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ pipeline/               # Core processing modules
â”‚   â”œâ”€â”€ fetcher.py         # arXiv paper fetcher with deduplication
â”‚   â”œâ”€â”€ processor.py       # Claude AI processor for summaries
â”‚   â”œâ”€â”€ generator.py       # HTML generator (full-featured)
â”‚   â”œâ”€â”€ simple_generator.py # HTML generator (no dependencies)
â”‚   â”œâ”€â”€ run_pipeline.py    # Main orchestrator
â”‚   â””â”€â”€ config.yaml        # Configuration file
â”œâ”€â”€ templates/             # Templates for summaries
â”‚   â””â”€â”€ summary-template.md
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ raw/              # Fetched papers (JSON)
â”‚   â”œâ”€â”€ processed/        # Processed summaries
â”‚   â””â”€â”€ archive/          # Archived old papers
â”œâ”€â”€ docs/                  # GitHub Pages website
â”‚   â”œâ”€â”€ index.html        # Main page
â”‚   â”œâ”€â”€ papers/           # Individual paper pages
â”‚   â””â”€â”€ assets/           # CSS and other assets
â””â”€â”€ .github/workflows/     # GitHub Actions
    â””â”€â”€ daily-ai-papers.yml
```

## ğŸ› ï¸ Setup

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd home
```

### 2. Install Dependencies (Optional)

For full functionality with markdown processing:
```bash
pip install pyyaml markdown requests
```

For Claude AI integration:
```bash
pip install anthropic
```

### 3. Configure the Pipeline

Edit `pipeline/config.yaml` to customize:
- arXiv categories to monitor
- Keywords for relevance scoring
- Processing limits
- Output settings

### 4. Set Up Claude API (Optional)

Add your Claude API key as an environment variable:
```bash
export CLAUDE_API_KEY="your-api-key-here"
```

For GitHub Actions, add it as a repository secret:
- Go to Settings â†’ Secrets â†’ Actions
- Add new secret: `CLAUDE_API_KEY`

### 5. Enable GitHub Pages

1. Go to repository Settings â†’ Pages
2. Source: Deploy from a branch
3. Branch: main, folder: /docs
4. Save

## ğŸ¯ Usage

### Manual Run

Fetch papers from yesterday:
```bash
cd pipeline
python3 fetcher.py
```

Fetch papers from specific date:
```bash
python3 fetcher.py --date 2025-09-04
```

Generate HTML from fetched papers:
```bash
python3 simple_generator.py ../data/raw/arxiv_papers_*.json
```

Run complete pipeline:
```bash
python3 run_pipeline.py
```

### Automated Runs

The GitHub Actions workflow runs automatically:
- **Schedule**: 9 AM and 6 PM UTC daily
- **Manual trigger**: Actions tab â†’ Run workflow

## ğŸ“Š Pipeline Components

### Fetcher (`fetcher.py`)
- Fetches papers from multiple arXiv categories
- Calculates relevance scores based on keywords
- Implements deduplication to avoid reprocessing
- Saves papers to JSON format

### Processor (`processor.py`)
- Processes papers with Claude AI (when API key configured)
- Generates structured summaries
- Falls back to basic summaries without API

### Generator (`generator.py` / `simple_generator.py`)
- Creates beautiful HTML pages
- Generates index with statistics
- Creates individual paper pages
- No external dependencies (simple version)

### Orchestrator (`run_pipeline.py`)
- Coordinates all components
- Handles configuration
- Manages data flow
- Provides logging and error handling

## ğŸ¨ Customization

### Categories and Keywords

Edit `pipeline/config.yaml`:
```yaml
arxiv:
  categories:
    - cs.AI
    - cs.LG
    - cs.CV
  keywords:
    - transformer
    - neural network
    - deep learning
```

### Relevance Scoring

Adjust minimum relevance score:
```yaml
arxiv:
  min_relevance_score: 2.0  # Only process papers with score >= 2.0
```

### Processing Limits

Control how many papers to process:
```yaml
processing:
  max_papers_per_run: 50
```

## ğŸ“ˆ Output

The pipeline generates:
- **Website**: Clean HTML with all papers
- **Statistics**: Paper counts, relevance distribution
- **Archives**: Historical data preservation

View your website at: `https://<username>.github.io/<repo-name>/`

## ğŸ”§ Troubleshooting

### No papers found
- Check if arXiv API is accessible
- Verify date parameter (papers may not exist for future dates)

### HTML not generating
- Ensure papers exist in data/raw/
- Check for Python errors in console

### GitHub Actions failing
- Verify secrets are configured
- Check workflow logs for specific errors

## ğŸ“ License

MIT License - feel free to use and modify

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

---

Built with â¤ï¸ for the AICOE Research Library